{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSJl8ZoSucQK"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
    "\n",
    "**Module 7 Assignment: Computer Vision Neural Network**\n",
    "\n",
    "**Student Name: Your Name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "If you are using Google CoLab, it will be necessary to mount your GDrive so that you can send your notebook during the submit process. Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eCUTf6n3BCb"
   },
   "source": [
    "# Assignment Submit Function\n",
    "\n",
    "You will submit the 10 programming assignments electronically.  The following submit function can be used to do this.  My server will perform a basic check of each assignment and let you know if it sees any basic problems. \n",
    "\n",
    "**It is unlikely that should need to modify this function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHb2ceEO3Qil"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vNkxmQDucQN"
   },
   "source": [
    "# Assignment Instructions\n",
    "\n",
    "For this assignment, you will use YOLO running on Google CoLab.  I suggest that you run this assignment on CoLab because the example code below is already setup to get you started with the correct versions of  YOLO on TensorFlow 2.0.\n",
    "\n",
    "For this assignment you are provided with 10 image files that contain 10 different webcam pictures taken at the [Venice Sidewalk Cafe](https://www.westland.net/beachcam/) a WebCam that has been in opration since 1996.  You can find the 10 images here:\n",
    "\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk1.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk2.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk3.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk4.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk5.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk6.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk7.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk8.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk9.png\n",
    "* https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk10.png\n",
    "\n",
    "You can see a sample of the WebCam here:\n",
    "\n",
    "![alt text](https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk1.png)\n",
    "\n",
    "YOLO does quite well-recognizing objects in this webcam, as the following image illustrates.\n",
    "\n",
    "![alt text](https://data.heatonresearch.com/data/t81-558/sidewalk/predictions.jpg)\n",
    "\n",
    "You are to write a script that counts the number of certain objects in each of the images.  Specifically, you are looking for:\n",
    "\n",
    "* person\n",
    "* car\n",
    "* bicycle\n",
    "* motorbike\n",
    "* umbrella\n",
    "* handbag\n",
    "\n",
    "It is essential that your use YOLO with a threshold of 10% if you want your results to match mine. The sample code below already contains this setting.  Your program can set this threshold with the following command.\n",
    "\n",
    "* FLAGS.yolo_score_threshold = 0.1\n",
    "\n",
    "Your submitted data frame should also contain a column that identifies which image generated each row.  This column should be named **image** and contain integer numbers between 1 and 10.  There should be 10 rows in total.  The complete data frame should look something like this.\n",
    "\n",
    "|image|person|car|bicycle|motorbike|umbrella|handbag|\n",
    "|-|-|-|-|-|-|-|\n",
    "|1|23|0|3|4|0|0|\n",
    "|1|27|1|8|2|0|0|\n",
    "|2|29|0|0|0|3|0|\n",
    "|...|...|...|...|...|...|...|\n",
    "\n",
    "\n",
    "The following code sets up YOLO and then dumps the classification information for the first image.  This notebook only serves to get you started.  Read in all ten images and generate a data frame that looks like the following. Use the **submit** function as you did in previous assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing YoloV3-TF2\n",
    "\n",
    "The following code is taken from the module, it installs YoLoV3-TF2 if not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/zzh8829/yolov3-tf2.git@master\n",
      "  Cloning https://github.com/zzh8829/yolov3-tf2.git (to revision master) to /private/var/folders/nz/jw1_lq4s10389lbx_j_vjyg4qy7h5_/T/pip-req-build-uta02k4d\n",
      "  Running command git clone -q https://github.com/zzh8829/yolov3-tf2.git /private/var/folders/nz/jw1_lq4s10389lbx_j_vjyg4qy7h5_/T/pip-req-build-uta02k4d\n",
      "Requirement already satisfied (use --upgrade to upgrade): yolov3-tf2==0.1 from git+https://github.com/zzh8829/yolov3-tf2.git@master in /Users/jheaton/miniconda3/envs/tensorflow/lib/python3.7/site-packages\n",
      "Building wheels for collected packages: yolov3-tf2\n",
      "  Building wheel for yolov3-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yolov3-tf2: filename=yolov3_tf2-0.1-cp37-none-any.whl size=9193 sha256=83daf024487dd631f3a40178f24f10c03768b6f0424b4c852754f20d1cff9b64\n",
      "  Stored in directory: /private/var/folders/nz/jw1_lq4s10389lbx_j_vjyg4qy7h5_/T/pip-ephem-wheel-cache-o1vsskds/wheels/59/1b/97/905ab51e9c0330efe8c3c518aff17de4ee91100412cd6dd553\n",
      "Successfully built yolov3-tf2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install git+https://github.com/zzh8829/yolov3-tf2.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is taken from the module, it downloads needed files for YoLoV3-TF2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/convert.py\n",
      "8192/1067 [======================================================================================================================================================================================================================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "if COLAB:\n",
    "  ROOT = '/content/drive/My Drive/projects/t81_558_dlearning/yolo'\n",
    "else:\n",
    "  ROOT = os.path.join(os.getcwd(),'data')\n",
    "\n",
    "filename_darknet_weights = tf.keras.utils.get_file(\n",
    "    os.path.join(ROOT,'yolov3.weights'),\n",
    "    origin='https://pjreddie.com/media/files/yolov3.weights')\n",
    "TINY = False\n",
    "\n",
    "filename_convert_script = tf.keras.utils.get_file(\n",
    "    os.path.join(os.getcwd(),'convert.py'),\n",
    "    origin='https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/convert.py')\n",
    "\n",
    "filename_classes = tf.keras.utils.get_file(\n",
    "    os.path.join(ROOT,'coco.names'),\n",
    "    origin='https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/data/coco.names')\n",
    "filename_converted_weights = os.path.join(ROOT,'yolov3.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering Weights\n",
    "\n",
    "The following code is taken from the module, it transfers preloaded weights into YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:05:19.771988: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-01-01 00:05:19.772340: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_darknet (Model)            ((None, None, None,  40620640    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_0 (Model)             (None, None, None, 5 11024384    yolo_darknet[1][2]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_1 (Model)             (None, None, None, 2 2957312     yolo_conv_0[1][0]                \n",
      "                                                                 yolo_darknet[1][1]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_2 (Model)             (None, None, None, 1 741376      yolo_conv_1[1][0]                \n",
      "                                                                 yolo_darknet[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_0 (Model)           (None, None, None, 3 4984063     yolo_conv_0[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_1 (Model)           (None, None, None, 3 1312511     yolo_conv_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_2 (Model)           (None, None, None, 3 361471      yolo_conv_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_nms (Lambda)               ((None, 100, 4), (No 0           yolo_boxes_0[0][0]               \n",
      "                                                                 yolo_boxes_0[0][1]               \n",
      "                                                                 yolo_boxes_0[0][2]               \n",
      "                                                                 yolo_boxes_1[0][0]               \n",
      "                                                                 yolo_boxes_1[0][1]               \n",
      "                                                                 yolo_boxes_1[0][2]               \n",
      "                                                                 yolo_boxes_2[0][0]               \n",
      "                                                                 yolo_boxes_2[0][1]               \n",
      "                                                                 yolo_boxes_2[0][2]               \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "I0101 00:05:27.726027 140735710913408 convert.py:19] model created\n",
      "I0101 00:05:27.727918 140735710913408 utils.py:45] yolo_darknet/conv2d bn\n",
      "I0101 00:05:27.730610 140735710913408 utils.py:45] yolo_darknet/conv2d_1 bn\n",
      "I0101 00:05:27.732955 140735710913408 utils.py:45] yolo_darknet/conv2d_2 bn\n",
      "I0101 00:05:27.735034 140735710913408 utils.py:45] yolo_darknet/conv2d_3 bn\n",
      "I0101 00:05:27.737543 140735710913408 utils.py:45] yolo_darknet/conv2d_4 bn\n",
      "I0101 00:05:27.740562 140735710913408 utils.py:45] yolo_darknet/conv2d_5 bn\n",
      "I0101 00:05:27.742768 140735710913408 utils.py:45] yolo_darknet/conv2d_6 bn\n",
      "I0101 00:05:27.745707 140735710913408 utils.py:45] yolo_darknet/conv2d_7 bn\n",
      "I0101 00:05:27.748442 140735710913408 utils.py:45] yolo_darknet/conv2d_8 bn\n",
      "I0101 00:05:27.751374 140735710913408 utils.py:45] yolo_darknet/conv2d_9 bn\n",
      "I0101 00:05:27.756246 140735710913408 utils.py:45] yolo_darknet/conv2d_10 bn\n",
      "I0101 00:05:27.758761 140735710913408 utils.py:45] yolo_darknet/conv2d_11 bn\n",
      "I0101 00:05:27.763235 140735710913408 utils.py:45] yolo_darknet/conv2d_12 bn\n",
      "I0101 00:05:27.766791 140735710913408 utils.py:45] yolo_darknet/conv2d_13 bn\n",
      "I0101 00:05:27.771401 140735710913408 utils.py:45] yolo_darknet/conv2d_14 bn\n",
      "I0101 00:05:27.774218 140735710913408 utils.py:45] yolo_darknet/conv2d_15 bn\n",
      "I0101 00:05:27.778388 140735710913408 utils.py:45] yolo_darknet/conv2d_16 bn\n",
      "I0101 00:05:27.781044 140735710913408 utils.py:45] yolo_darknet/conv2d_17 bn\n",
      "I0101 00:05:27.785137 140735710913408 utils.py:45] yolo_darknet/conv2d_18 bn\n",
      "I0101 00:05:27.788091 140735710913408 utils.py:45] yolo_darknet/conv2d_19 bn\n",
      "I0101 00:05:27.792218 140735710913408 utils.py:45] yolo_darknet/conv2d_20 bn\n",
      "I0101 00:05:27.795013 140735710913408 utils.py:45] yolo_darknet/conv2d_21 bn\n",
      "I0101 00:05:27.799947 140735710913408 utils.py:45] yolo_darknet/conv2d_22 bn\n",
      "I0101 00:05:27.803141 140735710913408 utils.py:45] yolo_darknet/conv2d_23 bn\n",
      "I0101 00:05:27.808761 140735710913408 utils.py:45] yolo_darknet/conv2d_24 bn\n",
      "I0101 00:05:27.812190 140735710913408 utils.py:45] yolo_darknet/conv2d_25 bn\n",
      "I0101 00:05:27.816740 140735710913408 utils.py:45] yolo_darknet/conv2d_26 bn\n",
      "I0101 00:05:27.836106 140735710913408 utils.py:45] yolo_darknet/conv2d_27 bn\n",
      "I0101 00:05:27.840915 140735710913408 utils.py:45] yolo_darknet/conv2d_28 bn\n",
      "I0101 00:05:27.856431 140735710913408 utils.py:45] yolo_darknet/conv2d_29 bn\n",
      "I0101 00:05:27.860378 140735710913408 utils.py:45] yolo_darknet/conv2d_30 bn\n",
      "I0101 00:05:27.876090 140735710913408 utils.py:45] yolo_darknet/conv2d_31 bn\n",
      "I0101 00:05:27.880883 140735710913408 utils.py:45] yolo_darknet/conv2d_32 bn\n",
      "I0101 00:05:27.896075 140735710913408 utils.py:45] yolo_darknet/conv2d_33 bn\n",
      "I0101 00:05:27.900032 140735710913408 utils.py:45] yolo_darknet/conv2d_34 bn\n",
      "I0101 00:05:27.915318 140735710913408 utils.py:45] yolo_darknet/conv2d_35 bn\n",
      "I0101 00:05:27.919420 140735710913408 utils.py:45] yolo_darknet/conv2d_36 bn\n",
      "I0101 00:05:27.934631 140735710913408 utils.py:45] yolo_darknet/conv2d_37 bn\n",
      "I0101 00:05:27.938657 140735710913408 utils.py:45] yolo_darknet/conv2d_38 bn\n",
      "I0101 00:05:27.953392 140735710913408 utils.py:45] yolo_darknet/conv2d_39 bn\n",
      "I0101 00:05:27.957299 140735710913408 utils.py:45] yolo_darknet/conv2d_40 bn\n",
      "I0101 00:05:27.972300 140735710913408 utils.py:45] yolo_darknet/conv2d_41 bn\n",
      "I0101 00:05:27.976281 140735710913408 utils.py:45] yolo_darknet/conv2d_42 bn\n",
      "I0101 00:05:27.991706 140735710913408 utils.py:45] yolo_darknet/conv2d_43 bn\n",
      "I0101 00:05:28.064110 140735710913408 utils.py:45] yolo_darknet/conv2d_44 bn\n",
      "I0101 00:05:28.074983 140735710913408 utils.py:45] yolo_darknet/conv2d_45 bn\n",
      "I0101 00:05:28.142597 140735710913408 utils.py:45] yolo_darknet/conv2d_46 bn\n",
      "I0101 00:05:28.152260 140735710913408 utils.py:45] yolo_darknet/conv2d_47 bn\n",
      "I0101 00:05:28.219226 140735710913408 utils.py:45] yolo_darknet/conv2d_48 bn\n",
      "I0101 00:05:28.229513 140735710913408 utils.py:45] yolo_darknet/conv2d_49 bn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0101 00:05:28.296176 140735710913408 utils.py:45] yolo_darknet/conv2d_50 bn\n",
      "I0101 00:05:28.306047 140735710913408 utils.py:45] yolo_darknet/conv2d_51 bn\n",
      "I0101 00:05:28.372566 140735710913408 utils.py:45] yolo_conv_0/conv2d_52 bn\n",
      "I0101 00:05:28.381467 140735710913408 utils.py:45] yolo_conv_0/conv2d_53 bn\n",
      "I0101 00:05:28.450322 140735710913408 utils.py:45] yolo_conv_0/conv2d_54 bn\n",
      "I0101 00:05:28.459434 140735710913408 utils.py:45] yolo_conv_0/conv2d_55 bn\n",
      "I0101 00:05:28.526379 140735710913408 utils.py:45] yolo_conv_0/conv2d_56 bn\n",
      "I0101 00:05:28.534791 140735710913408 utils.py:45] yolo_output_0/conv2d_57 bn\n",
      "I0101 00:05:28.601698 140735710913408 utils.py:45] yolo_output_0/conv2d_58 bias\n",
      "I0101 00:05:28.607778 140735710913408 utils.py:45] yolo_conv_1/conv2d_59 bn\n",
      "I0101 00:05:28.612086 140735710913408 utils.py:45] yolo_conv_1/conv2d_60 bn\n",
      "I0101 00:05:28.615308 140735710913408 utils.py:45] yolo_conv_1/conv2d_61 bn\n",
      "I0101 00:05:28.630283 140735710913408 utils.py:45] yolo_conv_1/conv2d_62 bn\n",
      "I0101 00:05:28.633562 140735710913408 utils.py:45] yolo_conv_1/conv2d_63 bn\n",
      "I0101 00:05:28.648599 140735710913408 utils.py:45] yolo_conv_1/conv2d_64 bn\n",
      "I0101 00:05:28.651022 140735710913408 utils.py:45] yolo_output_1/conv2d_65 bn\n",
      "I0101 00:05:28.665372 140735710913408 utils.py:45] yolo_output_1/conv2d_66 bias\n",
      "I0101 00:05:28.667402 140735710913408 utils.py:45] yolo_conv_2/conv2d_67 bn\n",
      "I0101 00:05:28.669167 140735710913408 utils.py:45] yolo_conv_2/conv2d_68 bn\n",
      "I0101 00:05:28.671075 140735710913408 utils.py:45] yolo_conv_2/conv2d_69 bn\n",
      "I0101 00:05:28.674639 140735710913408 utils.py:45] yolo_conv_2/conv2d_70 bn\n",
      "I0101 00:05:28.676549 140735710913408 utils.py:45] yolo_conv_2/conv2d_71 bn\n",
      "I0101 00:05:28.680378 140735710913408 utils.py:45] yolo_conv_2/conv2d_72 bn\n",
      "I0101 00:05:28.682330 140735710913408 utils.py:45] yolo_output_2/conv2d_73 bn\n",
      "I0101 00:05:28.685889 140735710913408 utils.py:45] yolo_output_2/conv2d_74 bias\n",
      "I0101 00:05:28.687332 140735710913408 convert.py:22] weights loaded\n",
      "I0101 00:05:30.248057 140735710913408 convert.py:26] sanity check passed\n",
      "I0101 00:05:30.816426 140735710913408 convert.py:29] weights saved\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} \"{filename_convert_script}\" --weights \"{filename_darknet_weights}\" --output \"{filename_converted_weights}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the files needed for YOLO, we are ready to use it to recognize components of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(filename_convert_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from yolov3_tf2.models import (YoloV3, YoloV3Tiny)\n",
    "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
    "from yolov3_tf2.utils import draw_outputs\n",
    "import sys\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "\n",
    "# Flags are used to define several options for YOLO.\n",
    "flags.DEFINE_string('classes', filename_classes, 'path to classes file')\n",
    "flags.DEFINE_string('weights', filename_converted_weights, 'path to weights file')\n",
    "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
    "flags.DEFINE_integer('size', 416, 'resize images to')\n",
    "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
    "flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n",
    "FLAGS([sys.argv[0]])\n",
    "\n",
    "# Locate devices to run YOLO on (e.g. GPU)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights loaded\n",
      "classes loaded\n"
     ]
    }
   ],
   "source": [
    "# This assignment does not use the \"Tiny version\"\n",
    "if FLAGS.tiny:\n",
    "    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n",
    "else:\n",
    "    yolo = YoloV3(classes=FLAGS.num_classes)\n",
    "\n",
    "# Load weights and classes\n",
    "yolo.load_weights(FLAGS.weights).expect_partial()\n",
    "print('weights loaded')\n",
    "\n",
    "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
    "print('classes loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code below to create your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detections:\n",
      "\tperson, 0.8610939383506775, [0.61918443 0.59739566 0.64552397 0.6964505 ]\n",
      "\tperson, 0.8247122168540955, [0.5645817  0.6453512  0.595969   0.74558043]\n",
      "\tperson, 0.7935143709182739, [0.6079407  0.69781303 0.6366278  0.7886913 ]\n",
      "\tperson, 0.7164211273193359, [0.6633641  0.68104416 0.6887379  0.79442984]\n",
      "\tperson, 0.6968549489974976, [0.09189358 0.33121654 0.11105401 0.37928268]\n",
      "\tperson, 0.6268394589424133, [0.11293059 0.32814595 0.13367462 0.38183174]\n",
      "\tperson, 0.5897685885429382, [0.6063562  0.7543961  0.63623744 0.8477465 ]\n",
      "\tperson, 0.4365485906600952, [0.8495312 0.4620815 0.8663175 0.5087753]\n",
      "\tperson, 0.43569520115852356, [0.43019524 0.7499078  0.4732755  0.85166055]\n",
      "\tperson, 0.4281286895275116, [0.59075445 0.6240619  0.61355954 0.6932853 ]\n",
      "\tperson, 0.39086809754371643, [0.808963  0.5757501 0.8373168 0.6760719]\n",
      "\tperson, 0.3769091069698334, [0.5117131 0.6074021 0.5481817 0.7174003]\n",
      "\tperson, 0.37344518303871155, [0.42949548 0.7256865  0.46626833 0.79150695]\n",
      "\tperson, 0.3069881200790405, [0.3984463  0.5203366  0.4362013  0.61008453]\n",
      "\tperson, 0.30384624004364014, [0.5358126  0.60312814 0.56915295 0.717491  ]\n",
      "\tperson, 0.29558682441711426, [0.65262705 0.7223847  0.67923063 0.8030238 ]\n",
      "\tperson, 0.29516708850860596, [0.7784145  0.77735645 0.80293584 0.9013105 ]\n",
      "\tmotorbike, 0.2879989743232727, [0.503016   0.65149045 0.5473068  0.72112453]\n",
      "\tperson, 0.2262924611568451, [0.55625015 0.63573825 0.58670956 0.7355976 ]\n",
      "\tmotorbike, 0.2003154307603836, [0.4301363  0.7773609  0.47228858 0.85768104]\n",
      "\tmotorbike, 0.1963575780391693, [0.3954409  0.78764945 0.4307563  0.85294145]\n",
      "\tperson, 0.19133178889751434, [0.76667327 0.47130704 0.78793555 0.5575557 ]\n",
      "\tbicycle, 0.15066389739513397, [0.4301363  0.7773609  0.47228858 0.85768104]\n",
      "\tperson, 0.13832373917102814, [0.8051935  0.4202181  0.83031243 0.4867506 ]\n",
      "\tperson, 0.13359948992729187, [0.75360006 0.47183043 0.7774052  0.5576901 ]\n",
      "\tbicycle, 0.13281375169754028, [0.3954409  0.78764945 0.4307563  0.85294145]\n",
      "\tbicycle, 0.13062599301338196, [0.49793977 0.65387976 0.561223   0.7203696 ]\n",
      "\tperson, 0.1108977422118187, [0.5361232  0.59820485 0.5715708  0.66090786]\n",
      "\tperson, 0.10892507433891296, [0.43477708 0.723783   0.45917106 0.767683  ]\n",
      "\tbench, 0.10668502002954483, [0.446218   0.3652647  0.49337965 0.39149672]\n",
      "\tmotorbike, 0.1064528226852417, [0.7992575  0.61630136 0.8361037  0.67813164]\n",
      "Success: Submitted Assignment #7 for heaton-jeff:\n",
      "You have submitted this assignment 6 times. (this is fine)\n",
      "Warning: Solution has 10 rows, yours has 0 rows.\n",
      "Warning: Solution has 7 columns, yours has 1 columns.\n",
      "Warning: Solution headers: bicycle,car,handbag,image,motorbike,person,umbrella; your headers: . (order not considered)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "i = 1\n",
    "url = f\"https://data.heatonresearch.com/data/t81-558/sidewalk/sidewalk{i}.png\"\n",
    "response = requests.get(url)\n",
    "img_raw = tf.image.decode_image(response.content, channels=3)\n",
    "\n",
    "# Preprocess image\n",
    "img = tf.expand_dims(img_raw, 0)\n",
    "img = transform_images(img, FLAGS.size)\n",
    "\n",
    "# Desired threshold (any sub-image below this confidence level will be ignored.)\n",
    "FLAGS.yolo_score_threshold = 0.1\n",
    "\n",
    "# Recognize and report results\n",
    "boxes, scores, classes, nums = yolo(img)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "print('detections:')\n",
    "for i in range(nums[0]):\n",
    "    cls = class_names[int(classes[0][i])]\n",
    "    score = np.array(scores[0][i])\n",
    "    box = np.array(boxes[0][i])\n",
    "    print(f\"\\t{cls}, {score}, {box}\")\n",
    "\n",
    "# This is your student key that I emailed to you at the beginnning of the semester.\n",
    "key = \"JNAl4M33jgax0oM1GJFuF6QHnAk58HWT3FElTJwQ\"  # This is an example key and will not work.\n",
    "\n",
    "# You must also identify your source file.  (modify for your local setup)\n",
    "# file='/content/drive/My Drive/Colab Notebooks/assignment_yourname_class7.ipynb'  # Google CoLab\n",
    "# file='C:\\\\Users\\\\jeffh\\\\projects\\\\t81_558_deep_learning\\\\assignments\\\\assignment_yourname_class7.ipynb'  # Windows\n",
    "file='/Users/jheaton/projects/t81_558_deep_learning/assignments/assignment_yourname_class7.ipynb'  # Mac/Linux\n",
    "\n",
    "submit(source_file=file,data=submit_df,key=key,no=7)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of assignment_yourname_class7.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

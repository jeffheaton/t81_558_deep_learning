{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 12: Deep Learning Applications**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tonight we will see how to apply deep learning networks to data science.  There are many applications of deep learning.  However, we will focus primarily upon data science.  For this class we will go beyond simple academic examples and see how to construct an ensemble that could potentially lead to a high score on a Kaggle competition.  We will see how to evaluate the importance of features and several ways to combine models.\n",
    "\n",
    "Tonights topics include:\n",
    "\n",
    "* Log Loss Error\n",
    "* Evaluating Feature Importance\n",
    "* The Biological Response Data Set\n",
    "* Neural Network Bagging\n",
    "* Nueral Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "These are exactly the same feature vector encoding functions from [Class 3](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class3_training.ipynb).  They must be defined for this class as well.  For more information, refer to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogLoss Error\n",
    "\n",
    "Log loss is an error metric that is often used in place of accuracy for classification.  Log loss allows for \"partial credit\" when a miss classification occurs.  For example, a model might be used to classify A, B and C.  The correct answer might be A, however if the classification network chose B as having the highest probability, then accuracy gives the neural network no credit for this classification.  \n",
    "\n",
    "However, with log loss, the probability of the correct answer is added to the score.  For example, the correct answer might be A, but if the neural network only predicted .8 probability of A being correct, then the value -log(.8) is added.\n",
    "\n",
    "$$ logloss = -\\frac{1}{N}\\sum^N_{i=1}\\sum^M_{j=1}y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "The following table shows the logloss scores that correspond to the average predicted accuracy for the correct item. The **pred** column specifies the average probability for the correct class.  The **logloss** column specifies the log loss for that probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.500000e-02</td>\n",
       "      <td>2.590267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.500000e-02</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>18.420681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred    logloss\n",
       "0   1.000000e+00  -0.000000\n",
       "1   9.000000e-01   0.105361\n",
       "2   8.000000e-01   0.223144\n",
       "3   7.000000e-01   0.356675\n",
       "4   6.000000e-01   0.510826\n",
       "5   5.000000e-01   0.693147\n",
       "6   4.000000e-01   0.916291\n",
       "7   3.000000e-01   1.203973\n",
       "8   2.000000e-01   1.609438\n",
       "9   1.000000e-01   2.302585\n",
       "10  7.500000e-02   2.590267\n",
       "11  5.000000e-02   2.995732\n",
       "12  2.500000e-02   3.688879\n",
       "13  1.000000e-08  18.420681"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.075, 0.05, 0.025, 1e-8 ]\n",
    "\n",
    "df = pd.DataFrame({'pred':loss, 'logloss': -np.log(loss)},columns=['pred','logloss'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the opposit.  For a given logloss, what is the average probability for the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logloss</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.904837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.740818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.548812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.496585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.449329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.406570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.223130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.049787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.030197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.018316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    logloss      pred\n",
       "0       0.1  0.904837\n",
       "1       0.2  0.818731\n",
       "2       0.3  0.740818\n",
       "3       0.4  0.670320\n",
       "4       0.5  0.606531\n",
       "5       0.6  0.548812\n",
       "6       0.7  0.496585\n",
       "7       0.8  0.449329\n",
       "8       0.9  0.406570\n",
       "9       1.0  0.367879\n",
       "10      1.5  0.223130\n",
       "11      2.0  0.135335\n",
       "12      2.5  0.082085\n",
       "13      3.0  0.049787\n",
       "14      3.5  0.030197\n",
       "15      4.0  0.018316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "loss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4 ]\n",
    "\n",
    "df = pd.DataFrame({'logloss':loss, 'pred': np.exp(np.negative(loss))},\n",
    "                  columns=['logloss','pred'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating Feature Importance\n",
    "\n",
    "Feature importance tells us how important each of the features (from the feature/import vector are to the prediction of a neural network, or other model.  There are many different ways to evaluate feature importance for neural networks.  The following paper presents a very good (and readable) overview of the various means of evaluating the importance of neural network inputs/features.\n",
    "\n",
    "Olden, J. D., Joy, M. K., & Death, R. G. (2004). [An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data](http://depts.washington.edu/oldenlab/wordpress/wp-content/uploads/2013/03/EcologicalModelling_2004.pdf). *Ecological Modelling*, 178(3), 389-397.\n",
    "\n",
    "In summary, the following methods are available to neural networks:\n",
    "\n",
    "* Connection Weights Algorithm\n",
    "* Partial Derivatives\n",
    "* Input Perturbation\n",
    "* Sensitivity Analysis\n",
    "* Forward Stepwise Addition \n",
    "* Improved Stepwise Selection 1\n",
    "* Backward Stepwise Elimination\n",
    "* Improved Stepwise Selection\n",
    "\n",
    "For this class we will use the **Input Perturbation** feature ranking algorithm.  This algorithm will work with any regression or classification network.  implementation of the input perturbation algorithm for scikit-learn is given in the next section. This algorithm is implemented in a function below that will work with any scikit-learn model.\n",
    "\n",
    "This algorithm was introduced by [Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) in his seminal paper on random forests.  Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.  This algorithm, known as the input perturbation algorithm, works by evaluating a trained model’s accuracy with each of the inputs individually shuffled from a data set.  Shuffling an input causes it to become useless—effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.  The TensorFlow version of this algorithm is taken from the following paper.\n",
    "\n",
    "Heaton, J., McElwee, S., & Cannady, J. (May 2017). Early stabilizing feature importance for TensorFlow deep neural networks. In *International Joint Conference on Neural Networks (IJCNN 2017)* (accepted for publication). IEEE.\n",
    "\n",
    "This algorithm will use logloss to evaluate a classification problem and RMSE for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00450: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff927d96cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "x,y = to_xy(df,\"species\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/38 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal_w</td>\n",
       "      <td>1.985183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal_l</td>\n",
       "      <td>1.769760</td>\n",
       "      <td>0.891485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepal_l</td>\n",
       "      <td>0.114387</td>\n",
       "      <td>0.057621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sepal_w</td>\n",
       "      <td>0.103801</td>\n",
       "      <td>0.052288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name     error  importance\n",
       "0  petal_w  1.985183    1.000000\n",
       "1  petal_l  1.769760    0.891485\n",
       "2  sepal_l  0.114387    0.057621\n",
       "3  sepal_w  0.103801    0.052288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"species\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Input Perturbation Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00183: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff9255d9c18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank MPG fields\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight</td>\n",
       "      <td>23.429295</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>17.563269</td>\n",
       "      <td>0.749629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>displacement</td>\n",
       "      <td>13.218753</td>\n",
       "      <td>0.564198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>origin-3</td>\n",
       "      <td>11.100803</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>10.933226</td>\n",
       "      <td>0.466648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>10.248572</td>\n",
       "      <td>0.437426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>origin-1</td>\n",
       "      <td>10.182336</td>\n",
       "      <td>0.434598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>origin-2</td>\n",
       "      <td>10.015005</td>\n",
       "      <td>0.427457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>9.928648</td>\n",
       "      <td>0.423771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      error  importance\n",
       "0        weight  23.429295    1.000000\n",
       "1          year  17.563269    0.749629\n",
       "2  displacement  13.218753    0.564198\n",
       "3      origin-3  11.100803    0.473800\n",
       "4    horsepower  10.933226    0.466648\n",
       "5  acceleration  10.248572    0.437426\n",
       "6      origin-1  10.182336    0.434598\n",
       "7      origin-2  10.015005    0.427457\n",
       "8     cylinders   9.928648    0.423771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df.columns) # x+y column names\n",
    "names.remove(\"mpg\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Biological Response Data Set\n",
    "\n",
    "* [Biological Response Dataset at Kaggle](https://www.kaggle.com/c/bioresponse)\n",
    "* [1st place interview for Boehringer Ingelheim Biological Response](http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"bio_train.csv\")\n",
    "filename_test = os.path.join(path,\"bio_test.csv\")\n",
    "filename_submit = os.path.join(path,\"bio_submit.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "activity_classes = encode_text_index(df_train,'Activity')\n",
    "\n",
    "#display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Response with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting/Training...\n",
      "Epoch 00009: early stopping\n",
      "Fitting done...\n",
      "Validation logloss: 0.6153108674929489\n",
      "Validation accuracy score: 0.7750533049040512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Encode feature vector\n",
    "x, y = to_xy(df_train,'Activity')\n",
    "x_submit = df_test.as_matrix().astype(np.float32)\n",
    "num_classes = len(activity_classes)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42) \n",
    "\n",
    "print(\"Fitting/Training...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "print(\"Fitting done...\")\n",
    "\n",
    "# Give logloss error\n",
    "pred = model.predict(x_test)\n",
    "pred2 = np.argmax(pred,axis=1)\n",
    "pred = pred[:,1]\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "print(\"Validation logloss: {}\".format(sklearn.metrics.log_loss(y_test,pred)))\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred_submit = pred.copy()\n",
    "y_test = y_test[:,1]\n",
    "score = metrics.accuracy_score(y_test, pred2)\n",
    "print(\"Validation accuracy score: {}\".format(score))\n",
    "\n",
    "# Build real submit file\n",
    "pred_submit = model.predict(x_submit)\n",
    "pred_submit = pred_submit[:,1]\n",
    "\n",
    "# Clip so that min is never exactly 0, max never 1\n",
    "pred = np.clip(pred,a_min=1e-6,a_max=(1-1e-6)) \n",
    "submit_df = pd.DataFrame({'MoleculeId':[x+1 for x in range(len(pred_submit))],'PredictedProbability':pred_submit})\n",
    "submit_df.to_csv(filename_submit, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Features/Columns are Important \n",
    "\n",
    "The following uses perturbation ranking to evaluate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/938 [>.............................] - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D27</td>\n",
       "      <td>0.652301</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1100</td>\n",
       "      <td>0.629362</td>\n",
       "      <td>0.964833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D958</td>\n",
       "      <td>0.629280</td>\n",
       "      <td>0.964708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1407</td>\n",
       "      <td>0.626536</td>\n",
       "      <td>0.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1203</td>\n",
       "      <td>0.625067</td>\n",
       "      <td>0.958248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D1094</td>\n",
       "      <td>0.624043</td>\n",
       "      <td>0.956678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D1059</td>\n",
       "      <td>0.623880</td>\n",
       "      <td>0.956429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D992</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.956306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D1008</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.956297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D1159</td>\n",
       "      <td>0.623476</td>\n",
       "      <td>0.955810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D1071</td>\n",
       "      <td>0.623314</td>\n",
       "      <td>0.955561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D1012</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.954174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D1026</td>\n",
       "      <td>0.622282</td>\n",
       "      <td>0.953979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D1153</td>\n",
       "      <td>0.621655</td>\n",
       "      <td>0.953019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D1009</td>\n",
       "      <td>0.621269</td>\n",
       "      <td>0.952426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D1188</td>\n",
       "      <td>0.621160</td>\n",
       "      <td>0.952259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D961</td>\n",
       "      <td>0.620997</td>\n",
       "      <td>0.952009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D1390</td>\n",
       "      <td>0.620717</td>\n",
       "      <td>0.951580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D1271</td>\n",
       "      <td>0.620710</td>\n",
       "      <td>0.951570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D1167</td>\n",
       "      <td>0.620677</td>\n",
       "      <td>0.951518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D1371</td>\n",
       "      <td>0.620646</td>\n",
       "      <td>0.951472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D1359</td>\n",
       "      <td>0.620460</td>\n",
       "      <td>0.951186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D1127</td>\n",
       "      <td>0.620345</td>\n",
       "      <td>0.951010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D1221</td>\n",
       "      <td>0.620292</td>\n",
       "      <td>0.950928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D974</td>\n",
       "      <td>0.620244</td>\n",
       "      <td>0.950854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D1140</td>\n",
       "      <td>0.620174</td>\n",
       "      <td>0.950747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D996</td>\n",
       "      <td>0.620122</td>\n",
       "      <td>0.950668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D1111</td>\n",
       "      <td>0.620074</td>\n",
       "      <td>0.950594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D1068</td>\n",
       "      <td>0.620044</td>\n",
       "      <td>0.950549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D1246</td>\n",
       "      <td>0.620025</td>\n",
       "      <td>0.950519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>D1073</td>\n",
       "      <td>0.612914</td>\n",
       "      <td>0.939618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>D1356</td>\n",
       "      <td>0.612832</td>\n",
       "      <td>0.939492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>D1607</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.939414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>D1334</td>\n",
       "      <td>0.612703</td>\n",
       "      <td>0.939295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>D1095</td>\n",
       "      <td>0.612653</td>\n",
       "      <td>0.939217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>D1016</td>\n",
       "      <td>0.612637</td>\n",
       "      <td>0.939193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>D1239</td>\n",
       "      <td>0.612617</td>\n",
       "      <td>0.939162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>D1290</td>\n",
       "      <td>0.612591</td>\n",
       "      <td>0.939122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>D1106</td>\n",
       "      <td>0.612362</td>\n",
       "      <td>0.938771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>D1205</td>\n",
       "      <td>0.612349</td>\n",
       "      <td>0.938752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>D1681</td>\n",
       "      <td>0.612193</td>\n",
       "      <td>0.938512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>D127</td>\n",
       "      <td>0.612164</td>\n",
       "      <td>0.938467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>D1261</td>\n",
       "      <td>0.612132</td>\n",
       "      <td>0.938419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>D1405</td>\n",
       "      <td>0.612131</td>\n",
       "      <td>0.938418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>D415</td>\n",
       "      <td>0.612116</td>\n",
       "      <td>0.938395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>D1385</td>\n",
       "      <td>0.612006</td>\n",
       "      <td>0.938226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>D1087</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.938124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>D1274</td>\n",
       "      <td>0.611661</td>\n",
       "      <td>0.937697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>D1415</td>\n",
       "      <td>0.611424</td>\n",
       "      <td>0.937334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>D985</td>\n",
       "      <td>0.611373</td>\n",
       "      <td>0.937255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>D1314</td>\n",
       "      <td>0.610700</td>\n",
       "      <td>0.936223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>D1454</td>\n",
       "      <td>0.610635</td>\n",
       "      <td>0.936124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>D1202</td>\n",
       "      <td>0.610477</td>\n",
       "      <td>0.935882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>D1400</td>\n",
       "      <td>0.610456</td>\n",
       "      <td>0.935849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>D1176</td>\n",
       "      <td>0.610408</td>\n",
       "      <td>0.935777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>D1195</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.935723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>D1281</td>\n",
       "      <td>0.610205</td>\n",
       "      <td>0.935465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>D997</td>\n",
       "      <td>0.610158</td>\n",
       "      <td>0.935393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>D1200</td>\n",
       "      <td>0.609203</td>\n",
       "      <td>0.933929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>D51</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.933061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name     error  importance\n",
       "0       D27  0.652301    1.000000\n",
       "1     D1100  0.629362    0.964833\n",
       "2      D958  0.629280    0.964708\n",
       "3     D1407  0.626536    0.960500\n",
       "4     D1203  0.625067    0.958248\n",
       "5     D1094  0.624043    0.956678\n",
       "6     D1059  0.623880    0.956429\n",
       "7      D992  0.623799    0.956306\n",
       "8     D1008  0.623794    0.956297\n",
       "9     D1159  0.623476    0.955810\n",
       "10    D1071  0.623314    0.955561\n",
       "11    D1012  0.622409    0.954174\n",
       "12    D1026  0.622282    0.953979\n",
       "13    D1153  0.621655    0.953019\n",
       "14    D1009  0.621269    0.952426\n",
       "15    D1188  0.621160    0.952259\n",
       "16     D961  0.620997    0.952009\n",
       "17    D1390  0.620717    0.951580\n",
       "18    D1271  0.620710    0.951570\n",
       "19    D1167  0.620677    0.951518\n",
       "20    D1371  0.620646    0.951472\n",
       "21    D1359  0.620460    0.951186\n",
       "22    D1127  0.620345    0.951010\n",
       "23    D1221  0.620292    0.950928\n",
       "24     D974  0.620244    0.950854\n",
       "25    D1140  0.620174    0.950747\n",
       "26     D996  0.620122    0.950668\n",
       "27    D1111  0.620074    0.950594\n",
       "28    D1068  0.620044    0.950549\n",
       "29    D1246  0.620025    0.950519\n",
       "...     ...       ...         ...\n",
       "1746  D1073  0.612914    0.939618\n",
       "1747  D1356  0.612832    0.939492\n",
       "1748  D1607  0.612781    0.939414\n",
       "1749  D1334  0.612703    0.939295\n",
       "1750  D1095  0.612653    0.939217\n",
       "1751  D1016  0.612637    0.939193\n",
       "1752  D1239  0.612617    0.939162\n",
       "1753  D1290  0.612591    0.939122\n",
       "1754  D1106  0.612362    0.938771\n",
       "1755  D1205  0.612349    0.938752\n",
       "1756  D1681  0.612193    0.938512\n",
       "1757   D127  0.612164    0.938467\n",
       "1758  D1261  0.612132    0.938419\n",
       "1759  D1405  0.612131    0.938418\n",
       "1760   D415  0.612116    0.938395\n",
       "1761  D1385  0.612006    0.938226\n",
       "1762  D1087  0.611940    0.938124\n",
       "1763  D1274  0.611661    0.937697\n",
       "1764  D1415  0.611424    0.937334\n",
       "1765   D985  0.611373    0.937255\n",
       "1766  D1314  0.610700    0.936223\n",
       "1767  D1454  0.610635    0.936124\n",
       "1768  D1202  0.610477    0.935882\n",
       "1769  D1400  0.610456    0.935849\n",
       "1770  D1176  0.610408    0.935777\n",
       "1771  D1195  0.610374    0.935723\n",
       "1772  D1281  0.610205    0.935465\n",
       "1773   D997  0.610158    0.935393\n",
       "1774  D1200  0.609203    0.933929\n",
       "1775    D51  0.608637    0.933061\n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank the features\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns) # x+y column names\n",
    "names.remove(\"Activity\") # remove the target(y)\n",
    "rank = perturbation_rank(model, x_test, y_test, names, False)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Ensemble\n",
    "\n",
    "A neural network ensemble combines neural network predictions with other models.  The exact blend of all of these models is determined by logistic regression.  The following code performs this blend for a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Model: 0 : <keras.wrappers.scikit_learn.KerasClassifier object at 0x7ff925396da0>\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6015     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4856     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4470     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4155     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3971     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3719     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3615     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3342     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3373     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.2998     \n",
      "2016/2501 [=======================>......] - ETA: 0sFold #0: loss=0.5763097287332929\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6164     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5101     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4710     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4397     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4184     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4128     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3961     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3786     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3715     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - ETA: 0s - loss: 0.357 - 0s - loss: 0.3602     \n",
      "2016/2501 [=======================>......] - ETA: 0sFold #1: loss=0.48613919533452843\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6392     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5304     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4717     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4369     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4185     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4018     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - ETA: 0s - loss: 0.370 - 0s - loss: 0.3829     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3690     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3527     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3389     \n",
      "2016/2501 [=======================>......] - ETA: 0sFold #2: loss=0.5927765394478075\n",
      "Epoch 1/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.6615     \n",
      "Epoch 2/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.5543     \n",
      "Epoch 3/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4889     \n",
      "Epoch 4/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4576     \n",
      "Epoch 5/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4323     \n",
      "Epoch 6/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4198     \n",
      "Epoch 7/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.4069     \n",
      "Epoch 8/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3958     \n",
      "Epoch 9/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3894     \n",
      "Epoch 10/10\n",
      "3375/3375 [==============================] - 0s - loss: 0.3842     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #3: loss=0.4929720141241314\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6086     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5142     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4557     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4320     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4031     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3891     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3739     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3618     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3396     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3358     \n",
      "1920/2501 [======================>.......] - ETA: 0sFold #4: loss=0.5765748214400738\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6355     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.5185     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4604     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4343     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4130     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4027     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3924     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3823     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3707     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3623     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #5: loss=0.6981074537144506\n",
      "Epoch 1/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.6050     \n",
      "Epoch 2/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4963     \n",
      "Epoch 3/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4499     \n",
      "Epoch 4/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.4151     \n",
      "Epoch 5/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3971     \n",
      "Epoch 6/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3730     \n",
      "Epoch 7/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3571     \n",
      "Epoch 8/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3455     \n",
      "Epoch 9/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3214     \n",
      "Epoch 10/10\n",
      "3376/3376 [==============================] - 0s - loss: 0.3138     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #6: loss=0.4883163010108354\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6229     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5173     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4652     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4447     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4200     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3996     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3903     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3740     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3637     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3481     \n",
      "1952/2501 [======================>.......] - ETA: 0sFold #7: loss=0.5105981384374014\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.5926     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4845     \n",
      "Epoch 3/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4393     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4188     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3893     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3721     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3450     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3405     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3224     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3036     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #8: loss=0.5304394062581672\n",
      "Epoch 1/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.6021     \n",
      "Epoch 2/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4874     \n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377/3377 [==============================] - 0s - loss: 0.4403     \n",
      "Epoch 4/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.4175     \n",
      "Epoch 5/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3885     \n",
      "Epoch 6/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3631     \n",
      "Epoch 7/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3460     \n",
      "Epoch 8/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3415     \n",
      "Epoch 9/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.3097     \n",
      "Epoch 10/10\n",
      "3377/3377 [==============================] - 0s - loss: 0.2957     \n",
      "1888/2501 [=====================>........] - ETA: 0sFold #9: loss=0.6146155536824807\n",
      "KerasClassifier: Mean loss=0.556684915218317\n",
      "Model: 1 : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Fold #0: loss=3.606678388314123\n",
      "Fold #1: loss=2.2197228940978317\n",
      "Fold #2: loss=3.6717523663107237\n",
      "Fold #3: loss=2.5045156203944594\n",
      "Fold #4: loss=4.443553550438037\n",
      "Fold #5: loss=4.410524301688227\n",
      "Fold #6: loss=3.400455469543658\n",
      "Fold #7: loss=3.0885474338547683\n",
      "Fold #8: loss=2.1219335323249253\n",
      "Fold #9: loss=3.0613772690497245\n",
      "KNeighborsClassifier: Mean loss=3.2529060826016476\n",
      "Model: 2 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.45859387026544035\n",
      "Fold #1: loss=0.43226994423427495\n",
      "Fold #2: loss=0.4762818884046444\n",
      "Fold #3: loss=0.43338810350051965\n",
      "Fold #4: loss=0.47502753371869\n",
      "Fold #5: loss=0.492658174605396\n",
      "Fold #6: loss=0.4015988631740329\n",
      "Fold #7: loss=0.4717953689427381\n",
      "Fold #8: loss=0.4517105069522408\n",
      "Fold #9: loss=0.46971357575792544\n",
      "RandomForestClassifier: Mean loss=0.45630378295559015\n",
      "Model: 3 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold #0: loss=0.4501929416405849\n",
      "Fold #1: loss=0.43527263084022183\n",
      "Fold #2: loss=0.4698774736941605\n",
      "Fold #3: loss=0.41784016293793536\n",
      "Fold #4: loss=0.470862719005535\n",
      "Fold #5: loss=0.5007009099421501\n",
      "Fold #6: loss=0.40496551263998337\n",
      "Fold #7: loss=0.4688833748405199\n",
      "Fold #8: loss=0.45798694087339165\n",
      "Fold #9: loss=0.45974118174298945\n",
      "RandomForestClassifier: Mean loss=0.45363238481574725\n",
      "Model: 4 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.45131247798132806\n",
      "Fold #1: loss=0.4230980001099943\n",
      "Fold #2: loss=0.586187893928544\n",
      "Fold #3: loss=0.42553475374872446\n",
      "Fold #4: loss=0.4964472681005012\n",
      "Fold #5: loss=0.494729595836481\n",
      "Fold #6: loss=0.41487748895885895\n",
      "Fold #7: loss=0.48506699974271766\n",
      "Fold #8: loss=0.4595462862215204\n",
      "Fold #9: loss=0.4673771268612419\n",
      "ExtraTreesClassifier: Mean loss=0.47041778914899124\n",
      "Model: 5 : ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold #0: loss=0.44916718203909234\n",
      "Fold #1: loss=0.4141554387911032\n",
      "Fold #2: loss=0.5897250522492203\n",
      "Fold #3: loss=0.4234716042493701\n",
      "Fold #4: loss=0.49267251836808157\n",
      "Fold #5: loss=0.5029294613500651\n",
      "Fold #6: loss=0.4138860223390099\n",
      "Fold #7: loss=0.6423108659854284\n",
      "Fold #8: loss=0.5382143341356395\n",
      "Fold #9: loss=0.6301460999525225\n",
      "ExtraTreesClassifier: Mean loss=0.5096678579459533\n",
      "Model: 6 : GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold #0: loss=0.48033546988804227\n",
      "Fold #1: loss=0.4659003726481674\n",
      "Fold #2: loss=0.4757100968118204\n",
      "Fold #3: loss=0.44494271863599\n",
      "Fold #4: loss=0.5016905324925709\n",
      "Fold #5: loss=0.4946939556794898\n",
      "Fold #6: loss=0.45002051472618254\n",
      "Fold #7: loss=0.4586389893553193\n",
      "Fold #8: loss=0.45452694136050165\n",
      "Fold #9: loss=0.47425905315384753\n",
      "GradientBoostingClassifier: Mean loss=0.4700718644751932\n",
      "\n",
      "Blending models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "PATH = \"./data/\"\n",
    "SHUFFLE = False\n",
    "FOLDS = 10\n",
    "\n",
    "def build_ann(input_size,classes,neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def mlogloss(y_test, preds):\n",
    "    epsilon = 1e-15\n",
    "    sum = 0\n",
    "    for row in zip(preds,y_test):\n",
    "        x = row[0][row[1]]\n",
    "        x = max(epsilon,x)\n",
    "        x = min(1-epsilon,x)\n",
    "        sum+=math.log(x)\n",
    "    return( (-1/len(preds))*sum)\n",
    "\n",
    "def stretch(y):\n",
    "    return (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "def blend_ensemble(x, y, x_submit):\n",
    "\n",
    "    folds = list(StratifiedKFold(y, FOLDS))\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "\n",
    "    models = [\n",
    "        KerasClassifier(build_fn=build_ann,neurons=20,input_size=x.shape[1],classes=2),\n",
    "        KNeighborsClassifier(n_neighbors=3),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
    "    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        print(\"Model: {} : {}\".format(j, model) )\n",
    "        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
    "        total_loss = 0\n",
    "        for i, (train, test) in enumerate(folds):\n",
    "            x_train = x[train]\n",
    "            y_train = y[train]\n",
    "            x_test = x[test]\n",
    "            y_test = y[test]\n",
    "            model.fit(x_train, y_train)\n",
    "            pred = np.array(model.predict_proba(x_test))\n",
    "            # pred = model.predict_proba(x_test)\n",
    "            dataset_blend_train[test, j] = pred[:, 1]\n",
    "            pred2 = np.array(model.predict_proba(x_submit))\n",
    "            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n",
    "            fold_sums[:, i] = pred2[:, 1]\n",
    "            loss = mlogloss(y_test, pred)\n",
    "            total_loss+=loss\n",
    "            print(\"Fold #{}: loss={}\".format(i,loss))\n",
    "        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n",
    "        dataset_blend_test[:, j] = fold_sums.mean(1)\n",
    "\n",
    "    print()\n",
    "    print(\"Blending models.\")\n",
    "    blend = LogisticRegression()\n",
    "    blend.fit(dataset_blend_train, y)\n",
    "    return blend.predict_proba(dataset_blend_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)  # seed to shuffle the train set\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    filename_train = os.path.join(PATH, \"bio_train.csv\")\n",
    "    df_train = pd.read_csv(filename_train, na_values=['NA', '?'])\n",
    "\n",
    "    filename_submit = os.path.join(PATH, \"bio_test.csv\")\n",
    "    df_submit = pd.read_csv(filename_submit, na_values=['NA', '?'])\n",
    "\n",
    "    predictors = list(df_train.columns.values)\n",
    "    predictors.remove('Activity')\n",
    "    x = df_train.as_matrix(predictors)\n",
    "    y = df_train['Activity']\n",
    "    x_submit = df_submit.as_matrix()\n",
    "\n",
    "    if SHUFFLE:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    submit_data = blend_ensemble(x, y, x_submit)\n",
    "    submit_data = stretch(submit_data)\n",
    "\n",
    "    ####################\n",
    "    # Build submit file\n",
    "    ####################\n",
    "    ids = [id+1 for id in range(submit_data.shape[0])]\n",
    "    submit_filename = os.path.join(PATH, \"bio_submit.csv\")\n",
    "    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n",
    "                             columns=['MoleculeId','PredictedProbability'])\n",
    "    submit_df.to_csv(submit_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

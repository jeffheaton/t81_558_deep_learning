{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Class 5: Backpropagation.**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions from Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "    \n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "# Plot an ROC. pred - the predictions, y - the expected output.\n",
    "def plot_roc(pred,y):\n",
    "    fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "# Plot a lift curve.  pred - the predictions, y - the expected output.\n",
    "def chart_regression(pred,y):\n",
    "    t = pd.DataFrame({'pred' : pred.flatten(), 'y' : y_test.flatten()})\n",
    "    t.sort_values(by=['y'],inplace=True)\n",
    "\n",
    "    plt.plot(t['y'].tolist(),label='expected')\n",
    "    plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classic Backpropagation\n",
    "Backpropagation is the primary means by which a neural network's weights are determined during training. Backpropagation works by calculating a weight change amount ($v_t$) for every weight($\\theta$) in the neural network.  This value is subtracted from every weight by the following equation: \n",
    "\n",
    "$ \\theta_t = \\theta_{t-1} - v_t $\n",
    "\n",
    "This process is repeated for every iteration($t$).  How the weight change is calculated depends on the training algorithm.  Classic backpropagation simply calculates a gradient ($\\nabla$) for every weight in the neural network with respect to the error function ($J$) of the neural network.  The gradient is scaled by a learning rate ($\\eta$).\n",
    "\n",
    "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) $\n",
    "\n",
    "The learning rate is an important concept for backpropagation training.  Setting the learning rate can be complex:\n",
    "\n",
    "* Too low of a learning rate will usually converge to a good solution; however, the process will be very slow.\n",
    "* Too high of a learning rate will either fail outright, or converge to a higher error than a better learning rate.\n",
    "\n",
    "Common values for learning rate are: 0.1, 0.01, 0.001, etc.\n",
    "\n",
    "Gradients:\n",
    "\n",
    "![Derivative](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_deriv.png \"Derivative\")\n",
    "\n",
    "The following link, from the book, shows how a simple [neural network is trained with backpropagation](http://www.heatonresearch.com/aifh/vol3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Backpropagation\n",
    "\n",
    "Momentum adds another term to the calculation of $v_t$:\n",
    "\n",
    "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) + \\lambda v_{t-1} $\n",
    "\n",
    "Like the learning rate, momentum adds another training parameter that scales the effect of momentum.  Momentum backpropagation has two training parameters: learning rate ($\\eta$) and momentum ($\\lambda$).  Momentum simply adds the scaled value of the previous weight change amount ($v_{t-1}$) to the current weight change amount($v_t$).\n",
    "\n",
    "This has the effect of adding additional force behind a direction a weight was moving.  This might allow the weight to escape a local minima:\n",
    "\n",
    "![Momentum](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_momentum.png \"Momentum\")\n",
    "\n",
    "A very common value for momentum is 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and Online Backpropagation\n",
    "\n",
    "How often should the weights of a neural network be updated?  Gradients can be calculated for a training set element.  These gradients can also be summed together into batches and the weights updated once per batch.\n",
    "\n",
    "* **Online Training** - Update the weights based on gradients calculated from a single training set element.\n",
    "* **Batch Training** - Update the weights based on the sum of the gradients over all training set elements.\n",
    "* **Batch Size** - Update the weights based on the sum of some batch size of training set elements.\n",
    "* **Mini-Batch Training** - The same as batch size, but with a very small batch size.  Mini-batches are very popular and they are often in the 32-64 element range.\n",
    "\n",
    "Because the batch size is smaller than the complete training set size, it may take several batches to make it completely through the training set.  You may have noticed TensorFlow reporting both steps and epochs when a neural network is trained:\n",
    "\n",
    "```\n",
    "Step #100, epoch #7, avg. train loss: 23.02969\n",
    "Step #200, epoch #15, avg. train loss: 2.67576\n",
    "Step #300, epoch #23, avg. train loss: 1.33839\n",
    "Step #400, epoch #30, avg. train loss: 0.86830\n",
    "Step #500, epoch #38, avg. train loss: 0.67166\n",
    "Step #600, epoch #46, avg. train loss: 0.54569\n",
    "Step #700, epoch #53, avg. train loss: 0.47544\n",
    "Step #800, epoch #61, avg. train loss: 0.39358\n",
    "Step #900, epoch #69, avg. train loss: 0.36052\n",
    "```\n",
    "\n",
    "* **Step/Iteration** - The number of batches that were processed.\n",
    "* **Epoch** - The number of times the complete training set was processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) is currently one of the most popular neural network training algorithms.  It works very similarly to Batch/Mini-Batch training, except that the batches are made up of a random set of training elements.\n",
    "\n",
    "This leads to a very irregular convergence in error during training:\n",
    "\n",
    "![SGD Error](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_sgd_error.png \"SGD Error\")\n",
    "[Image from Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Because the neural network is trained on a random sample of the complete training set each time, the error does not make a smooth transition downward.  However, the error usually does go down.\n",
    "\n",
    "Advantages to SGD include:\n",
    "\n",
    "* Computationally efficient.  Even with a very large training set, each training step can be relatively fast.\n",
    "* Decreases overfitting by focusing on only a portion of the training set each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Techniques\n",
    "\n",
    "One problem with simple backpropagation training algorithms is that they are highly sensative to learning rate and momentum.  This is difficult because:\n",
    "\n",
    "* Learning rate must be adjusted to a small enough level to train an accurate neural network.\n",
    "* Momentum must be large enough to overcome local minima, yet small enough to not destabilize the training.\n",
    "* A single learning rate/momentum is often not good enough for the entire training process. It is often useful to automatically decrease learning rate as the training progresses.\n",
    "* All weights share a single learning rate/momentum.\n",
    "\n",
    "Other training techniques:\n",
    "\n",
    "* **Resilient Propagation** - Use only the magnitude of the gradient and allow each neuron to learn at its own rate.  No need for learning rate/momentum; however, only works in full batch mode.\n",
    "* **Nesterov accelerated gradient** - Helps midigate the risk of choosing a bad mini-batch.\n",
    "* **Adagrad** - Allows an automatically decaying per-weight learning rate and momentum concept.\n",
    "* **Adadelta** - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
    "* **Non-Gradient Methods** - Non-gradient methods can *sometimes* be useful, though rarely outperform gradient-based backpropagation methods.  These include: [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), [Nelder Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), and [many more](https://en.wikipedia.org/wiki/Category:Optimization_algorithms_and_methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM Update\n",
    "\n",
    "ADAM is the first training algorithm you should try.  It is very effective.  Kingma and Ba (2014) introduced the Adam update rule that derives its name from the adaptive moment estimates that it uses.  Adam estimates the first (mean) and second (variance) moments to determine the weight corrections.  Adam begins with an exponentially decaying average of past gradients (m):\n",
    "\n",
    "$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $\n",
    "\n",
    "This average accomplishes a similar goal as classic momentum update; however, its value is calculated automatically based on the current gradient ($g_t$).  The update rule then calculates the second moment ($v_t$):\n",
    "\n",
    "$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $\n",
    "\n",
    "The values $m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively.  However, they will have a strong bias towards zero in the initial training cycles.  The first moment’s bias is corrected as follows.\n",
    "\n",
    "$ \\hat{m}_t = \\frac{m_t}{1-\\beta^t_1} $\n",
    "\n",
    "Similarly, the second moment is also corrected:\n",
    "\n",
    "$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $\n",
    "\n",
    "These bias-corrected first and second moment estimates are applied to the ultimate Adam update rule, as follows:\n",
    "\n",
    "$ \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\eta} \\hat{m}_t $\n",
    "\n",
    "Adam is very tolerant to initial learning rate (η) and other training parameters. Kingma and Ba (2014)  propose default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and 10-8 for $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods Compared\n",
    "\n",
    "The following image shows how each of these algorithms train (image credits: [author](Alec Radford), [where I found it](http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms) ):\n",
    "\n",
    "![Training Techniques](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/contours_evaluation_optimizers.gif \"Training Techniques\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Update Rule in Tensorflow\n",
    "\n",
    "TensorFlow allows the update rule to be set to one of:\n",
    "\n",
    "* Adagrad\n",
    "* **Adam**\n",
    "* Ftrl\n",
    "* Momentum\n",
    "* RMSProp\n",
    "* **SGD**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #50, epoch #5, avg. train loss: 20.34096, avg. val loss: 14.38684\n",
      "Step #100, epoch #10, avg. train loss: 8.21658, avg. val loss: 7.30104\n",
      "Step #150, epoch #15, avg. train loss: 6.48438, avg. val loss: 5.86963\n",
      "Step #200, epoch #20, avg. train loss: 5.58166, avg. val loss: 5.14109\n",
      "Step #250, epoch #25, avg. train loss: 4.89689, avg. val loss: 4.63683\n",
      "Step #300, epoch #30, avg. train loss: 4.64199, avg. val loss: 4.25890\n",
      "Step #350, epoch #35, avg. train loss: 4.39752, avg. val loss: 4.06373\n",
      "Step #400, epoch #40, avg. train loss: 3.93127, avg. val loss: 3.76228\n",
      "Step #450, epoch #45, avg. train loss: 3.74136, avg. val loss: 3.53878\n",
      "Step #500, epoch #50, avg. train loss: 3.46492, avg. val loss: 3.32482\n",
      "Step #550, epoch #55, avg. train loss: 3.28064, avg. val loss: 3.17043\n",
      "Step #600, epoch #60, avg. train loss: 3.13331, avg. val loss: 3.03592\n",
      "Step #650, epoch #65, avg. train loss: 3.01669, avg. val loss: 2.94601\n",
      "Step #700, epoch #70, avg. train loss: 2.92124, avg. val loss: 2.85489\n",
      "Step #750, epoch #75, avg. train loss: 2.85828, avg. val loss: 2.77217\n",
      "Step #800, epoch #80, avg. train loss: 2.74972, avg. val loss: 2.70176\n",
      "Step #850, epoch #85, avg. train loss: 2.62822, avg. val loss: 2.63710\n",
      "Step #900, epoch #90, avg. train loss: 2.64184, avg. val loss: 2.58683\n",
      "Step #950, epoch #95, avg. train loss: 2.57191, avg. val loss: 2.53547\n",
      "Step #1000, epoch #100, avg. train loss: 2.49394, avg. val loss: 2.47502\n",
      "Step #1050, epoch #105, avg. train loss: 2.44556, avg. val loss: 2.39774\n",
      "Step #1100, epoch #110, avg. train loss: 2.33057, avg. val loss: 2.32994\n",
      "Final score (RMSE): 1.7927926778793335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best step:\n",
      " step 907 with loss 1.4028416872024536\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZRS8WhiUEopFc//A/8sU933RuoPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d001ea898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "from numpy import arange\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'horsepower')\n",
    "encode_numeric_zscore(df, 'weight')\n",
    "encode_numeric_zscore(df, 'cylinders')\n",
    "encode_numeric_zscore(df, 'displacement')\n",
    "encode_numeric_zscore(df, 'acceleration')\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a deep neural network with 3 hidden layers of 50, 25, 10\n",
    "regressor = skflow.TensorFlowDNNRegressor(\n",
    "    hidden_units=[50, 25, 10], \n",
    "    batch_size = 32,\n",
    "    #momentum=0.9,\n",
    "    optimizer='SGD', \n",
    "    learning_rate=0.01,  \n",
    "    steps=5000)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = skflow.monitors.ValidationMonitor(x_test, y_test,\n",
    "    early_stopping_rounds=200, print_steps=50)\n",
    "\n",
    "# Fit/train neural network\n",
    "regressor.fit(x_train, y_train, monitor=early_stop)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "pred = regressor.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "# Plot the chart\n",
    "chart_regression(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

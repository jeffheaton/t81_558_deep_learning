{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZXh7dogJlHH"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_03_4_early_stop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKH1QxMuJlHK"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 3: Introduction to PyTorch**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwkbs9-gJlHL"
   },
   "source": [
    "# Module 3 Material\n",
    "\n",
    "* Part 3.1: Deep Learning and Neural Network Introduction [[Video]](https://www.youtube.com/watch?v=zYnI4iWRmpc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_1_neural_net.ipynb)\n",
    "* Part 3.2: Introduction to Tensorflow and PyTorch [[Video]](https://www.youtube.com/watch?v=PsE73jk55cE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_2_pytorch.ipynb)\n",
    "* Part 3.3: Saving and Loading a PyTorch Neural Network [[Video]](https://www.youtube.com/watch?v=-9QfbGM1qGw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_3_save_load.ipynb)\n",
    "* **Part 3.4: Early Stopping in PyTorch to Prevent Overfitting** [[Video]](https://www.youtube.com/watch?v=m1LNunuI2fk&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_4_early_stop.ipynb)\n",
    "* Part 3.5: Extracting Weights and Manual Calculation [[Video]](https://www.youtube.com/watch?v=7PWgx16kH8s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_5_weights.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovYF1H1ZJlHL"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed. We also initialize the PyTorch device to either GPU/MPS (if available) or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4wO3BiMJlHM",
    "outputId": "09134cf7-4fdd-442f-cf02-9cdb58d3edc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWo4ptCdJlHN"
   },
   "source": [
    "# Part 3.4: Early Stopping in Keras to Prevent Overfitting\n",
    "\n",
    "It can be difficult to determine how many epochs to cycle through to train a neural network. Overfitting will occur if you train the neural network for too many epochs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize, as demonstrated in Figure 3.OVER. \n",
    "\n",
    "**Figure 3.OVER: Training vs. Validation Error for Overfitting**\n",
    "![Training vs. Validation Error for Overfitting](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
    "\n",
    "It is important to segment the original dataset into several datasets:\n",
    "\n",
    "* **Training Set**\n",
    "* **Validation Set**\n",
    "* **Holdout Set**\n",
    "\n",
    "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
    "\n",
    "The first method is a training and validation set. We use the training data to train the neural network until the validation set no longer improves. This attempts to stop at a near-optimal training point. This method will only give accurate \"out of sample\" predictions for the validation set; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network. Figure 3.VAL demonstrates how we divide the dataset.\n",
    "\n",
    "**Figure 3.VAL: Training with a Validation Set**\n",
    "![Training with a Validation Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_1_train_val.png \"Training with a Validation Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfQxxVVs--7K"
   },
   "source": [
    "Because PyTorch does not include a built-in early stopping function, we must define one of our own. We will use the following **EarlyStopping** class throughout this course.\n",
    "\n",
    "We can provide several parameters to the **EarlyStopping** object: \n",
    "\n",
    "* **min_delta** This value should be kept small; it specifies the minimum change that should be considered an improvement. Setting it even smaller will not likely have a great deal of impact.\n",
    "* **patience** How long should the training wait for the validation error to improve?  \n",
    "* **restore_best_weights** You should usually set this to true, as it restores the weights to the values they were at when the validation set is the highest. \n",
    "\n",
    "We will now see an example of this class in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CAezCpVfOFAF"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrsobz8ZJlHO"
   },
   "source": [
    "## Early Stopping with Classification\n",
    "\n",
    "We will now see an example of classification training with early stopping. We will train the neural network until the error no longer improves on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Isrrl6hyJlHP",
    "outputId": "118b06c5-8757-449e-cd47-4e4c50aa9240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 0.9478541612625122, vloss: 1.001355, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 161.85it/s]\n",
      "Epoch: 2, tloss: 0.9206601977348328, vloss: 0.848633, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 198.77it/s]\n",
      "Epoch: 3, tloss: 0.8024201989173889, vloss: 0.774461, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 174.90it/s]\n",
      "Epoch: 4, tloss: 0.7728373408317566, vloss: 0.728952, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 177.88it/s]\n",
      "Epoch: 5, tloss: 0.6665185689926147, vloss: 0.682056, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 159.69it/s]\n",
      "Epoch: 6, tloss: 0.7093963623046875, vloss: 0.670011, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 167.05it/s]\n",
      "Epoch: 7, tloss: 0.6074371337890625, vloss: 0.618681, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 188.56it/s]\n",
      "Epoch: 8, tloss: 0.6469656825065613, vloss: 0.607962, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 180.12it/s]\n",
      "Epoch: 9, tloss: 0.581074595451355, vloss: 0.602975, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 186.09it/s]\n",
      "Epoch: 10, tloss: 0.6460404992103577, vloss: 0.624206, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 183.91it/s]\n",
      "Epoch: 11, tloss: 0.5670906901359558, vloss: 0.620319, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 199.55it/s]\n",
      "Epoch: 12, tloss: 0.5732506513595581, vloss: 0.598604, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 154.97it/s]\n",
      "Epoch: 13, tloss: 0.593787670135498, vloss: 0.612807, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 177.91it/s]\n",
      "Epoch: 14, tloss: 0.6080103516578674, vloss: 0.585451, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 166.22it/s]\n",
      "Epoch: 15, tloss: 0.5540326237678528, vloss: 0.607828, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 166.27it/s]\n",
      "Epoch: 16, tloss: 0.5536108016967773, vloss: 0.582929, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 189.46it/s]\n",
      "Epoch: 17, tloss: 0.5898925065994263, vloss: 0.612238, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 182.57it/s]\n",
      "Epoch: 18, tloss: 0.6796722412109375, vloss: 0.582940, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 183.51it/s]\n",
      "Epoch: 19, tloss: 0.6240145564079285, vloss: 0.581136, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 190.57it/s]\n",
      "Epoch: 20, tloss: 0.5622360110282898, vloss: 0.603108, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 189.04it/s]\n",
      "Epoch: 21, tloss: 0.6114561557769775, vloss: 0.593397, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 190.70it/s]\n",
      "Epoch: 22, tloss: 0.5602938532829285, vloss: 0.595952, EStop:[3/5]: 100%|██████████| 7/7 [00:00<00:00, 164.11it/s]\n",
      "Epoch: 23, tloss: 0.5564962029457092, vloss: 0.595305, EStop:[4/5]: 100%|██████████| 7/7 [00:00<00:00, 136.86it/s]\n",
      "Epoch: 24, tloss: 0.6151353120803833, vloss: 0.658613, EStop:[Stopped on 5]: 100%|██████████| 7/7 [00:00<00:00, 176.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.softmax(self.fc3(x))\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "y = le.fit_transform(df['species'])\n",
    "species = le.classes_\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n",
    "\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],len(species)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device))\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test)\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJCDY-FcP41U",
    "outputId": "b6ae1cd8-2366-4470-a9a5-c37bb889461a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.5811357498168945\n"
     ]
    }
   ],
   "source": [
    "pred = model(x_test)\n",
    "vloss = loss_fn(pred, y_test)\n",
    "print(f\"Loss = {vloss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATJhTzRjJlHQ"
   },
   "source": [
    "As you can see from above, we did not use the total number of requested epochs.  The neural network training stopped once the validation set no longer improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0iNHDxNJlHR",
    "outputId": "997d43bd-0351-4e66-f6fe-8ddd4e4b8c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = model(x_test)\n",
    "_, predict_classes = torch.max(pred, 1)\n",
    "correct = accuracy_score(y_test.cpu(),predict_classes.cpu())\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR03ea5QJlHS"
   },
   "source": [
    "## Early Stopping with Regression\n",
    "\n",
    "The following code demonstrates how we can apply early stopping to a regression problem.  The technique is similar to the early stopping for classification code that we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTuEcZE4JlHS",
    "outputId": "ab9be7d8-4ce3-4dd6-d207-1599e537837f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch: 1, tloss 124.45263671875:  95%|█████████▍| 18/19 [00:00<00:00, 173.22it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch: 1, tloss: 108.49400329589844, vloss: 104.442307, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 165.18it/s]\n",
      "Epoch: 2, tloss: 203.3367919921875, vloss: 114.063995, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 168.05it/s]\n",
      "Epoch: 3, tloss: 122.97042846679688, vloss: 107.160622, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 189.83it/s]\n",
      "Epoch: 4, tloss: 59.45405960083008, vloss: 100.068298, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 171.37it/s]\n",
      "Epoch: 5, tloss: 98.15242004394531, vloss: 99.504707, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 193.20it/s]\n",
      "Epoch: 6, tloss: 100.73394012451172, vloss: 113.081009, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 165.11it/s]\n",
      "Epoch: 7, tloss: 129.69395446777344, vloss: 104.281494, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 177.01it/s]\n",
      "Epoch: 8, tloss: 111.96381378173828, vloss: 95.910423, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 160.44it/s]\n",
      "Epoch: 9, tloss: 131.66673278808594, vloss: 92.468109, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 164.69it/s]\n",
      "Epoch: 10, tloss: 97.4975357055664, vloss: 93.595200, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 172.26it/s]\n",
      "Epoch: 11, tloss: 100.02388763427734, vloss: 89.051826, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 171.51it/s]\n",
      "Epoch: 12, tloss: 62.23514938354492, vloss: 87.470665, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 166.13it/s]\n",
      "Epoch: 13, tloss: 158.0749969482422, vloss: 82.528603, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 143.55it/s]\n",
      "Epoch: 14, tloss: 91.9535903930664, vloss: 97.458488, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 158.56it/s]\n",
      "Epoch: 15, tloss: 93.16546630859375, vloss: 79.409988, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 171.54it/s]\n",
      "Epoch: 16, tloss: 70.71858215332031, vloss: 76.206322, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 156.46it/s]\n",
      "Epoch: 17, tloss: 102.21485900878906, vloss: 80.470932, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 164.51it/s]\n",
      "Epoch: 18, tloss: 62.73160171508789, vloss: 84.353310, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 176.87it/s]\n",
      "Epoch: 19, tloss: 106.30236053466797, vloss: 70.861259, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 175.86it/s]\n",
      "Epoch: 20, tloss: 81.817138671875, vloss: 71.962715, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 188.80it/s]\n",
      "Epoch: 21, tloss: 41.81819152832031, vloss: 98.058807, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 192.64it/s]\n",
      "Epoch: 22, tloss: 151.48724365234375, vloss: 73.960365, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 163.10it/s]\n",
      "Epoch: 23, tloss: 56.182498931884766, vloss: 72.399513, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 196.41it/s]\n",
      "Epoch: 24, tloss: 87.37374877929688, vloss: 67.318016, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 161.53it/s]\n",
      "Epoch: 25, tloss: 82.52413177490234, vloss: 66.145233, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 192.87it/s]\n",
      "Epoch: 26, tloss: 106.68697357177734, vloss: 76.784302, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 158.94it/s]\n",
      "Epoch: 27, tloss: 25.88142204284668, vloss: 62.421524, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 150.07it/s]\n",
      "Epoch: 28, tloss: 71.9531021118164, vloss: 63.351372, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 166.65it/s]\n",
      "Epoch: 29, tloss: 166.8109588623047, vloss: 70.530334, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 151.54it/s]\n",
      "Epoch: 30, tloss: 197.7742156982422, vloss: 124.124771, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 176.86it/s]\n",
      "Epoch: 31, tloss: 57.51386642456055, vloss: 75.873375, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 167.87it/s]\n",
      "Epoch: 32, tloss: 81.78349304199219, vloss: 68.833748, EStop:[Stopped on 5]: 100%|██████████| 19/19 [00:00<00:00, 168.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        # We must define each of the layers.\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward pass, we must calculate all of the layers we \n",
    "        # previously defined.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Read the MPG dataset.\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
    "    na_values=['NA', '?'])\n",
    "\n",
    "cars = df['name']\n",
    "\n",
    "# Handle missing value\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "# Pandas to Numpy\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values # regression\n",
    "\n",
    "# Numpy to PyTorch\n",
    "x = torch.Tensor(x).float()\n",
    "y = torch.Tensor(y).float()\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.tensor(x_train,device=device,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,device=device,dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(x_test,device=device,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,device=device,dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],1).to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device))\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test)\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjvaHmp5JlHS"
   },
   "source": [
    "Finally, we evaluate the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bvqiX-AJlHS",
    "outputId": "4fbdb754-42b9-401a-ca5f-4d8d18a1cebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 8.885063171386719\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "pred = model(x_test)\n",
    "score = torch.sqrt(torch.nn.functional.mse_loss(pred.flatten(),y_test))\n",
    "print(f\"Final score (RMSE): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYi-h2LNXqoZ",
    "outputId": "e0936381-5476-49cb-e320-dd6a06c1844e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33.0000, 28.0000, 19.0000, 13.0000, 14.0000, 27.0000, 24.0000, 13.0000,\n",
       "        17.0000, 21.0000, 15.0000, 38.0000, 26.0000, 15.0000, 25.0000, 12.0000,\n",
       "        31.0000, 17.0000, 16.0000, 31.0000, 22.0000, 22.0000, 22.0000, 33.5000,\n",
       "        18.0000, 44.0000, 26.0000, 24.5000, 18.1000, 12.0000, 27.0000, 36.0000,\n",
       "        23.0000, 24.0000, 37.2000, 16.0000, 21.0000, 19.2000, 16.0000, 29.0000,\n",
       "        26.8000, 27.0000, 18.0000, 10.0000, 23.0000, 36.0000, 26.0000, 25.0000,\n",
       "        25.0000, 25.0000, 22.0000, 34.1000, 32.4000, 13.0000, 23.5000, 14.0000,\n",
       "        18.5000, 29.8000, 28.0000, 19.0000, 11.0000, 33.0000, 23.0000, 21.0000,\n",
       "        23.0000, 25.0000, 23.8000, 34.4000, 24.5000, 13.0000, 34.7000, 14.0000,\n",
       "        15.0000, 18.0000, 25.0000, 19.9000, 17.5000, 28.0000, 29.0000, 17.0000,\n",
       "        16.0000, 27.0000, 37.0000, 36.1000, 23.0000, 14.0000, 32.8000, 29.9000,\n",
       "        20.0000, 12.0000, 15.5000, 23.7000, 24.0000, 36.0000, 19.0000, 38.0000,\n",
       "        29.0000, 21.5000, 27.9000, 14.0000], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dqUztPo3JlHT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "new22 t81_558_class_03_4_early_stop.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

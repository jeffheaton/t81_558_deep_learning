{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 12: Deep Learning and Security**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 Video Material\n",
    "\n",
    "* Part 12.1: Introduction to the OpenAI Gym [[Video]](https://www.youtube.com/watch?v=_KbUxgyisjM&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_01_ai_gym.ipynb)\n",
    "* Part 12.2: Introduction to Q-Learning [[Video]](https://www.youtube.com/watch?v=uwcXWe_Fra0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_02_qlearningreinforcement.ipynb)\n",
    "* **Part 12.3: Keras Q-Learning in the OpenAI Gym** [[Video]](https://www.youtube.com/watch?v=Ya1gYt63o3M&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_03_keras_reinforce.ipynb)\n",
    "* Part 12.4: Atari Games with Keras Neural Networks [[Video]](https://www.youtube.com/watch?v=t2yIu6cRa38&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_04_atari.ipynb)\n",
    "* Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess [[Video]](https://www.youtube.com/watch?v=ikDgyD7nVI8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_12_05_alpha_zero.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12.3: Keras Q-Learning in the OpenAI Gym\n",
    "\n",
    "![Deep Q-Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/deepqlearning.png \"Reinforcement Learning\")\n",
    "\n",
    "* **CEMAgent**\n",
    "    * **model** - The neural network that will be trained.\n",
    "    * **nb_actions** - The number of actions the agent can take (e.g. up, down, left, right, fire)\n",
    "    * **memory** - The EpisodeParameterMemory object to use.  This object observes and save all of the state transitions so that you can train your network on them later on (instead of having to make observations from the environment all the time).\n",
    "    * **batch_size** - The batch size for neural network training, same concept as deep learning batch sizes.\n",
    "    * **nb_steps_warmup** - Number of training steps to pass before any learning occurs.\n",
    "    * **train_interval** - Logging interval, defines how often to log.\n",
    "    * **elite_frac**\n",
    "* **CEMAgent.fit**\n",
    "    * **env** - The OpenAI gym environment being used.\n",
    "    * **nb_steps** - Number of training steps to be performed.\n",
    "    * **visualize** - If `True`, the environment is visualized during training. However,\n",
    "                this is likely going to slow down training significantly and is thus intended to be\n",
    "                a debugging instrument.\n",
    "    * **verbose** - 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "    28/100000: episode: 1, duration: 0.049s, episode steps:  28, steps per second: 567, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "    41/100000: episode: 2, duration: 0.008s, episode steps:  13, steps per second: 1537, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "    58/100000: episode: 3, duration: 0.011s, episode steps:  17, steps per second: 1543, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "    69/100000: episode: 4, duration: 0.007s, episode steps:  11, steps per second: 1488, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  mean_best_reward: --\n",
      "    91/100000: episode: 5, duration: 0.013s, episode steps:  22, steps per second: 1690, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "   120/100000: episode: 6, duration: 0.017s, episode steps:  29, steps per second: 1682, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "   134/100000: episode: 7, duration: 0.010s, episode steps:  14, steps per second: 1435, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "   150/100000: episode: 8, duration: 0.010s, episode steps:  16, steps per second: 1683, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "   175/100000: episode: 9, duration: 0.015s, episode steps:  25, steps per second: 1724, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  mean_best_reward: --\n",
      "   234/100000: episode: 10, duration: 0.038s, episode steps:  59, steps per second: 1560, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      "   245/100000: episode: 11, duration: 0.007s, episode steps:  11, steps per second: 1470, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "   267/100000: episode: 12, duration: 0.016s, episode steps:  22, steps per second: 1373, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "   285/100000: episode: 13, duration: 0.012s, episode steps:  18, steps per second: 1515, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  mean_best_reward: --\n",
      "   302/100000: episode: 14, duration: 0.013s, episode steps:  17, steps per second: 1332, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "   340/100000: episode: 15, duration: 0.024s, episode steps:  38, steps per second: 1558, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  mean_best_reward: --\n",
      "   353/100000: episode: 16, duration: 0.009s, episode steps:  13, steps per second: 1463, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  mean_best_reward: --\n",
      "   372/100000: episode: 17, duration: 0.011s, episode steps:  19, steps per second: 1770, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  mean_best_reward: --\n",
      "   386/100000: episode: 18, duration: 0.010s, episode steps:  14, steps per second: 1392, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "   399/100000: episode: 19, duration: 0.007s, episode steps:  13, steps per second: 1822, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "   443/100000: episode: 20, duration: 0.028s, episode steps:  44, steps per second: 1575, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   477/100000: episode: 21, duration: 0.020s, episode steps:  34, steps per second: 1702, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "   496/100000: episode: 22, duration: 0.012s, episode steps:  19, steps per second: 1565, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "   507/100000: episode: 23, duration: 0.007s, episode steps:  11, steps per second: 1505, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "   520/100000: episode: 24, duration: 0.009s, episode steps:  13, steps per second: 1503, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  mean_best_reward: --\n",
      "   532/100000: episode: 25, duration: 0.008s, episode steps:  12, steps per second: 1537, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n",
      "   544/100000: episode: 26, duration: 0.009s, episode steps:  12, steps per second: 1406, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   578/100000: episode: 27, duration: 0.020s, episode steps:  34, steps per second: 1710, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "   595/100000: episode: 28, duration: 0.012s, episode steps:  17, steps per second: 1468, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "   605/100000: episode: 29, duration: 0.008s, episode steps:  10, steps per second: 1317, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "   617/100000: episode: 30, duration: 0.010s, episode steps:  12, steps per second: 1256, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  mean_best_reward: --\n",
      "   630/100000: episode: 31, duration: 0.009s, episode steps:  13, steps per second: 1482, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      "   649/100000: episode: 32, duration: 0.010s, episode steps:  19, steps per second: 1892, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "   659/100000: episode: 33, duration: 0.008s, episode steps:  10, steps per second: 1328, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "   674/100000: episode: 34, duration: 0.009s, episode steps:  15, steps per second: 1683, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  mean_best_reward: --\n",
      "   696/100000: episode: 35, duration: 0.012s, episode steps:  22, steps per second: 1847, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  mean_best_reward: --\n",
      "   708/100000: episode: 36, duration: 0.008s, episode steps:  12, steps per second: 1423, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "   725/100000: episode: 37, duration: 0.010s, episode steps:  17, steps per second: 1766, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "   739/100000: episode: 38, duration: 0.008s, episode steps:  14, steps per second: 1684, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "   764/100000: episode: 39, duration: 0.013s, episode steps:  25, steps per second: 1957, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "   776/100000: episode: 40, duration: 0.008s, episode steps:  12, steps per second: 1534, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n",
      "   796/100000: episode: 41, duration: 0.012s, episode steps:  20, steps per second: 1705, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  mean_best_reward: --\n",
      "   818/100000: episode: 42, duration: 0.013s, episode steps:  22, steps per second: 1641, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "   838/100000: episode: 43, duration: 0.011s, episode steps:  20, steps per second: 1790, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "   852/100000: episode: 44, duration: 0.008s, episode steps:  14, steps per second: 1736, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   883/100000: episode: 45, duration: 0.018s, episode steps:  31, steps per second: 1678, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      "   902/100000: episode: 46, duration: 0.015s, episode steps:  19, steps per second: 1245, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      "   917/100000: episode: 47, duration: 0.011s, episode steps:  15, steps per second: 1343, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "   945/100000: episode: 48, duration: 0.016s, episode steps:  28, steps per second: 1728, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.679 [0.000, 1.000],  mean_best_reward: --\n",
      "   974/100000: episode: 49, duration: 0.018s, episode steps:  29, steps per second: 1607, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "  1002/100000: episode: 50, duration: 0.016s, episode steps:  28, steps per second: 1754, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  1024/100000: episode: 51, duration: 0.011s, episode steps:  22, steps per second: 1970, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  1038/100000: episode: 52, duration: 0.009s, episode steps:  14, steps per second: 1583, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  1075/100000: episode: 53, duration: 0.021s, episode steps:  37, steps per second: 1751, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      "  1094/100000: episode: 54, duration: 0.011s, episode steps:  19, steps per second: 1665, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  1110/100000: episode: 55, duration: 0.010s, episode steps:  16, steps per second: 1642, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  1124/100000: episode: 56, duration: 0.009s, episode steps:  14, steps per second: 1556, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  1139/100000: episode: 57, duration: 0.009s, episode steps:  15, steps per second: 1678, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  1149/100000: episode: 58, duration: 0.007s, episode steps:  10, steps per second: 1363, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  1168/100000: episode: 59, duration: 0.011s, episode steps:  19, steps per second: 1676, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      "  1184/100000: episode: 60, duration: 0.010s, episode steps:  16, steps per second: 1533, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      "  1205/100000: episode: 61, duration: 0.013s, episode steps:  21, steps per second: 1623, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  1219/100000: episode: 62, duration: 0.009s, episode steps:  14, steps per second: 1490, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  1238/100000: episode: 63, duration: 0.013s, episode steps:  19, steps per second: 1432, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  1267/100000: episode: 64, duration: 0.018s, episode steps:  29, steps per second: 1576, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  1280/100000: episode: 65, duration: 0.008s, episode steps:  13, steps per second: 1698, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  1289/100000: episode: 66, duration: 0.006s, episode steps:   9, steps per second: 1407, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  mean_best_reward: --\n",
      "  1300/100000: episode: 67, duration: 0.007s, episode steps:  11, steps per second: 1612, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  1314/100000: episode: 68, duration: 0.008s, episode steps:  14, steps per second: 1728, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  1336/100000: episode: 69, duration: 0.013s, episode steps:  22, steps per second: 1645, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      "  1372/100000: episode: 70, duration: 0.021s, episode steps:  36, steps per second: 1707, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      "  1389/100000: episode: 71, duration: 0.010s, episode steps:  17, steps per second: 1666, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  1400/100000: episode: 72, duration: 0.008s, episode steps:  11, steps per second: 1322, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "  1414/100000: episode: 73, duration: 0.008s, episode steps:  14, steps per second: 1754, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  1427/100000: episode: 74, duration: 0.009s, episode steps:  13, steps per second: 1378, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  1441/100000: episode: 75, duration: 0.008s, episode steps:  14, steps per second: 1791, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  1460/100000: episode: 76, duration: 0.012s, episode steps:  19, steps per second: 1638, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      "  1503/100000: episode: 77, duration: 0.025s, episode steps:  43, steps per second: 1718, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      "  1512/100000: episode: 78, duration: 0.006s, episode steps:   9, steps per second: 1542, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      "  1523/100000: episode: 79, duration: 0.008s, episode steps:  11, steps per second: 1441, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  1538/100000: episode: 80, duration: 0.011s, episode steps:  15, steps per second: 1386, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  1547/100000: episode: 81, duration: 0.006s, episode steps:   9, steps per second: 1630, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  1569/100000: episode: 82, duration: 0.014s, episode steps:  22, steps per second: 1623, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      "  1582/100000: episode: 83, duration: 0.010s, episode steps:  13, steps per second: 1289, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  1593/100000: episode: 84, duration: 0.007s, episode steps:  11, steps per second: 1534, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      "  1610/100000: episode: 85, duration: 0.012s, episode steps:  17, steps per second: 1447, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      "  1620/100000: episode: 86, duration: 0.007s, episode steps:  10, steps per second: 1418, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  mean_best_reward: --\n",
      "  1637/100000: episode: 87, duration: 0.011s, episode steps:  17, steps per second: 1504, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  mean_best_reward: --\n",
      "  1676/100000: episode: 88, duration: 0.023s, episode steps:  39, steps per second: 1672, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  mean_best_reward: --\n",
      "  1688/100000: episode: 89, duration: 0.007s, episode steps:  12, steps per second: 1779, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  1714/100000: episode: 90, duration: 0.015s, episode steps:  26, steps per second: 1699, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "  1726/100000: episode: 91, duration: 0.007s, episode steps:  12, steps per second: 1662, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  1742/100000: episode: 92, duration: 0.009s, episode steps:  16, steps per second: 1735, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      "  1754/100000: episode: 93, duration: 0.009s, episode steps:  12, steps per second: 1347, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  1777/100000: episode: 94, duration: 0.012s, episode steps:  23, steps per second: 1841, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      "  1788/100000: episode: 95, duration: 0.008s, episode steps:  11, steps per second: 1462, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "  1800/100000: episode: 96, duration: 0.008s, episode steps:  12, steps per second: 1575, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1848/100000: episode: 97, duration: 0.028s, episode steps:  48, steps per second: 1735, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      "  1866/100000: episode: 98, duration: 0.012s, episode steps:  18, steps per second: 1549, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  1882/100000: episode: 99, duration: 0.011s, episode steps:  16, steps per second: 1498, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      "  1897/100000: episode: 100, duration: 0.011s, episode steps:  15, steps per second: 1414, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.133 [0.000, 1.000],  mean_best_reward: --\n",
      "  1910/100000: episode: 101, duration: 0.009s, episode steps:  13, steps per second: 1490, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  1953/100000: episode: 102, duration: 0.024s, episode steps:  43, steps per second: 1816, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.628 [0.000, 1.000],  mean_best_reward: --\n",
      "  1965/100000: episode: 103, duration: 0.008s, episode steps:  12, steps per second: 1592, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  1977/100000: episode: 104, duration: 0.007s, episode steps:  12, steps per second: 1656, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  1990/100000: episode: 105, duration: 0.007s, episode steps:  13, steps per second: 1771, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      "  2004/100000: episode: 106, duration: 0.009s, episode steps:  14, steps per second: 1506, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2014/100000: episode: 107, duration: 0.006s, episode steps:  10, steps per second: 1671, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  mean_best_reward: --\n",
      "  2066/100000: episode: 108, duration: 0.028s, episode steps:  52, steps per second: 1853, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  2074/100000: episode: 109, duration: 0.005s, episode steps:   8, steps per second: 1570, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  2085/100000: episode: 110, duration: 0.007s, episode steps:  11, steps per second: 1472, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "  2099/100000: episode: 111, duration: 0.009s, episode steps:  14, steps per second: 1586, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  2120/100000: episode: 112, duration: 0.012s, episode steps:  21, steps per second: 1741, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2143/100000: episode: 113, duration: 0.013s, episode steps:  23, steps per second: 1745, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      "  2183/100000: episode: 114, duration: 0.023s, episode steps:  40, steps per second: 1757, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2204/100000: episode: 115, duration: 0.014s, episode steps:  21, steps per second: 1525, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  2229/100000: episode: 116, duration: 0.015s, episode steps:  25, steps per second: 1723, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      "  2255/100000: episode: 117, duration: 0.015s, episode steps:  26, steps per second: 1754, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      "  2274/100000: episode: 118, duration: 0.012s, episode steps:  19, steps per second: 1622, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  2297/100000: episode: 119, duration: 0.013s, episode steps:  23, steps per second: 1743, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  mean_best_reward: --\n",
      "  2315/100000: episode: 120, duration: 0.010s, episode steps:  18, steps per second: 1787, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  mean_best_reward: --\n",
      "  2325/100000: episode: 121, duration: 0.007s, episode steps:  10, steps per second: 1457, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "  2343/100000: episode: 122, duration: 0.010s, episode steps:  18, steps per second: 1829, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  2357/100000: episode: 123, duration: 0.009s, episode steps:  14, steps per second: 1525, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  2368/100000: episode: 124, duration: 0.006s, episode steps:  11, steps per second: 1735, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  2380/100000: episode: 125, duration: 0.008s, episode steps:  12, steps per second: 1530, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  2416/100000: episode: 126, duration: 0.020s, episode steps:  36, steps per second: 1804, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  2438/100000: episode: 127, duration: 0.012s, episode steps:  22, steps per second: 1827, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2457/100000: episode: 128, duration: 0.010s, episode steps:  19, steps per second: 1823, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  2481/100000: episode: 129, duration: 0.014s, episode steps:  24, steps per second: 1735, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2492/100000: episode: 130, duration: 0.007s, episode steps:  11, steps per second: 1553, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      "  2507/100000: episode: 131, duration: 0.010s, episode steps:  15, steps per second: 1503, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  2535/100000: episode: 132, duration: 0.019s, episode steps:  28, steps per second: 1510, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2553/100000: episode: 133, duration: 0.013s, episode steps:  18, steps per second: 1394, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  2565/100000: episode: 134, duration: 0.008s, episode steps:  12, steps per second: 1429, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  2594/100000: episode: 135, duration: 0.017s, episode steps:  29, steps per second: 1754, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      "  2616/100000: episode: 136, duration: 0.011s, episode steps:  22, steps per second: 1995, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "  2632/100000: episode: 137, duration: 0.009s, episode steps:  16, steps per second: 1840, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  2643/100000: episode: 138, duration: 0.007s, episode steps:  11, steps per second: 1628, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  2655/100000: episode: 139, duration: 0.007s, episode steps:  12, steps per second: 1701, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  2679/100000: episode: 140, duration: 0.012s, episode steps:  24, steps per second: 1942, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      "  2702/100000: episode: 141, duration: 0.012s, episode steps:  23, steps per second: 1981, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  mean_best_reward: --\n",
      "  2711/100000: episode: 142, duration: 0.006s, episode steps:   9, steps per second: 1464, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      "  2746/100000: episode: 143, duration: 0.017s, episode steps:  35, steps per second: 2061, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.686 [0.000, 1.000],  mean_best_reward: --\n",
      "  2759/100000: episode: 144, duration: 0.008s, episode steps:  13, steps per second: 1598, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  2772/100000: episode: 145, duration: 0.007s, episode steps:  13, steps per second: 1743, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  mean_best_reward: --\n",
      "  2874/100000: episode: 146, duration: 0.049s, episode steps: 102, steps per second: 2072, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2912/100000: episode: 147, duration: 0.020s, episode steps:  38, steps per second: 1899, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      "  2925/100000: episode: 148, duration: 0.009s, episode steps:  13, steps per second: 1482, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  3000/100000: episode: 149, duration: 0.036s, episode steps:  75, steps per second: 2068, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  3037/100000: episode: 150, duration: 0.019s, episode steps:  37, steps per second: 1985, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.351 [0.000, 1.000],  mean_best_reward: --\n",
      "  3054/100000: episode: 151, duration: 0.010s, episode steps:  17, steps per second: 1648, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  mean_best_reward: 45.500000\n",
      "  3067/100000: episode: 152, duration: 0.008s, episode steps:  13, steps per second: 1713, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  3097/100000: episode: 153, duration: 0.015s, episode steps:  30, steps per second: 2020, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.633 [0.000, 1.000],  mean_best_reward: --\n",
      "  3114/100000: episode: 154, duration: 0.009s, episode steps:  17, steps per second: 1957, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      "  3132/100000: episode: 155, duration: 0.010s, episode steps:  18, steps per second: 1752, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      "  3151/100000: episode: 156, duration: 0.011s, episode steps:  19, steps per second: 1733, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  3165/100000: episode: 157, duration: 0.008s, episode steps:  14, steps per second: 1803, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  3184/100000: episode: 158, duration: 0.010s, episode steps:  19, steps per second: 1840, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  3198/100000: episode: 159, duration: 0.007s, episode steps:  14, steps per second: 1937, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  3208/100000: episode: 160, duration: 0.007s, episode steps:  10, steps per second: 1476, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  mean_best_reward: --\n",
      "  3239/100000: episode: 161, duration: 0.015s, episode steps:  31, steps per second: 2021, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.677 [0.000, 1.000],  mean_best_reward: --\n",
      "  3270/100000: episode: 162, duration: 0.015s, episode steps:  31, steps per second: 2027, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      "  3293/100000: episode: 163, duration: 0.013s, episode steps:  23, steps per second: 1731, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      "  3313/100000: episode: 164, duration: 0.012s, episode steps:  20, steps per second: 1689, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  3324/100000: episode: 165, duration: 0.007s, episode steps:  11, steps per second: 1616, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  mean_best_reward: --\n",
      "  3336/100000: episode: 166, duration: 0.007s, episode steps:  12, steps per second: 1689, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  3355/100000: episode: 167, duration: 0.010s, episode steps:  19, steps per second: 1876, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      "  3382/100000: episode: 168, duration: 0.014s, episode steps:  27, steps per second: 1942, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  3395/100000: episode: 169, duration: 0.007s, episode steps:  13, steps per second: 1756, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  3406/100000: episode: 170, duration: 0.006s, episode steps:  11, steps per second: 1738, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "  3431/100000: episode: 171, duration: 0.013s, episode steps:  25, steps per second: 1880, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      "  3468/100000: episode: 172, duration: 0.018s, episode steps:  37, steps per second: 2027, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  3487/100000: episode: 173, duration: 0.010s, episode steps:  19, steps per second: 1856, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      "  3501/100000: episode: 174, duration: 0.008s, episode steps:  14, steps per second: 1788, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  3536/100000: episode: 175, duration: 0.018s, episode steps:  35, steps per second: 1981, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  3558/100000: episode: 176, duration: 0.014s, episode steps:  22, steps per second: 1625, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      "  3570/100000: episode: 177, duration: 0.008s, episode steps:  12, steps per second: 1479, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      "  3582/100000: episode: 178, duration: 0.008s, episode steps:  12, steps per second: 1490, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n",
      "  3597/100000: episode: 179, duration: 0.009s, episode steps:  15, steps per second: 1580, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  3605/100000: episode: 180, duration: 0.005s, episode steps:   8, steps per second: 1555, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  mean_best_reward: --\n",
      "  3619/100000: episode: 181, duration: 0.008s, episode steps:  14, steps per second: 1682, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      "  3631/100000: episode: 182, duration: 0.007s, episode steps:  12, steps per second: 1804, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  3641/100000: episode: 183, duration: 0.007s, episode steps:  10, steps per second: 1461, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      "  3650/100000: episode: 184, duration: 0.006s, episode steps:   9, steps per second: 1487, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  mean_best_reward: --\n",
      "  3668/100000: episode: 185, duration: 0.011s, episode steps:  18, steps per second: 1604, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  3681/100000: episode: 186, duration: 0.008s, episode steps:  13, steps per second: 1698, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  3701/100000: episode: 187, duration: 0.011s, episode steps:  20, steps per second: 1876, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  3722/100000: episode: 188, duration: 0.011s, episode steps:  21, steps per second: 1992, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  mean_best_reward: --\n",
      "  3736/100000: episode: 189, duration: 0.008s, episode steps:  14, steps per second: 1828, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  3761/100000: episode: 190, duration: 0.013s, episode steps:  25, steps per second: 1931, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      "  3779/100000: episode: 191, duration: 0.009s, episode steps:  18, steps per second: 1980, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      "  3826/100000: episode: 192, duration: 0.024s, episode steps:  47, steps per second: 1999, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      "  3838/100000: episode: 193, duration: 0.007s, episode steps:  12, steps per second: 1775, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  3861/100000: episode: 194, duration: 0.013s, episode steps:  23, steps per second: 1832, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  mean_best_reward: --\n",
      "  3876/100000: episode: 195, duration: 0.008s, episode steps:  15, steps per second: 1872, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  3886/100000: episode: 196, duration: 0.006s, episode steps:  10, steps per second: 1661, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  mean_best_reward: --\n",
      "  3906/100000: episode: 197, duration: 0.010s, episode steps:  20, steps per second: 1939, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      "  3919/100000: episode: 198, duration: 0.008s, episode steps:  13, steps per second: 1671, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  3937/100000: episode: 199, duration: 0.009s, episode steps:  18, steps per second: 2006, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  3952/100000: episode: 200, duration: 0.009s, episode steps:  15, steps per second: 1672, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  3969/100000: episode: 201, duration: 0.010s, episode steps:  17, steps per second: 1747, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: 77.000000\n",
      "  3983/100000: episode: 202, duration: 0.008s, episode steps:  14, steps per second: 1817, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  4000/100000: episode: 203, duration: 0.009s, episode steps:  17, steps per second: 1824, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4012/100000: episode: 204, duration: 0.008s, episode steps:  12, steps per second: 1490, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  4030/100000: episode: 205, duration: 0.011s, episode steps:  18, steps per second: 1664, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  4043/100000: episode: 206, duration: 0.007s, episode steps:  13, steps per second: 1839, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  4061/100000: episode: 207, duration: 0.011s, episode steps:  18, steps per second: 1695, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4111/100000: episode: 208, duration: 0.026s, episode steps:  50, steps per second: 1951, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      "  4124/100000: episode: 209, duration: 0.007s, episode steps:  13, steps per second: 1776, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  4168/100000: episode: 210, duration: 0.021s, episode steps:  44, steps per second: 2072, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      "  4196/100000: episode: 211, duration: 0.015s, episode steps:  28, steps per second: 1891, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4209/100000: episode: 212, duration: 0.008s, episode steps:  13, steps per second: 1717, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  4226/100000: episode: 213, duration: 0.009s, episode steps:  17, steps per second: 1807, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "  4266/100000: episode: 214, duration: 0.020s, episode steps:  40, steps per second: 1992, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      "  4329/100000: episode: 215, duration: 0.032s, episode steps:  63, steps per second: 1988, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      "  4338/100000: episode: 216, duration: 0.006s, episode steps:   9, steps per second: 1613, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      "  4372/100000: episode: 217, duration: 0.018s, episode steps:  34, steps per second: 1922, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  4385/100000: episode: 218, duration: 0.007s, episode steps:  13, steps per second: 1833, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  4421/100000: episode: 219, duration: 0.020s, episode steps:  36, steps per second: 1822, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  4434/100000: episode: 220, duration: 0.008s, episode steps:  13, steps per second: 1680, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  4446/100000: episode: 221, duration: 0.006s, episode steps:  12, steps per second: 1872, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      "  4458/100000: episode: 222, duration: 0.008s, episode steps:  12, steps per second: 1488, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      "  4471/100000: episode: 223, duration: 0.007s, episode steps:  13, steps per second: 1799, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  4539/100000: episode: 224, duration: 0.035s, episode steps:  68, steps per second: 1940, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  mean_best_reward: --\n",
      "  4573/100000: episode: 225, duration: 0.020s, episode steps:  34, steps per second: 1685, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  4585/100000: episode: 226, duration: 0.007s, episode steps:  12, steps per second: 1826, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  mean_best_reward: --\n",
      "  4597/100000: episode: 227, duration: 0.007s, episode steps:  12, steps per second: 1812, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4613/100000: episode: 228, duration: 0.010s, episode steps:  16, steps per second: 1682, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  4624/100000: episode: 229, duration: 0.007s, episode steps:  11, steps per second: 1671, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  4657/100000: episode: 230, duration: 0.018s, episode steps:  33, steps per second: 1838, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      "  4692/100000: episode: 231, duration: 0.018s, episode steps:  35, steps per second: 1930, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  4720/100000: episode: 232, duration: 0.015s, episode steps:  28, steps per second: 1863, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4739/100000: episode: 233, duration: 0.010s, episode steps:  19, steps per second: 1936, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  4797/100000: episode: 234, duration: 0.030s, episode steps:  58, steps per second: 1951, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  mean_best_reward: --\n",
      "  4806/100000: episode: 235, duration: 0.006s, episode steps:   9, steps per second: 1518, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      "  4824/100000: episode: 236, duration: 0.009s, episode steps:  18, steps per second: 1901, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  4882/100000: episode: 237, duration: 0.029s, episode steps:  58, steps per second: 1984, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  4897/100000: episode: 238, duration: 0.008s, episode steps:  15, steps per second: 1839, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  4925/100000: episode: 239, duration: 0.015s, episode steps:  28, steps per second: 1919, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  4938/100000: episode: 240, duration: 0.007s, episode steps:  13, steps per second: 1748, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  4959/100000: episode: 241, duration: 0.011s, episode steps:  21, steps per second: 1843, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      "  4975/100000: episode: 242, duration: 0.009s, episode steps:  16, steps per second: 1834, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      "  4997/100000: episode: 243, duration: 0.011s, episode steps:  22, steps per second: 1980, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "  5037/100000: episode: 244, duration: 0.020s, episode steps:  40, steps per second: 1954, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5061/100000: episode: 245, duration: 0.013s, episode steps:  24, steps per second: 1814, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  5092/100000: episode: 246, duration: 0.017s, episode steps:  31, steps per second: 1856, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.742 [0.000, 1.000],  mean_best_reward: --\n",
      "  5105/100000: episode: 247, duration: 0.007s, episode steps:  13, steps per second: 1752, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  mean_best_reward: --\n",
      "  5128/100000: episode: 248, duration: 0.013s, episode steps:  23, steps per second: 1775, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      "  5149/100000: episode: 249, duration: 0.012s, episode steps:  21, steps per second: 1818, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5161/100000: episode: 250, duration: 0.008s, episode steps:  12, steps per second: 1570, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  5270/100000: episode: 251, duration: 0.061s, episode steps: 109, steps per second: 1774, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: 88.500000\n",
      "  5332/100000: episode: 252, duration: 0.039s, episode steps:  62, steps per second: 1593, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      "  5355/100000: episode: 253, duration: 0.014s, episode steps:  23, steps per second: 1640, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      "  5384/100000: episode: 254, duration: 0.017s, episode steps:  29, steps per second: 1659, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  5406/100000: episode: 255, duration: 0.013s, episode steps:  22, steps per second: 1649, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      "  5424/100000: episode: 256, duration: 0.012s, episode steps:  18, steps per second: 1470, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  5441/100000: episode: 257, duration: 0.010s, episode steps:  17, steps per second: 1773, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  5540/100000: episode: 258, duration: 0.058s, episode steps:  99, steps per second: 1710, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  5560/100000: episode: 259, duration: 0.014s, episode steps:  20, steps per second: 1387, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      "  5574/100000: episode: 260, duration: 0.009s, episode steps:  14, steps per second: 1518, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  5630/100000: episode: 261, duration: 0.034s, episode steps:  56, steps per second: 1642, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  mean_best_reward: --\n",
      "  5649/100000: episode: 262, duration: 0.012s, episode steps:  19, steps per second: 1605, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  5670/100000: episode: 263, duration: 0.013s, episode steps:  21, steps per second: 1572, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      "  5719/100000: episode: 264, duration: 0.026s, episode steps:  49, steps per second: 1906, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  5797/100000: episode: 265, duration: 0.039s, episode steps:  78, steps per second: 1997, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      "  5837/100000: episode: 266, duration: 0.021s, episode steps:  40, steps per second: 1929, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      "  5859/100000: episode: 267, duration: 0.012s, episode steps:  22, steps per second: 1857, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      "  5881/100000: episode: 268, duration: 0.011s, episode steps:  22, steps per second: 1954, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  5915/100000: episode: 269, duration: 0.019s, episode steps:  34, steps per second: 1757, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  5925/100000: episode: 270, duration: 0.007s, episode steps:  10, steps per second: 1533, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      "  5997/100000: episode: 271, duration: 0.038s, episode steps:  72, steps per second: 1893, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      "  6015/100000: episode: 272, duration: 0.010s, episode steps:  18, steps per second: 1817, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  6025/100000: episode: 273, duration: 0.006s, episode steps:  10, steps per second: 1737, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      "  6043/100000: episode: 274, duration: 0.010s, episode steps:  18, steps per second: 1798, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      "  6056/100000: episode: 275, duration: 0.008s, episode steps:  13, steps per second: 1681, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  6093/100000: episode: 276, duration: 0.018s, episode steps:  37, steps per second: 2001, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  6109/100000: episode: 277, duration: 0.008s, episode steps:  16, steps per second: 1897, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      "  6123/100000: episode: 278, duration: 0.008s, episode steps:  14, steps per second: 1698, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  6133/100000: episode: 279, duration: 0.006s, episode steps:  10, steps per second: 1701, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      "  6147/100000: episode: 280, duration: 0.008s, episode steps:  14, steps per second: 1671, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  6161/100000: episode: 281, duration: 0.008s, episode steps:  14, steps per second: 1686, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  6174/100000: episode: 282, duration: 0.007s, episode steps:  13, steps per second: 1785, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      "  6190/100000: episode: 283, duration: 0.008s, episode steps:  16, steps per second: 1928, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      "  6209/100000: episode: 284, duration: 0.010s, episode steps:  19, steps per second: 1829, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      "  6243/100000: episode: 285, duration: 0.018s, episode steps:  34, steps per second: 1902, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      "  6261/100000: episode: 286, duration: 0.011s, episode steps:  18, steps per second: 1639, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  6275/100000: episode: 287, duration: 0.009s, episode steps:  14, steps per second: 1566, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  6289/100000: episode: 288, duration: 0.009s, episode steps:  14, steps per second: 1566, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  6321/100000: episode: 289, duration: 0.017s, episode steps:  32, steps per second: 1867, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  6373/100000: episode: 290, duration: 0.025s, episode steps:  52, steps per second: 2058, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  mean_best_reward: --\n",
      "  6384/100000: episode: 291, duration: 0.006s, episode steps:  11, steps per second: 1699, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  6421/100000: episode: 292, duration: 0.019s, episode steps:  37, steps per second: 1966, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      "  6447/100000: episode: 293, duration: 0.014s, episode steps:  26, steps per second: 1908, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      "  6465/100000: episode: 294, duration: 0.010s, episode steps:  18, steps per second: 1869, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      "  6505/100000: episode: 295, duration: 0.020s, episode steps:  40, steps per second: 1954, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      "  6530/100000: episode: 296, duration: 0.013s, episode steps:  25, steps per second: 1852, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  mean_best_reward: --\n",
      "  6542/100000: episode: 297, duration: 0.009s, episode steps:  12, steps per second: 1348, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  6554/100000: episode: 298, duration: 0.007s, episode steps:  12, steps per second: 1647, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      "  6580/100000: episode: 299, duration: 0.015s, episode steps:  26, steps per second: 1715, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  6592/100000: episode: 300, duration: 0.009s, episode steps:  12, steps per second: 1386, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  6626/100000: episode: 301, duration: 0.018s, episode steps:  34, steps per second: 1928, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: 49.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6642/100000: episode: 302, duration: 0.010s, episode steps:  16, steps per second: 1617, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      "  6685/100000: episode: 303, duration: 0.023s, episode steps:  43, steps per second: 1839, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      "  6769/100000: episode: 304, duration: 0.040s, episode steps:  84, steps per second: 2111, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      "  6788/100000: episode: 305, duration: 0.011s, episode steps:  19, steps per second: 1761, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  6824/100000: episode: 306, duration: 0.018s, episode steps:  36, steps per second: 1962, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      "  6869/100000: episode: 307, duration: 0.023s, episode steps:  45, steps per second: 1937, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      "  6885/100000: episode: 308, duration: 0.009s, episode steps:  16, steps per second: 1825, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  6920/100000: episode: 309, duration: 0.018s, episode steps:  35, steps per second: 1942, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  6944/100000: episode: 310, duration: 0.013s, episode steps:  24, steps per second: 1877, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7014/100000: episode: 311, duration: 0.034s, episode steps:  70, steps per second: 2069, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7039/100000: episode: 312, duration: 0.014s, episode steps:  25, steps per second: 1777, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      "  7068/100000: episode: 313, duration: 0.016s, episode steps:  29, steps per second: 1783, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "  7084/100000: episode: 314, duration: 0.008s, episode steps:  16, steps per second: 1940, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      "  7112/100000: episode: 315, duration: 0.015s, episode steps:  28, steps per second: 1879, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  7128/100000: episode: 316, duration: 0.009s, episode steps:  16, steps per second: 1812, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7205/100000: episode: 317, duration: 0.037s, episode steps:  77, steps per second: 2068, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      "  7227/100000: episode: 318, duration: 0.012s, episode steps:  22, steps per second: 1797, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      "  7240/100000: episode: 319, duration: 0.007s, episode steps:  13, steps per second: 1793, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      "  7258/100000: episode: 320, duration: 0.010s, episode steps:  18, steps per second: 1822, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      "  7281/100000: episode: 321, duration: 0.013s, episode steps:  23, steps per second: 1814, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      "  7299/100000: episode: 322, duration: 0.009s, episode steps:  18, steps per second: 1920, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      "  7325/100000: episode: 323, duration: 0.014s, episode steps:  26, steps per second: 1884, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      "  7360/100000: episode: 324, duration: 0.017s, episode steps:  35, steps per second: 2055, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      "  7378/100000: episode: 325, duration: 0.010s, episode steps:  18, steps per second: 1894, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  7404/100000: episode: 326, duration: 0.013s, episode steps:  26, steps per second: 1993, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      "  7432/100000: episode: 327, duration: 0.017s, episode steps:  28, steps per second: 1676, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      "  7475/100000: episode: 328, duration: 0.022s, episode steps:  43, steps per second: 1998, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      "  7494/100000: episode: 329, duration: 0.011s, episode steps:  19, steps per second: 1690, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      "  7523/100000: episode: 330, duration: 0.015s, episode steps:  29, steps per second: 1907, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      "  7542/100000: episode: 331, duration: 0.009s, episode steps:  19, steps per second: 2019, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      "  7593/100000: episode: 332, duration: 0.027s, episode steps:  51, steps per second: 1890, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      "  7652/100000: episode: 333, duration: 0.033s, episode steps:  59, steps per second: 1787, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      "  7674/100000: episode: 334, duration: 0.013s, episode steps:  22, steps per second: 1668, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  7713/100000: episode: 335, duration: 0.019s, episode steps:  39, steps per second: 2013, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      "  7737/100000: episode: 336, duration: 0.012s, episode steps:  24, steps per second: 1993, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      "  7751/100000: episode: 337, duration: 0.008s, episode steps:  14, steps per second: 1714, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      "  7768/100000: episode: 338, duration: 0.010s, episode steps:  17, steps per second: 1700, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      "  7809/100000: episode: 339, duration: 0.022s, episode steps:  41, steps per second: 1870, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      "  7845/100000: episode: 340, duration: 0.020s, episode steps:  36, steps per second: 1794, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      "  7854/100000: episode: 341, duration: 0.005s, episode steps:   9, steps per second: 1667, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  mean_best_reward: --\n",
      "  7869/100000: episode: 342, duration: 0.008s, episode steps:  15, steps per second: 1827, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      "  7902/100000: episode: 343, duration: 0.017s, episode steps:  33, steps per second: 1920, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "  7919/100000: episode: 344, duration: 0.009s, episode steps:  17, steps per second: 1814, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      "  7981/100000: episode: 345, duration: 0.031s, episode steps:  62, steps per second: 1998, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  8004/100000: episode: 346, duration: 0.012s, episode steps:  23, steps per second: 1854, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      "  8039/100000: episode: 347, duration: 0.017s, episode steps:  35, steps per second: 2056, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      "  8073/100000: episode: 348, duration: 0.017s, episode steps:  34, steps per second: 1998, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      "  8086/100000: episode: 349, duration: 0.008s, episode steps:  13, steps per second: 1601, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  8106/100000: episode: 350, duration: 0.012s, episode steps:  20, steps per second: 1728, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  8133/100000: episode: 351, duration: 0.013s, episode steps:  27, steps per second: 2003, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: 81.000000\n",
      "  8153/100000: episode: 352, duration: 0.011s, episode steps:  20, steps per second: 1828, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8192/100000: episode: 353, duration: 0.021s, episode steps:  39, steps per second: 1849, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      "  8212/100000: episode: 354, duration: 0.012s, episode steps:  20, steps per second: 1723, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  mean_best_reward: --\n",
      "  8223/100000: episode: 355, duration: 0.006s, episode steps:  11, steps per second: 1695, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      "  8237/100000: episode: 356, duration: 0.007s, episode steps:  14, steps per second: 1869, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  8252/100000: episode: 357, duration: 0.008s, episode steps:  15, steps per second: 1900, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  8267/100000: episode: 358, duration: 0.009s, episode steps:  15, steps per second: 1599, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  8300/100000: episode: 359, duration: 0.016s, episode steps:  33, steps per second: 2028, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  8330/100000: episode: 360, duration: 0.015s, episode steps:  30, steps per second: 1974, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      "  8368/100000: episode: 361, duration: 0.019s, episode steps:  38, steps per second: 1954, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      "  8451/100000: episode: 362, duration: 0.041s, episode steps:  83, steps per second: 2000, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      "  8465/100000: episode: 363, duration: 0.007s, episode steps:  14, steps per second: 1916, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      "  8508/100000: episode: 364, duration: 0.022s, episode steps:  43, steps per second: 2000, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      "  8520/100000: episode: 365, duration: 0.007s, episode steps:  12, steps per second: 1609, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      "  8535/100000: episode: 366, duration: 0.008s, episode steps:  15, steps per second: 1860, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  8546/100000: episode: 367, duration: 0.006s, episode steps:  11, steps per second: 1710, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      "  8570/100000: episode: 368, duration: 0.014s, episode steps:  24, steps per second: 1675, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  8605/100000: episode: 369, duration: 0.019s, episode steps:  35, steps per second: 1863, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  8654/100000: episode: 370, duration: 0.026s, episode steps:  49, steps per second: 1863, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  mean_best_reward: --\n",
      "  8677/100000: episode: 371, duration: 0.014s, episode steps:  23, steps per second: 1650, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      "  8704/100000: episode: 372, duration: 0.015s, episode steps:  27, steps per second: 1760, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  8729/100000: episode: 373, duration: 0.014s, episode steps:  25, steps per second: 1759, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      "  8805/100000: episode: 374, duration: 0.037s, episode steps:  76, steps per second: 2078, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.566 [0.000, 1.000],  mean_best_reward: --\n",
      "  8821/100000: episode: 375, duration: 0.009s, episode steps:  16, steps per second: 1809, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  8858/100000: episode: 376, duration: 0.019s, episode steps:  37, steps per second: 1937, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      "  8889/100000: episode: 377, duration: 0.017s, episode steps:  31, steps per second: 1859, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.645 [0.000, 1.000],  mean_best_reward: --\n",
      "  8900/100000: episode: 378, duration: 0.007s, episode steps:  11, steps per second: 1565, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: --\n",
      "  8933/100000: episode: 379, duration: 0.016s, episode steps:  33, steps per second: 2060, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      "  8963/100000: episode: 380, duration: 0.018s, episode steps:  30, steps per second: 1676, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  9019/100000: episode: 381, duration: 0.028s, episode steps:  56, steps per second: 2025, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9037/100000: episode: 382, duration: 0.011s, episode steps:  18, steps per second: 1645, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  9109/100000: episode: 383, duration: 0.035s, episode steps:  72, steps per second: 2074, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      "  9126/100000: episode: 384, duration: 0.010s, episode steps:  17, steps per second: 1735, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      "  9139/100000: episode: 385, duration: 0.007s, episode steps:  13, steps per second: 1910, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      "  9182/100000: episode: 386, duration: 0.022s, episode steps:  43, steps per second: 1954, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      "  9193/100000: episode: 387, duration: 0.006s, episode steps:  11, steps per second: 1705, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  mean_best_reward: --\n",
      "  9221/100000: episode: 388, duration: 0.014s, episode steps:  28, steps per second: 2018, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9244/100000: episode: 389, duration: 0.012s, episode steps:  23, steps per second: 1936, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      "  9269/100000: episode: 390, duration: 0.013s, episode steps:  25, steps per second: 1933, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      "  9299/100000: episode: 391, duration: 0.016s, episode steps:  30, steps per second: 1896, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      "  9342/100000: episode: 392, duration: 0.021s, episode steps:  43, steps per second: 2061, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9366/100000: episode: 393, duration: 0.013s, episode steps:  24, steps per second: 1792, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9401/100000: episode: 394, duration: 0.019s, episode steps:  35, steps per second: 1818, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      "  9439/100000: episode: 395, duration: 0.019s, episode steps:  38, steps per second: 2017, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9452/100000: episode: 396, duration: 0.007s, episode steps:  13, steps per second: 1848, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  mean_best_reward: --\n",
      "  9485/100000: episode: 397, duration: 0.018s, episode steps:  33, steps per second: 1868, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      "  9500/100000: episode: 398, duration: 0.008s, episode steps:  15, steps per second: 1787, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      "  9518/100000: episode: 399, duration: 0.010s, episode steps:  18, steps per second: 1807, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      "  9544/100000: episode: 400, duration: 0.013s, episode steps:  26, steps per second: 1998, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      "  9564/100000: episode: 401, duration: 0.011s, episode steps:  20, steps per second: 1883, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  mean_best_reward: 53.000000\n",
      "  9583/100000: episode: 402, duration: 0.010s, episode steps:  19, steps per second: 1969, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      "  9607/100000: episode: 403, duration: 0.013s, episode steps:  24, steps per second: 1848, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9622/100000: episode: 404, duration: 0.009s, episode steps:  15, steps per second: 1742, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  9649/100000: episode: 405, duration: 0.014s, episode steps:  27, steps per second: 1907, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      "  9665/100000: episode: 406, duration: 0.009s, episode steps:  16, steps per second: 1854, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      "  9681/100000: episode: 407, duration: 0.010s, episode steps:  16, steps per second: 1657, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      "  9731/100000: episode: 408, duration: 0.028s, episode steps:  50, steps per second: 1779, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9745/100000: episode: 409, duration: 0.011s, episode steps:  14, steps per second: 1254, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      "  9781/100000: episode: 410, duration: 0.020s, episode steps:  36, steps per second: 1794, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      "  9809/100000: episode: 411, duration: 0.015s, episode steps:  28, steps per second: 1883, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  mean_best_reward: --\n",
      "  9820/100000: episode: 412, duration: 0.007s, episode steps:  11, steps per second: 1572, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      "  9865/100000: episode: 413, duration: 0.023s, episode steps:  45, steps per second: 1936, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      "  9886/100000: episode: 414, duration: 0.011s, episode steps:  21, steps per second: 1873, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      "  9906/100000: episode: 415, duration: 0.011s, episode steps:  20, steps per second: 1807, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      "  9928/100000: episode: 416, duration: 0.011s, episode steps:  22, steps per second: 1927, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      "  9957/100000: episode: 417, duration: 0.015s, episode steps:  29, steps per second: 1883, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      "  9977/100000: episode: 418, duration: 0.011s, episode steps:  20, steps per second: 1896, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      "  9993/100000: episode: 419, duration: 0.009s, episode steps:  16, steps per second: 1882, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 10013/100000: episode: 420, duration: 0.011s, episode steps:  20, steps per second: 1796, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 10064/100000: episode: 421, duration: 0.026s, episode steps:  51, steps per second: 1938, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 10075/100000: episode: 422, duration: 0.006s, episode steps:  11, steps per second: 1865, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      " 10099/100000: episode: 423, duration: 0.012s, episode steps:  24, steps per second: 1966, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10137/100000: episode: 424, duration: 0.021s, episode steps:  38, steps per second: 1792, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10149/100000: episode: 425, duration: 0.007s, episode steps:  12, steps per second: 1767, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 10159/100000: episode: 426, duration: 0.006s, episode steps:  10, steps per second: 1591, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      " 10181/100000: episode: 427, duration: 0.013s, episode steps:  22, steps per second: 1693, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10248/100000: episode: 428, duration: 0.034s, episode steps:  67, steps per second: 1988, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 10270/100000: episode: 429, duration: 0.011s, episode steps:  22, steps per second: 2029, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 10308/100000: episode: 430, duration: 0.020s, episode steps:  38, steps per second: 1947, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 10341/100000: episode: 431, duration: 0.017s, episode steps:  33, steps per second: 1910, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 10363/100000: episode: 432, duration: 0.011s, episode steps:  22, steps per second: 1957, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 10389/100000: episode: 433, duration: 0.013s, episode steps:  26, steps per second: 1976, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 10423/100000: episode: 434, duration: 0.017s, episode steps:  34, steps per second: 1966, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10436/100000: episode: 435, duration: 0.007s, episode steps:  13, steps per second: 1777, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  mean_best_reward: --\n",
      " 10493/100000: episode: 436, duration: 0.028s, episode steps:  57, steps per second: 2068, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10535/100000: episode: 437, duration: 0.022s, episode steps:  42, steps per second: 1871, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 10560/100000: episode: 438, duration: 0.014s, episode steps:  25, steps per second: 1793, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 10613/100000: episode: 439, duration: 0.027s, episode steps:  53, steps per second: 1972, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 10647/100000: episode: 440, duration: 0.018s, episode steps:  34, steps per second: 1939, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 10776/100000: episode: 441, duration: 0.066s, episode steps: 129, steps per second: 1968, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 10804/100000: episode: 442, duration: 0.016s, episode steps:  28, steps per second: 1719, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 10843/100000: episode: 443, duration: 0.021s, episode steps:  39, steps per second: 1841, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 10891/100000: episode: 444, duration: 0.024s, episode steps:  48, steps per second: 1999, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 10915/100000: episode: 445, duration: 0.012s, episode steps:  24, steps per second: 1944, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 10977/100000: episode: 446, duration: 0.034s, episode steps:  62, steps per second: 1827, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 11013/100000: episode: 447, duration: 0.018s, episode steps:  36, steps per second: 2034, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 11029/100000: episode: 448, duration: 0.008s, episode steps:  16, steps per second: 1916, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 11048/100000: episode: 449, duration: 0.011s, episode steps:  19, steps per second: 1795, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 11078/100000: episode: 450, duration: 0.016s, episode steps:  30, steps per second: 1920, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: --\n",
      " 11097/100000: episode: 451, duration: 0.011s, episode steps:  19, steps per second: 1725, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: 59.000000\n",
      " 11111/100000: episode: 452, duration: 0.008s, episode steps:  14, steps per second: 1841, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 11149/100000: episode: 453, duration: 0.019s, episode steps:  38, steps per second: 2018, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 11181/100000: episode: 454, duration: 0.016s, episode steps:  32, steps per second: 1941, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 11208/100000: episode: 455, duration: 0.014s, episode steps:  27, steps per second: 1866, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 11231/100000: episode: 456, duration: 0.012s, episode steps:  23, steps per second: 1956, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 11250/100000: episode: 457, duration: 0.010s, episode steps:  19, steps per second: 1921, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 11277/100000: episode: 458, duration: 0.015s, episode steps:  27, steps per second: 1860, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 11313/100000: episode: 459, duration: 0.021s, episode steps:  36, steps per second: 1756, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 11343/100000: episode: 460, duration: 0.016s, episode steps:  30, steps per second: 1915, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 11356/100000: episode: 461, duration: 0.007s, episode steps:  13, steps per second: 1904, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 11386/100000: episode: 462, duration: 0.016s, episode steps:  30, steps per second: 1921, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11443/100000: episode: 463, duration: 0.029s, episode steps:  57, steps per second: 1969, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 11459/100000: episode: 464, duration: 0.008s, episode steps:  16, steps per second: 1972, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 11480/100000: episode: 465, duration: 0.012s, episode steps:  21, steps per second: 1822, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 11546/100000: episode: 466, duration: 0.032s, episode steps:  66, steps per second: 2048, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 11560/100000: episode: 467, duration: 0.008s, episode steps:  14, steps per second: 1786, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 11584/100000: episode: 468, duration: 0.012s, episode steps:  24, steps per second: 2009, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 11646/100000: episode: 469, duration: 0.031s, episode steps:  62, steps per second: 2001, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 11681/100000: episode: 470, duration: 0.017s, episode steps:  35, steps per second: 2007, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 11703/100000: episode: 471, duration: 0.013s, episode steps:  22, steps per second: 1715, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 11717/100000: episode: 472, duration: 0.009s, episode steps:  14, steps per second: 1474, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 11733/100000: episode: 473, duration: 0.008s, episode steps:  16, steps per second: 1930, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 11752/100000: episode: 474, duration: 0.011s, episode steps:  19, steps per second: 1792, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      " 11768/100000: episode: 475, duration: 0.010s, episode steps:  16, steps per second: 1630, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 11802/100000: episode: 476, duration: 0.018s, episode steps:  34, steps per second: 1932, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 11829/100000: episode: 477, duration: 0.016s, episode steps:  27, steps per second: 1696, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 11872/100000: episode: 478, duration: 0.025s, episode steps:  43, steps per second: 1724, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 11884/100000: episode: 479, duration: 0.007s, episode steps:  12, steps per second: 1623, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 11919/100000: episode: 480, duration: 0.018s, episode steps:  35, steps per second: 1989, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 11951/100000: episode: 481, duration: 0.017s, episode steps:  32, steps per second: 1845, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 11968/100000: episode: 482, duration: 0.010s, episode steps:  17, steps per second: 1778, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 11987/100000: episode: 483, duration: 0.011s, episode steps:  19, steps per second: 1769, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 12041/100000: episode: 484, duration: 0.027s, episode steps:  54, steps per second: 2033, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12094/100000: episode: 485, duration: 0.029s, episode steps:  53, steps per second: 1831, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 12121/100000: episode: 486, duration: 0.014s, episode steps:  27, steps per second: 1888, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 12158/100000: episode: 487, duration: 0.019s, episode steps:  37, steps per second: 1970, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 12178/100000: episode: 488, duration: 0.011s, episode steps:  20, steps per second: 1834, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 12193/100000: episode: 489, duration: 0.008s, episode steps:  15, steps per second: 1893, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 12212/100000: episode: 490, duration: 0.011s, episode steps:  19, steps per second: 1775, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 12226/100000: episode: 491, duration: 0.008s, episode steps:  14, steps per second: 1806, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 12253/100000: episode: 492, duration: 0.013s, episode steps:  27, steps per second: 2046, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 12296/100000: episode: 493, duration: 0.022s, episode steps:  43, steps per second: 1959, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 12312/100000: episode: 494, duration: 0.009s, episode steps:  16, steps per second: 1882, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      " 12334/100000: episode: 495, duration: 0.012s, episode steps:  22, steps per second: 1774, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 12379/100000: episode: 496, duration: 0.021s, episode steps:  45, steps per second: 2115, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 12414/100000: episode: 497, duration: 0.018s, episode steps:  35, steps per second: 1941, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 12434/100000: episode: 498, duration: 0.011s, episode steps:  20, steps per second: 1773, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 12448/100000: episode: 499, duration: 0.008s, episode steps:  14, steps per second: 1797, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 12465/100000: episode: 500, duration: 0.009s, episode steps:  17, steps per second: 1843, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 12485/100000: episode: 501, duration: 0.013s, episode steps:  20, steps per second: 1555, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: 66.000000\n",
      " 12536/100000: episode: 502, duration: 0.025s, episode steps:  51, steps per second: 2057, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 12565/100000: episode: 503, duration: 0.016s, episode steps:  29, steps per second: 1834, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 12613/100000: episode: 504, duration: 0.025s, episode steps:  48, steps per second: 1925, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 12650/100000: episode: 505, duration: 0.018s, episode steps:  37, steps per second: 2027, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 12671/100000: episode: 506, duration: 0.012s, episode steps:  21, steps per second: 1814, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  mean_best_reward: --\n",
      " 12685/100000: episode: 507, duration: 0.007s, episode steps:  14, steps per second: 1935, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 12722/100000: episode: 508, duration: 0.019s, episode steps:  37, steps per second: 1961, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 12769/100000: episode: 509, duration: 0.024s, episode steps:  47, steps per second: 1988, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 12814/100000: episode: 510, duration: 0.022s, episode steps:  45, steps per second: 2024, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 12837/100000: episode: 511, duration: 0.012s, episode steps:  23, steps per second: 1982, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 12857/100000: episode: 512, duration: 0.012s, episode steps:  20, steps per second: 1724, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 12893/100000: episode: 513, duration: 0.023s, episode steps:  36, steps per second: 1542, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 12920/100000: episode: 514, duration: 0.016s, episode steps:  27, steps per second: 1674, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 13015/100000: episode: 515, duration: 0.049s, episode steps:  95, steps per second: 1951, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  mean_best_reward: --\n",
      " 13053/100000: episode: 516, duration: 0.019s, episode steps:  38, steps per second: 1973, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13072/100000: episode: 517, duration: 0.010s, episode steps:  19, steps per second: 1933, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 13151/100000: episode: 518, duration: 0.038s, episode steps:  79, steps per second: 2064, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 13218/100000: episode: 519, duration: 0.033s, episode steps:  67, steps per second: 2031, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 13268/100000: episode: 520, duration: 0.025s, episode steps:  50, steps per second: 1969, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 13296/100000: episode: 521, duration: 0.016s, episode steps:  28, steps per second: 1760, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13323/100000: episode: 522, duration: 0.014s, episode steps:  27, steps per second: 1874, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 13431/100000: episode: 523, duration: 0.053s, episode steps: 108, steps per second: 2024, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 13461/100000: episode: 524, duration: 0.015s, episode steps:  30, steps per second: 1985, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13548/100000: episode: 525, duration: 0.042s, episode steps:  87, steps per second: 2068, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 13569/100000: episode: 526, duration: 0.011s, episode steps:  21, steps per second: 1885, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 13611/100000: episode: 527, duration: 0.021s, episode steps:  42, steps per second: 2026, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13639/100000: episode: 528, duration: 0.014s, episode steps:  28, steps per second: 1965, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 13663/100000: episode: 529, duration: 0.014s, episode steps:  24, steps per second: 1687, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13727/100000: episode: 530, duration: 0.035s, episode steps:  64, steps per second: 1824, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 13748/100000: episode: 531, duration: 0.011s, episode steps:  21, steps per second: 1907, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 13782/100000: episode: 532, duration: 0.017s, episode steps:  34, steps per second: 1965, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13852/100000: episode: 533, duration: 0.035s, episode steps:  70, steps per second: 2019, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13892/100000: episode: 534, duration: 0.020s, episode steps:  40, steps per second: 2025, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 13922/100000: episode: 535, duration: 0.015s, episode steps:  30, steps per second: 1972, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 13959/100000: episode: 536, duration: 0.019s, episode steps:  37, steps per second: 1977, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  mean_best_reward: --\n",
      " 14014/100000: episode: 537, duration: 0.031s, episode steps:  55, steps per second: 1778, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 14067/100000: episode: 538, duration: 0.028s, episode steps:  53, steps per second: 1871, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 14102/100000: episode: 539, duration: 0.018s, episode steps:  35, steps per second: 1906, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 14166/100000: episode: 540, duration: 0.034s, episode steps:  64, steps per second: 1906, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 14203/100000: episode: 541, duration: 0.019s, episode steps:  37, steps per second: 1996, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 14244/100000: episode: 542, duration: 0.021s, episode steps:  41, steps per second: 1960, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 14282/100000: episode: 543, duration: 0.019s, episode steps:  38, steps per second: 1974, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 14315/100000: episode: 544, duration: 0.017s, episode steps:  33, steps per second: 1997, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 14338/100000: episode: 545, duration: 0.012s, episode steps:  23, steps per second: 1888, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 14388/100000: episode: 546, duration: 0.024s, episode steps:  50, steps per second: 2069, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 14426/100000: episode: 547, duration: 0.019s, episode steps:  38, steps per second: 1976, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 14471/100000: episode: 548, duration: 0.022s, episode steps:  45, steps per second: 2011, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 14487/100000: episode: 549, duration: 0.009s, episode steps:  16, steps per second: 1846, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 14531/100000: episode: 550, duration: 0.024s, episode steps:  44, steps per second: 1847, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 14551/100000: episode: 551, duration: 0.014s, episode steps:  20, steps per second: 1441, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  mean_best_reward: 73.000000\n",
      " 14595/100000: episode: 552, duration: 0.025s, episode steps:  44, steps per second: 1783, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 14616/100000: episode: 553, duration: 0.011s, episode steps:  21, steps per second: 1854, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 14655/100000: episode: 554, duration: 0.020s, episode steps:  39, steps per second: 1950, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 14706/100000: episode: 555, duration: 0.026s, episode steps:  51, steps per second: 1973, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 14790/100000: episode: 556, duration: 0.041s, episode steps:  84, steps per second: 2032, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 14818/100000: episode: 557, duration: 0.014s, episode steps:  28, steps per second: 1952, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 14839/100000: episode: 558, duration: 0.011s, episode steps:  21, steps per second: 1936, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 14863/100000: episode: 559, duration: 0.013s, episode steps:  24, steps per second: 1859, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 14877/100000: episode: 560, duration: 0.008s, episode steps:  14, steps per second: 1858, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      " 14912/100000: episode: 561, duration: 0.018s, episode steps:  35, steps per second: 1946, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 14924/100000: episode: 562, duration: 0.007s, episode steps:  12, steps per second: 1795, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 14959/100000: episode: 563, duration: 0.020s, episode steps:  35, steps per second: 1763, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 14975/100000: episode: 564, duration: 0.009s, episode steps:  16, steps per second: 1793, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 15044/100000: episode: 565, duration: 0.037s, episode steps:  69, steps per second: 1876, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 15059/100000: episode: 566, duration: 0.011s, episode steps:  15, steps per second: 1410, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  mean_best_reward: --\n",
      " 15082/100000: episode: 567, duration: 0.013s, episode steps:  23, steps per second: 1727, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 15115/100000: episode: 568, duration: 0.019s, episode steps:  33, steps per second: 1759, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 15149/100000: episode: 569, duration: 0.017s, episode steps:  34, steps per second: 1986, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 15202/100000: episode: 570, duration: 0.026s, episode steps:  53, steps per second: 2069, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 15224/100000: episode: 571, duration: 0.012s, episode steps:  22, steps per second: 1903, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 15275/100000: episode: 572, duration: 0.025s, episode steps:  51, steps per second: 2032, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 15296/100000: episode: 573, duration: 0.012s, episode steps:  21, steps per second: 1802, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 15310/100000: episode: 574, duration: 0.008s, episode steps:  14, steps per second: 1753, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15337/100000: episode: 575, duration: 0.015s, episode steps:  27, steps per second: 1857, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 15390/100000: episode: 576, duration: 0.028s, episode steps:  53, steps per second: 1907, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 15425/100000: episode: 577, duration: 0.019s, episode steps:  35, steps per second: 1811, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 15455/100000: episode: 578, duration: 0.015s, episode steps:  30, steps per second: 1968, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 15491/100000: episode: 579, duration: 0.019s, episode steps:  36, steps per second: 1920, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15534/100000: episode: 580, duration: 0.021s, episode steps:  43, steps per second: 2070, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.651 [0.000, 1.000],  mean_best_reward: --\n",
      " 15619/100000: episode: 581, duration: 0.041s, episode steps:  85, steps per second: 2084, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 15672/100000: episode: 582, duration: 0.026s, episode steps:  53, steps per second: 2007, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 15762/100000: episode: 583, duration: 0.046s, episode steps:  90, steps per second: 1936, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.578 [0.000, 1.000],  mean_best_reward: --\n",
      " 15787/100000: episode: 584, duration: 0.013s, episode steps:  25, steps per second: 1871, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 15804/100000: episode: 585, duration: 0.009s, episode steps:  17, steps per second: 1832, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 15816/100000: episode: 586, duration: 0.008s, episode steps:  12, steps per second: 1575, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  mean_best_reward: --\n",
      " 15840/100000: episode: 587, duration: 0.012s, episode steps:  24, steps per second: 2056, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 15901/100000: episode: 588, duration: 0.031s, episode steps:  61, steps per second: 1954, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  mean_best_reward: --\n",
      " 15928/100000: episode: 589, duration: 0.015s, episode steps:  27, steps per second: 1797, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 15948/100000: episode: 590, duration: 0.010s, episode steps:  20, steps per second: 1947, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16001/100000: episode: 591, duration: 0.026s, episode steps:  53, steps per second: 2025, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 16031/100000: episode: 592, duration: 0.015s, episode steps:  30, steps per second: 2014, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 16090/100000: episode: 593, duration: 0.029s, episode steps:  59, steps per second: 2057, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 16128/100000: episode: 594, duration: 0.021s, episode steps:  38, steps per second: 1795, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 16184/100000: episode: 595, duration: 0.034s, episode steps:  56, steps per second: 1624, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16198/100000: episode: 596, duration: 0.008s, episode steps:  14, steps per second: 1709, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 16217/100000: episode: 597, duration: 0.011s, episode steps:  19, steps per second: 1694, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 16248/100000: episode: 598, duration: 0.015s, episode steps:  31, steps per second: 2071, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 16267/100000: episode: 599, duration: 0.010s, episode steps:  19, steps per second: 1908, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 16282/100000: episode: 600, duration: 0.009s, episode steps:  15, steps per second: 1734, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 16303/100000: episode: 601, duration: 0.011s, episode steps:  21, steps per second: 1881, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: 75.500000\n",
      " 16337/100000: episode: 602, duration: 0.018s, episode steps:  34, steps per second: 1940, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16372/100000: episode: 603, duration: 0.018s, episode steps:  35, steps per second: 1915, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 16407/100000: episode: 604, duration: 0.018s, episode steps:  35, steps per second: 1934, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 16462/100000: episode: 605, duration: 0.027s, episode steps:  55, steps per second: 2070, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 16498/100000: episode: 606, duration: 0.018s, episode steps:  36, steps per second: 2007, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 16554/100000: episode: 607, duration: 0.029s, episode steps:  56, steps per second: 1954, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 16565/100000: episode: 608, duration: 0.007s, episode steps:  11, steps per second: 1523, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 16599/100000: episode: 609, duration: 0.019s, episode steps:  34, steps per second: 1809, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 16615/100000: episode: 610, duration: 0.008s, episode steps:  16, steps per second: 1921, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 16627/100000: episode: 611, duration: 0.008s, episode steps:  12, steps per second: 1540, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 16646/100000: episode: 612, duration: 0.010s, episode steps:  19, steps per second: 1960, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 16658/100000: episode: 613, duration: 0.008s, episode steps:  12, steps per second: 1490, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 16677/100000: episode: 614, duration: 0.010s, episode steps:  19, steps per second: 1939, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 16696/100000: episode: 615, duration: 0.010s, episode steps:  19, steps per second: 1877, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 16710/100000: episode: 616, duration: 0.008s, episode steps:  14, steps per second: 1665, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      " 16729/100000: episode: 617, duration: 0.010s, episode steps:  19, steps per second: 1945, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 16764/100000: episode: 618, duration: 0.017s, episode steps:  35, steps per second: 2035, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 16782/100000: episode: 619, duration: 0.011s, episode steps:  18, steps per second: 1711, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 16856/100000: episode: 620, duration: 0.037s, episode steps:  74, steps per second: 2017, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 16896/100000: episode: 621, duration: 0.019s, episode steps:  40, steps per second: 2057, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 16922/100000: episode: 622, duration: 0.014s, episode steps:  26, steps per second: 1873, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16940/100000: episode: 623, duration: 0.011s, episode steps:  18, steps per second: 1600, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 16951/100000: episode: 624, duration: 0.007s, episode steps:  11, steps per second: 1643, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  mean_best_reward: --\n",
      " 16978/100000: episode: 625, duration: 0.015s, episode steps:  27, steps per second: 1836, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 17014/100000: episode: 626, duration: 0.019s, episode steps:  36, steps per second: 1927, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 17044/100000: episode: 627, duration: 0.016s, episode steps:  30, steps per second: 1928, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 17074/100000: episode: 628, duration: 0.015s, episode steps:  30, steps per second: 1945, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 17106/100000: episode: 629, duration: 0.016s, episode steps:  32, steps per second: 1973, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 17122/100000: episode: 630, duration: 0.010s, episode steps:  16, steps per second: 1680, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 17139/100000: episode: 631, duration: 0.009s, episode steps:  17, steps per second: 1836, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 17157/100000: episode: 632, duration: 0.011s, episode steps:  18, steps per second: 1697, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 17200/100000: episode: 633, duration: 0.024s, episode steps:  43, steps per second: 1760, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 17215/100000: episode: 634, duration: 0.009s, episode steps:  15, steps per second: 1706, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 17236/100000: episode: 635, duration: 0.013s, episode steps:  21, steps per second: 1638, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 17275/100000: episode: 636, duration: 0.020s, episode steps:  39, steps per second: 1945, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  mean_best_reward: --\n",
      " 17294/100000: episode: 637, duration: 0.010s, episode steps:  19, steps per second: 1862, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 17312/100000: episode: 638, duration: 0.011s, episode steps:  18, steps per second: 1637, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 17338/100000: episode: 639, duration: 0.014s, episode steps:  26, steps per second: 1843, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 17357/100000: episode: 640, duration: 0.011s, episode steps:  19, steps per second: 1785, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 17375/100000: episode: 641, duration: 0.010s, episode steps:  18, steps per second: 1738, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 17397/100000: episode: 642, duration: 0.011s, episode steps:  22, steps per second: 1974, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17410/100000: episode: 643, duration: 0.007s, episode steps:  13, steps per second: 1767, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 17430/100000: episode: 644, duration: 0.011s, episode steps:  20, steps per second: 1854, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17460/100000: episode: 645, duration: 0.015s, episode steps:  30, steps per second: 1940, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 17491/100000: episode: 646, duration: 0.016s, episode steps:  31, steps per second: 1958, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 17523/100000: episode: 647, duration: 0.016s, episode steps:  32, steps per second: 1977, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17537/100000: episode: 648, duration: 0.008s, episode steps:  14, steps per second: 1709, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 17568/100000: episode: 649, duration: 0.017s, episode steps:  31, steps per second: 1858, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 17616/100000: episode: 650, duration: 0.023s, episode steps:  48, steps per second: 2116, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.604 [0.000, 1.000],  mean_best_reward: --\n",
      " 17649/100000: episode: 651, duration: 0.018s, episode steps:  33, steps per second: 1880, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: 81.000000\n",
      " 17705/100000: episode: 652, duration: 0.030s, episode steps:  56, steps per second: 1886, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 17766/100000: episode: 653, duration: 0.031s, episode steps:  61, steps per second: 1958, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 17789/100000: episode: 654, duration: 0.013s, episode steps:  23, steps per second: 1794, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 17837/100000: episode: 655, duration: 0.023s, episode steps:  48, steps per second: 2056, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 17860/100000: episode: 656, duration: 0.012s, episode steps:  23, steps per second: 1851, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  mean_best_reward: --\n",
      " 17958/100000: episode: 657, duration: 0.047s, episode steps:  98, steps per second: 2095, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 17979/100000: episode: 658, duration: 0.011s, episode steps:  21, steps per second: 1866, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 18020/100000: episode: 659, duration: 0.021s, episode steps:  41, steps per second: 1915, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 18043/100000: episode: 660, duration: 0.012s, episode steps:  23, steps per second: 1937, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 18063/100000: episode: 661, duration: 0.011s, episode steps:  20, steps per second: 1894, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18120/100000: episode: 662, duration: 0.032s, episode steps:  57, steps per second: 1778, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  mean_best_reward: --\n",
      " 18179/100000: episode: 663, duration: 0.029s, episode steps:  59, steps per second: 2051, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 18207/100000: episode: 664, duration: 0.015s, episode steps:  28, steps per second: 1882, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18265/100000: episode: 665, duration: 0.033s, episode steps:  58, steps per second: 1747, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18274/100000: episode: 666, duration: 0.006s, episode steps:   9, steps per second: 1552, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      " 18338/100000: episode: 667, duration: 0.033s, episode steps:  64, steps per second: 1941, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18375/100000: episode: 668, duration: 0.019s, episode steps:  37, steps per second: 1949, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 18396/100000: episode: 669, duration: 0.011s, episode steps:  21, steps per second: 1907, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 18418/100000: episode: 670, duration: 0.012s, episode steps:  22, steps per second: 1794, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18438/100000: episode: 671, duration: 0.011s, episode steps:  20, steps per second: 1905, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 18507/100000: episode: 672, duration: 0.035s, episode steps:  69, steps per second: 1981, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 18531/100000: episode: 673, duration: 0.013s, episode steps:  24, steps per second: 1820, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18571/100000: episode: 674, duration: 0.020s, episode steps:  40, steps per second: 2012, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18623/100000: episode: 675, duration: 0.026s, episode steps:  52, steps per second: 2029, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18674/100000: episode: 676, duration: 0.025s, episode steps:  51, steps per second: 2022, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 18708/100000: episode: 677, duration: 0.018s, episode steps:  34, steps per second: 1882, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 18746/100000: episode: 678, duration: 0.019s, episode steps:  38, steps per second: 1992, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18792/100000: episode: 679, duration: 0.024s, episode steps:  46, steps per second: 1879, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 18849/100000: episode: 680, duration: 0.028s, episode steps:  57, steps per second: 2020, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 18893/100000: episode: 681, duration: 0.022s, episode steps:  44, steps per second: 2010, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 18913/100000: episode: 682, duration: 0.013s, episode steps:  20, steps per second: 1494, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18933/100000: episode: 683, duration: 0.012s, episode steps:  20, steps per second: 1715, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 18965/100000: episode: 684, duration: 0.017s, episode steps:  32, steps per second: 1937, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  mean_best_reward: --\n",
      " 18998/100000: episode: 685, duration: 0.018s, episode steps:  33, steps per second: 1839, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 19014/100000: episode: 686, duration: 0.009s, episode steps:  16, steps per second: 1826, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 19060/100000: episode: 687, duration: 0.023s, episode steps:  46, steps per second: 2013, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19075/100000: episode: 688, duration: 0.008s, episode steps:  15, steps per second: 1877, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 19161/100000: episode: 689, duration: 0.042s, episode steps:  86, steps per second: 2072, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19223/100000: episode: 690, duration: 0.031s, episode steps:  62, steps per second: 1984, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19244/100000: episode: 691, duration: 0.011s, episode steps:  21, steps per second: 1893, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 19291/100000: episode: 692, duration: 0.023s, episode steps:  47, steps per second: 2033, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 19326/100000: episode: 693, duration: 0.024s, episode steps:  35, steps per second: 1466, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 19360/100000: episode: 694, duration: 0.020s, episode steps:  34, steps per second: 1711, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19381/100000: episode: 695, duration: 0.012s, episode steps:  21, steps per second: 1756, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  mean_best_reward: --\n",
      " 19401/100000: episode: 696, duration: 0.011s, episode steps:  20, steps per second: 1815, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 19435/100000: episode: 697, duration: 0.018s, episode steps:  34, steps per second: 1860, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19460/100000: episode: 698, duration: 0.013s, episode steps:  25, steps per second: 1973, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 19496/100000: episode: 699, duration: 0.018s, episode steps:  36, steps per second: 2004, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 19532/100000: episode: 700, duration: 0.018s, episode steps:  36, steps per second: 1979, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19550/100000: episode: 701, duration: 0.012s, episode steps:  18, steps per second: 1467, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 60.500000\n",
      " 19595/100000: episode: 702, duration: 0.022s, episode steps:  45, steps per second: 2065, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 19652/100000: episode: 703, duration: 0.028s, episode steps:  57, steps per second: 2014, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 19681/100000: episode: 704, duration: 0.015s, episode steps:  29, steps per second: 1928, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19717/100000: episode: 705, duration: 0.020s, episode steps:  36, steps per second: 1834, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 19764/100000: episode: 706, duration: 0.025s, episode steps:  47, steps per second: 1904, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 19784/100000: episode: 707, duration: 0.010s, episode steps:  20, steps per second: 1982, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 19822/100000: episode: 708, duration: 0.019s, episode steps:  38, steps per second: 2015, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 19873/100000: episode: 709, duration: 0.026s, episode steps:  51, steps per second: 1993, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 19896/100000: episode: 710, duration: 0.012s, episode steps:  23, steps per second: 1886, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 19938/100000: episode: 711, duration: 0.023s, episode steps:  42, steps per second: 1862, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 19957/100000: episode: 712, duration: 0.011s, episode steps:  19, steps per second: 1710, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 20010/100000: episode: 713, duration: 0.027s, episode steps:  53, steps per second: 1963, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 20045/100000: episode: 714, duration: 0.017s, episode steps:  35, steps per second: 2010, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 20082/100000: episode: 715, duration: 0.019s, episode steps:  37, steps per second: 1938, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 20100/100000: episode: 716, duration: 0.010s, episode steps:  18, steps per second: 1839, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20119/100000: episode: 717, duration: 0.012s, episode steps:  19, steps per second: 1531, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 20180/100000: episode: 718, duration: 0.031s, episode steps:  61, steps per second: 1978, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 20204/100000: episode: 719, duration: 0.013s, episode steps:  24, steps per second: 1919, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 20232/100000: episode: 720, duration: 0.015s, episode steps:  28, steps per second: 1839, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 20258/100000: episode: 721, duration: 0.014s, episode steps:  26, steps per second: 1914, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20309/100000: episode: 722, duration: 0.025s, episode steps:  51, steps per second: 2034, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 20321/100000: episode: 723, duration: 0.006s, episode steps:  12, steps per second: 1885, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20348/100000: episode: 724, duration: 0.015s, episode steps:  27, steps per second: 1796, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 20411/100000: episode: 725, duration: 0.034s, episode steps:  63, steps per second: 1854, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 20428/100000: episode: 726, duration: 0.011s, episode steps:  17, steps per second: 1615, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 20443/100000: episode: 727, duration: 0.009s, episode steps:  15, steps per second: 1745, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 20510/100000: episode: 728, duration: 0.035s, episode steps:  67, steps per second: 1893, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 20545/100000: episode: 729, duration: 0.018s, episode steps:  35, steps per second: 1946, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 20589/100000: episode: 730, duration: 0.022s, episode steps:  44, steps per second: 2003, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20630/100000: episode: 731, duration: 0.021s, episode steps:  41, steps per second: 1983, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 20752/100000: episode: 732, duration: 0.059s, episode steps: 122, steps per second: 2057, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 20773/100000: episode: 733, duration: 0.011s, episode steps:  21, steps per second: 1875, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 20836/100000: episode: 734, duration: 0.030s, episode steps:  63, steps per second: 2094, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 20865/100000: episode: 735, duration: 0.015s, episode steps:  29, steps per second: 1888, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 20888/100000: episode: 736, duration: 0.012s, episode steps:  23, steps per second: 1886, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 20921/100000: episode: 737, duration: 0.019s, episode steps:  33, steps per second: 1775, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 20944/100000: episode: 738, duration: 0.013s, episode steps:  23, steps per second: 1740, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 20994/100000: episode: 739, duration: 0.027s, episode steps:  50, steps per second: 1866, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21037/100000: episode: 740, duration: 0.022s, episode steps:  43, steps per second: 1947, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 21052/100000: episode: 741, duration: 0.008s, episode steps:  15, steps per second: 1801, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 21083/100000: episode: 742, duration: 0.016s, episode steps:  31, steps per second: 1987, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 21095/100000: episode: 743, duration: 0.007s, episode steps:  12, steps per second: 1723, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 21111/100000: episode: 744, duration: 0.009s, episode steps:  16, steps per second: 1703, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21152/100000: episode: 745, duration: 0.021s, episode steps:  41, steps per second: 1962, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 21173/100000: episode: 746, duration: 0.011s, episode steps:  21, steps per second: 1939, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 21190/100000: episode: 747, duration: 0.009s, episode steps:  17, steps per second: 1896, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 21213/100000: episode: 748, duration: 0.012s, episode steps:  23, steps per second: 1866, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  mean_best_reward: --\n",
      " 21267/100000: episode: 749, duration: 0.027s, episode steps:  54, steps per second: 2008, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21305/100000: episode: 750, duration: 0.020s, episode steps:  38, steps per second: 1908, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 21338/100000: episode: 751, duration: 0.019s, episode steps:  33, steps per second: 1745, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: 69.000000\n",
      " 21350/100000: episode: 752, duration: 0.007s, episode steps:  12, steps per second: 1797, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 21382/100000: episode: 753, duration: 0.016s, episode steps:  32, steps per second: 2009, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 21423/100000: episode: 754, duration: 0.021s, episode steps:  41, steps per second: 1975, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 21454/100000: episode: 755, duration: 0.018s, episode steps:  31, steps per second: 1716, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 21483/100000: episode: 756, duration: 0.016s, episode steps:  29, steps per second: 1804, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 21494/100000: episode: 757, duration: 0.007s, episode steps:  11, steps per second: 1617, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 21512/100000: episode: 758, duration: 0.011s, episode steps:  18, steps per second: 1653, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21542/100000: episode: 759, duration: 0.016s, episode steps:  30, steps per second: 1882, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 21571/100000: episode: 760, duration: 0.015s, episode steps:  29, steps per second: 1973, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 21598/100000: episode: 761, duration: 0.015s, episode steps:  27, steps per second: 1850, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 21641/100000: episode: 762, duration: 0.021s, episode steps:  43, steps per second: 2014, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 21689/100000: episode: 763, duration: 0.026s, episode steps:  48, steps per second: 1858, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 21702/100000: episode: 764, duration: 0.008s, episode steps:  13, steps per second: 1652, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 21733/100000: episode: 765, duration: 0.015s, episode steps:  31, steps per second: 2001, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  mean_best_reward: --\n",
      " 21762/100000: episode: 766, duration: 0.014s, episode steps:  29, steps per second: 2037, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  mean_best_reward: --\n",
      " 21775/100000: episode: 767, duration: 0.007s, episode steps:  13, steps per second: 1804, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 21787/100000: episode: 768, duration: 0.007s, episode steps:  12, steps per second: 1602, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 21840/100000: episode: 769, duration: 0.026s, episode steps:  53, steps per second: 2051, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 21860/100000: episode: 770, duration: 0.011s, episode steps:  20, steps per second: 1852, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 21873/100000: episode: 771, duration: 0.008s, episode steps:  13, steps per second: 1652, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 21939/100000: episode: 772, duration: 0.032s, episode steps:  66, steps per second: 2043, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 22005/100000: episode: 773, duration: 0.033s, episode steps:  66, steps per second: 2027, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 22023/100000: episode: 774, duration: 0.010s, episode steps:  18, steps per second: 1775, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22046/100000: episode: 775, duration: 0.012s, episode steps:  23, steps per second: 1906, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 22074/100000: episode: 776, duration: 0.015s, episode steps:  28, steps per second: 1828, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22085/100000: episode: 777, duration: 0.007s, episode steps:  11, steps per second: 1538, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      " 22101/100000: episode: 778, duration: 0.010s, episode steps:  16, steps per second: 1609, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22183/100000: episode: 779, duration: 0.040s, episode steps:  82, steps per second: 2054, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  mean_best_reward: --\n",
      " 22201/100000: episode: 780, duration: 0.010s, episode steps:  18, steps per second: 1879, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 22255/100000: episode: 781, duration: 0.027s, episode steps:  54, steps per second: 2005, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22298/100000: episode: 782, duration: 0.022s, episode steps:  43, steps per second: 1979, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.698 [0.000, 1.000],  mean_best_reward: --\n",
      " 22329/100000: episode: 783, duration: 0.015s, episode steps:  31, steps per second: 2004, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.645 [0.000, 1.000],  mean_best_reward: --\n",
      " 22345/100000: episode: 784, duration: 0.008s, episode steps:  16, steps per second: 1908, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 22362/100000: episode: 785, duration: 0.009s, episode steps:  17, steps per second: 1806, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 22378/100000: episode: 786, duration: 0.009s, episode steps:  16, steps per second: 1794, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22397/100000: episode: 787, duration: 0.011s, episode steps:  19, steps per second: 1739, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 22432/100000: episode: 788, duration: 0.018s, episode steps:  35, steps per second: 1972, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22475/100000: episode: 789, duration: 0.024s, episode steps:  43, steps per second: 1811, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 22499/100000: episode: 790, duration: 0.014s, episode steps:  24, steps per second: 1696, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 22548/100000: episode: 791, duration: 0.027s, episode steps:  49, steps per second: 1835, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 22581/100000: episode: 792, duration: 0.018s, episode steps:  33, steps per second: 1792, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 22644/100000: episode: 793, duration: 0.031s, episode steps:  63, steps per second: 2018, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 22681/100000: episode: 794, duration: 0.018s, episode steps:  37, steps per second: 2034, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 22716/100000: episode: 795, duration: 0.019s, episode steps:  35, steps per second: 1846, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 22753/100000: episode: 796, duration: 0.020s, episode steps:  37, steps per second: 1860, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 22777/100000: episode: 797, duration: 0.012s, episode steps:  24, steps per second: 1999, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 22800/100000: episode: 798, duration: 0.012s, episode steps:  23, steps per second: 1888, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 22840/100000: episode: 799, duration: 0.020s, episode steps:  40, steps per second: 2048, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  mean_best_reward: --\n",
      " 22864/100000: episode: 800, duration: 0.015s, episode steps:  24, steps per second: 1650, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 22927/100000: episode: 801, duration: 0.032s, episode steps:  63, steps per second: 1945, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: 86.000000\n",
      " 22970/100000: episode: 802, duration: 0.021s, episode steps:  43, steps per second: 2075, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 23014/100000: episode: 803, duration: 0.024s, episode steps:  44, steps per second: 1856, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 23030/100000: episode: 804, duration: 0.008s, episode steps:  16, steps per second: 1945, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 23047/100000: episode: 805, duration: 0.009s, episode steps:  17, steps per second: 1880, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 23094/100000: episode: 806, duration: 0.023s, episode steps:  47, steps per second: 2036, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 23148/100000: episode: 807, duration: 0.027s, episode steps:  54, steps per second: 2010, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 23193/100000: episode: 808, duration: 0.022s, episode steps:  45, steps per second: 2020, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 23234/100000: episode: 809, duration: 0.023s, episode steps:  41, steps per second: 1770, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  mean_best_reward: --\n",
      " 23253/100000: episode: 810, duration: 0.010s, episode steps:  19, steps per second: 1828, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      " 23297/100000: episode: 811, duration: 0.027s, episode steps:  44, steps per second: 1642, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23323/100000: episode: 812, duration: 0.016s, episode steps:  26, steps per second: 1666, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 23340/100000: episode: 813, duration: 0.008s, episode steps:  17, steps per second: 2001, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 23381/100000: episode: 814, duration: 0.020s, episode steps:  41, steps per second: 2099, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 23398/100000: episode: 815, duration: 0.008s, episode steps:  17, steps per second: 2033, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 23434/100000: episode: 816, duration: 0.018s, episode steps:  36, steps per second: 2024, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 23453/100000: episode: 817, duration: 0.010s, episode steps:  19, steps per second: 1948, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 23500/100000: episode: 818, duration: 0.022s, episode steps:  47, steps per second: 2124, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 23526/100000: episode: 819, duration: 0.012s, episode steps:  26, steps per second: 2095, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23558/100000: episode: 820, duration: 0.016s, episode steps:  32, steps per second: 2042, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23600/100000: episode: 821, duration: 0.021s, episode steps:  42, steps per second: 1974, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 23617/100000: episode: 822, duration: 0.010s, episode steps:  17, steps per second: 1764, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 23656/100000: episode: 823, duration: 0.022s, episode steps:  39, steps per second: 1778, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 23687/100000: episode: 824, duration: 0.015s, episode steps:  31, steps per second: 2064, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 23718/100000: episode: 825, duration: 0.015s, episode steps:  31, steps per second: 2031, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 23749/100000: episode: 826, duration: 0.016s, episode steps:  31, steps per second: 1996, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 23780/100000: episode: 827, duration: 0.015s, episode steps:  31, steps per second: 2057, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 23814/100000: episode: 828, duration: 0.016s, episode steps:  34, steps per second: 2140, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 23861/100000: episode: 829, duration: 0.024s, episode steps:  47, steps per second: 1973, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 23892/100000: episode: 830, duration: 0.016s, episode steps:  31, steps per second: 1992, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 23973/100000: episode: 831, duration: 0.040s, episode steps:  81, steps per second: 2040, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 24020/100000: episode: 832, duration: 0.023s, episode steps:  47, steps per second: 2049, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 24058/100000: episode: 833, duration: 0.018s, episode steps:  38, steps per second: 2133, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24098/100000: episode: 834, duration: 0.019s, episode steps:  40, steps per second: 2058, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 24136/100000: episode: 835, duration: 0.018s, episode steps:  38, steps per second: 2068, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24161/100000: episode: 836, duration: 0.013s, episode steps:  25, steps per second: 1858, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 24205/100000: episode: 837, duration: 0.021s, episode steps:  44, steps per second: 2070, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 24236/100000: episode: 838, duration: 0.014s, episode steps:  31, steps per second: 2160, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 24268/100000: episode: 839, duration: 0.015s, episode steps:  32, steps per second: 2077, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24324/100000: episode: 840, duration: 0.026s, episode steps:  56, steps per second: 2146, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 24343/100000: episode: 841, duration: 0.011s, episode steps:  19, steps per second: 1756, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 24397/100000: episode: 842, duration: 0.025s, episode steps:  54, steps per second: 2155, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 24443/100000: episode: 843, duration: 0.022s, episode steps:  46, steps per second: 2120, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24531/100000: episode: 844, duration: 0.044s, episode steps:  88, steps per second: 2016, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 24596/100000: episode: 845, duration: 0.031s, episode steps:  65, steps per second: 2105, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 24646/100000: episode: 846, duration: 0.023s, episode steps:  50, steps per second: 2195, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 24696/100000: episode: 847, duration: 0.024s, episode steps:  50, steps per second: 2061, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 24775/100000: episode: 848, duration: 0.039s, episode steps:  79, steps per second: 2049, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 24839/100000: episode: 849, duration: 0.031s, episode steps:  64, steps per second: 2092, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 24870/100000: episode: 850, duration: 0.015s, episode steps:  31, steps per second: 2115, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 24912/100000: episode: 851, duration: 0.020s, episode steps:  42, steps per second: 2074, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 115.000000\n",
      " 24941/100000: episode: 852, duration: 0.014s, episode steps:  29, steps per second: 2018, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 24964/100000: episode: 853, duration: 0.013s, episode steps:  23, steps per second: 1832, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 24995/100000: episode: 854, duration: 0.015s, episode steps:  31, steps per second: 2086, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 25037/100000: episode: 855, duration: 0.020s, episode steps:  42, steps per second: 2096, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 25056/100000: episode: 856, duration: 0.009s, episode steps:  19, steps per second: 2006, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  mean_best_reward: --\n",
      " 25084/100000: episode: 857, duration: 0.014s, episode steps:  28, steps per second: 2018, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 25102/100000: episode: 858, duration: 0.009s, episode steps:  18, steps per second: 1998, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 25133/100000: episode: 859, duration: 0.015s, episode steps:  31, steps per second: 2041, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 25162/100000: episode: 860, duration: 0.014s, episode steps:  29, steps per second: 2108, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 25193/100000: episode: 861, duration: 0.015s, episode steps:  31, steps per second: 2025, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 25307/100000: episode: 862, duration: 0.052s, episode steps: 114, steps per second: 2209, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 25367/100000: episode: 863, duration: 0.028s, episode steps:  60, steps per second: 2134, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 25394/100000: episode: 864, duration: 0.015s, episode steps:  27, steps per second: 1750, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 25430/100000: episode: 865, duration: 0.018s, episode steps:  36, steps per second: 2050, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 25498/100000: episode: 866, duration: 0.032s, episode steps:  68, steps per second: 2156, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 25531/100000: episode: 867, duration: 0.016s, episode steps:  33, steps per second: 2070, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 25546/100000: episode: 868, duration: 0.007s, episode steps:  15, steps per second: 2004, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 25599/100000: episode: 869, duration: 0.026s, episode steps:  53, steps per second: 2043, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 25617/100000: episode: 870, duration: 0.009s, episode steps:  18, steps per second: 1905, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 25665/100000: episode: 871, duration: 0.022s, episode steps:  48, steps per second: 2138, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 25679/100000: episode: 872, duration: 0.007s, episode steps:  14, steps per second: 1938, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 25714/100000: episode: 873, duration: 0.017s, episode steps:  35, steps per second: 2058, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 25744/100000: episode: 874, duration: 0.015s, episode steps:  30, steps per second: 2062, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 25798/100000: episode: 875, duration: 0.025s, episode steps:  54, steps per second: 2122, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25848/100000: episode: 876, duration: 0.025s, episode steps:  50, steps per second: 1979, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 25894/100000: episode: 877, duration: 0.023s, episode steps:  46, steps per second: 1973, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 25922/100000: episode: 878, duration: 0.015s, episode steps:  28, steps per second: 1908, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 25939/100000: episode: 879, duration: 0.010s, episode steps:  17, steps per second: 1740, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 25972/100000: episode: 880, duration: 0.016s, episode steps:  33, steps per second: 2069, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 26001/100000: episode: 881, duration: 0.015s, episode steps:  29, steps per second: 1991, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 26046/100000: episode: 882, duration: 0.021s, episode steps:  45, steps per second: 2126, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 26092/100000: episode: 883, duration: 0.022s, episode steps:  46, steps per second: 2113, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 26119/100000: episode: 884, duration: 0.013s, episode steps:  27, steps per second: 2056, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  mean_best_reward: --\n",
      " 26180/100000: episode: 885, duration: 0.029s, episode steps:  61, steps per second: 2133, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 26232/100000: episode: 886, duration: 0.024s, episode steps:  52, steps per second: 2144, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 26268/100000: episode: 887, duration: 0.018s, episode steps:  36, steps per second: 1992, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 26303/100000: episode: 888, duration: 0.017s, episode steps:  35, steps per second: 2027, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 26344/100000: episode: 889, duration: 0.019s, episode steps:  41, steps per second: 2139, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 26369/100000: episode: 890, duration: 0.012s, episode steps:  25, steps per second: 2087, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 26412/100000: episode: 891, duration: 0.021s, episode steps:  43, steps per second: 2072, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 26464/100000: episode: 892, duration: 0.024s, episode steps:  52, steps per second: 2172, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 26498/100000: episode: 893, duration: 0.017s, episode steps:  34, steps per second: 2031, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26523/100000: episode: 894, duration: 0.012s, episode steps:  25, steps per second: 2045, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 26576/100000: episode: 895, duration: 0.025s, episode steps:  53, steps per second: 2109, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 26631/100000: episode: 896, duration: 0.026s, episode steps:  55, steps per second: 2141, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 26656/100000: episode: 897, duration: 0.012s, episode steps:  25, steps per second: 2074, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 26712/100000: episode: 898, duration: 0.028s, episode steps:  56, steps per second: 1985, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 26748/100000: episode: 899, duration: 0.017s, episode steps:  36, steps per second: 2065, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26821/100000: episode: 900, duration: 0.034s, episode steps:  73, steps per second: 2173, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 26845/100000: episode: 901, duration: 0.012s, episode steps:  24, steps per second: 2000, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: 83.500000\n",
      " 26879/100000: episode: 902, duration: 0.017s, episode steps:  34, steps per second: 2013, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 26921/100000: episode: 903, duration: 0.021s, episode steps:  42, steps per second: 2036, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 26975/100000: episode: 904, duration: 0.025s, episode steps:  54, steps per second: 2147, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 27003/100000: episode: 905, duration: 0.013s, episode steps:  28, steps per second: 2089, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27018/100000: episode: 906, duration: 0.008s, episode steps:  15, steps per second: 1841, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 27033/100000: episode: 907, duration: 0.008s, episode steps:  15, steps per second: 1808, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 27054/100000: episode: 908, duration: 0.011s, episode steps:  21, steps per second: 1847, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 27134/100000: episode: 909, duration: 0.039s, episode steps:  80, steps per second: 2026, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 27159/100000: episode: 910, duration: 0.013s, episode steps:  25, steps per second: 1873, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 27170/100000: episode: 911, duration: 0.006s, episode steps:  11, steps per second: 1897, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 27192/100000: episode: 912, duration: 0.011s, episode steps:  22, steps per second: 2023, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 27214/100000: episode: 913, duration: 0.011s, episode steps:  22, steps per second: 1937, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 27239/100000: episode: 914, duration: 0.012s, episode steps:  25, steps per second: 2102, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 27271/100000: episode: 915, duration: 0.016s, episode steps:  32, steps per second: 1992, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.656 [0.000, 1.000],  mean_best_reward: --\n",
      " 27284/100000: episode: 916, duration: 0.007s, episode steps:  13, steps per second: 1796, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 27303/100000: episode: 917, duration: 0.009s, episode steps:  19, steps per second: 2025, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 27331/100000: episode: 918, duration: 0.014s, episode steps:  28, steps per second: 2045, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27361/100000: episode: 919, duration: 0.014s, episode steps:  30, steps per second: 2071, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27401/100000: episode: 920, duration: 0.019s, episode steps:  40, steps per second: 2160, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 27411/100000: episode: 921, duration: 0.007s, episode steps:  10, steps per second: 1519, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 27422/100000: episode: 922, duration: 0.006s, episode steps:  11, steps per second: 1836, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 27443/100000: episode: 923, duration: 0.011s, episode steps:  21, steps per second: 1912, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 27455/100000: episode: 924, duration: 0.006s, episode steps:  12, steps per second: 1942, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 27502/100000: episode: 925, duration: 0.022s, episode steps:  47, steps per second: 2114, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 27514/100000: episode: 926, duration: 0.006s, episode steps:  12, steps per second: 1909, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27543/100000: episode: 927, duration: 0.015s, episode steps:  29, steps per second: 1961, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  mean_best_reward: --\n",
      " 27559/100000: episode: 928, duration: 0.009s, episode steps:  16, steps per second: 1789, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 27572/100000: episode: 929, duration: 0.007s, episode steps:  13, steps per second: 1850, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 27595/100000: episode: 930, duration: 0.012s, episode steps:  23, steps per second: 1998, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 27652/100000: episode: 931, duration: 0.026s, episode steps:  57, steps per second: 2156, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 27661/100000: episode: 932, duration: 0.005s, episode steps:   9, steps per second: 1837, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 27680/100000: episode: 933, duration: 0.010s, episode steps:  19, steps per second: 1917, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      " 27724/100000: episode: 934, duration: 0.021s, episode steps:  44, steps per second: 2131, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 27759/100000: episode: 935, duration: 0.017s, episode steps:  35, steps per second: 2121, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 27802/100000: episode: 936, duration: 0.021s, episode steps:  43, steps per second: 2079, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.395 [0.000, 1.000],  mean_best_reward: --\n",
      " 27813/100000: episode: 937, duration: 0.006s, episode steps:  11, steps per second: 1756, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 27837/100000: episode: 938, duration: 0.012s, episode steps:  24, steps per second: 2024, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 27874/100000: episode: 939, duration: 0.018s, episode steps:  37, steps per second: 2095, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 27895/100000: episode: 940, duration: 0.010s, episode steps:  21, steps per second: 2065, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 27904/100000: episode: 941, duration: 0.005s, episode steps:   9, steps per second: 1768, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      " 27932/100000: episode: 942, duration: 0.014s, episode steps:  28, steps per second: 1996, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 27953/100000: episode: 943, duration: 0.011s, episode steps:  21, steps per second: 1878, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 27966/100000: episode: 944, duration: 0.007s, episode steps:  13, steps per second: 1743, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 28009/100000: episode: 945, duration: 0.021s, episode steps:  43, steps per second: 2082, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 28026/100000: episode: 946, duration: 0.009s, episode steps:  17, steps per second: 1898, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      " 28058/100000: episode: 947, duration: 0.017s, episode steps:  32, steps per second: 1933, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  mean_best_reward: --\n",
      " 28126/100000: episode: 948, duration: 0.032s, episode steps:  68, steps per second: 2113, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  mean_best_reward: --\n",
      " 28147/100000: episode: 949, duration: 0.013s, episode steps:  21, steps per second: 1652, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 28161/100000: episode: 950, duration: 0.011s, episode steps:  14, steps per second: 1259, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 28179/100000: episode: 951, duration: 0.015s, episode steps:  18, steps per second: 1241, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: 66.500000\n",
      " 28204/100000: episode: 952, duration: 0.017s, episode steps:  25, steps per second: 1450, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 28236/100000: episode: 953, duration: 0.021s, episode steps:  32, steps per second: 1505, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 28248/100000: episode: 954, duration: 0.008s, episode steps:  12, steps per second: 1467, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 28287/100000: episode: 955, duration: 0.020s, episode steps:  39, steps per second: 1916, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 28316/100000: episode: 956, duration: 0.016s, episode steps:  29, steps per second: 1863, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 28341/100000: episode: 957, duration: 0.012s, episode steps:  25, steps per second: 2053, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 28353/100000: episode: 958, duration: 0.006s, episode steps:  12, steps per second: 1913, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 28367/100000: episode: 959, duration: 0.008s, episode steps:  14, steps per second: 1821, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 28385/100000: episode: 960, duration: 0.009s, episode steps:  18, steps per second: 2032, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28417/100000: episode: 961, duration: 0.016s, episode steps:  32, steps per second: 2034, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28457/100000: episode: 962, duration: 0.020s, episode steps:  40, steps per second: 2036, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 28475/100000: episode: 963, duration: 0.010s, episode steps:  18, steps per second: 1725, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 28530/100000: episode: 964, duration: 0.026s, episode steps:  55, steps per second: 2095, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 28546/100000: episode: 965, duration: 0.008s, episode steps:  16, steps per second: 1895, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28600/100000: episode: 966, duration: 0.025s, episode steps:  54, steps per second: 2130, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28630/100000: episode: 967, duration: 0.014s, episode steps:  30, steps per second: 2085, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28732/100000: episode: 968, duration: 0.048s, episode steps: 102, steps per second: 2127, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 28753/100000: episode: 969, duration: 0.011s, episode steps:  21, steps per second: 1918, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  mean_best_reward: --\n",
      " 28768/100000: episode: 970, duration: 0.008s, episode steps:  15, steps per second: 1927, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 28857/100000: episode: 971, duration: 0.041s, episode steps:  89, steps per second: 2157, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 28874/100000: episode: 972, duration: 0.009s, episode steps:  17, steps per second: 1920, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 28890/100000: episode: 973, duration: 0.008s, episode steps:  16, steps per second: 1986, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 28903/100000: episode: 974, duration: 0.007s, episode steps:  13, steps per second: 1837, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 28916/100000: episode: 975, duration: 0.007s, episode steps:  13, steps per second: 1942, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  mean_best_reward: --\n",
      " 28941/100000: episode: 976, duration: 0.012s, episode steps:  25, steps per second: 2067, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 28955/100000: episode: 977, duration: 0.008s, episode steps:  14, steps per second: 1821, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: --\n",
      " 28996/100000: episode: 978, duration: 0.019s, episode steps:  41, steps per second: 2168, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 29009/100000: episode: 979, duration: 0.007s, episode steps:  13, steps per second: 1828, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 29025/100000: episode: 980, duration: 0.008s, episode steps:  16, steps per second: 1924, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 29044/100000: episode: 981, duration: 0.010s, episode steps:  19, steps per second: 1982, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 29057/100000: episode: 982, duration: 0.007s, episode steps:  13, steps per second: 1939, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 29084/100000: episode: 983, duration: 0.014s, episode steps:  27, steps per second: 1869, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 29115/100000: episode: 984, duration: 0.015s, episode steps:  31, steps per second: 2107, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 29130/100000: episode: 985, duration: 0.009s, episode steps:  15, steps per second: 1641, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 29155/100000: episode: 986, duration: 0.014s, episode steps:  25, steps per second: 1744, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 29185/100000: episode: 987, duration: 0.016s, episode steps:  30, steps per second: 1916, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 29233/100000: episode: 988, duration: 0.030s, episode steps:  48, steps per second: 1602, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29262/100000: episode: 989, duration: 0.020s, episode steps:  29, steps per second: 1429, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 29289/100000: episode: 990, duration: 0.019s, episode steps:  27, steps per second: 1418, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  mean_best_reward: --\n",
      " 29306/100000: episode: 991, duration: 0.012s, episode steps:  17, steps per second: 1465, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 29343/100000: episode: 992, duration: 0.022s, episode steps:  37, steps per second: 1662, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 29372/100000: episode: 993, duration: 0.017s, episode steps:  29, steps per second: 1699, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 29425/100000: episode: 994, duration: 0.033s, episode steps:  53, steps per second: 1583, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 29446/100000: episode: 995, duration: 0.014s, episode steps:  21, steps per second: 1454, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 29468/100000: episode: 996, duration: 0.017s, episode steps:  22, steps per second: 1314, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29498/100000: episode: 997, duration: 0.021s, episode steps:  30, steps per second: 1436, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  mean_best_reward: --\n",
      " 29543/100000: episode: 998, duration: 0.024s, episode steps:  45, steps per second: 1910, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 29618/100000: episode: 999, duration: 0.035s, episode steps:  75, steps per second: 2158, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 29650/100000: episode: 1000, duration: 0.016s, episode steps:  32, steps per second: 2018, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 29668/100000: episode: 1001, duration: 0.009s, episode steps:  18, steps per second: 1973, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 98.500000\n",
      " 29761/100000: episode: 1002, duration: 0.044s, episode steps:  93, steps per second: 2092, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 29801/100000: episode: 1003, duration: 0.019s, episode steps:  40, steps per second: 2097, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 29839/100000: episode: 1004, duration: 0.020s, episode steps:  38, steps per second: 1873, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 29861/100000: episode: 1005, duration: 0.013s, episode steps:  22, steps per second: 1635, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29880/100000: episode: 1006, duration: 0.010s, episode steps:  19, steps per second: 1897, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 29909/100000: episode: 1007, duration: 0.014s, episode steps:  29, steps per second: 2028, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 29943/100000: episode: 1008, duration: 0.017s, episode steps:  34, steps per second: 2046, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 29968/100000: episode: 1009, duration: 0.012s, episode steps:  25, steps per second: 2084, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 29999/100000: episode: 1010, duration: 0.015s, episode steps:  31, steps per second: 2104, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 30019/100000: episode: 1011, duration: 0.010s, episode steps:  20, steps per second: 1930, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 30061/100000: episode: 1012, duration: 0.020s, episode steps:  42, steps per second: 2078, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30100/100000: episode: 1013, duration: 0.019s, episode steps:  39, steps per second: 2028, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  mean_best_reward: --\n",
      " 30121/100000: episode: 1014, duration: 0.011s, episode steps:  21, steps per second: 1985, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 30172/100000: episode: 1015, duration: 0.024s, episode steps:  51, steps per second: 2134, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 30198/100000: episode: 1016, duration: 0.013s, episode steps:  26, steps per second: 2047, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30252/100000: episode: 1017, duration: 0.027s, episode steps:  54, steps per second: 1970, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 30272/100000: episode: 1018, duration: 0.012s, episode steps:  20, steps per second: 1718, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 30293/100000: episode: 1019, duration: 0.011s, episode steps:  21, steps per second: 1930, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 30319/100000: episode: 1020, duration: 0.014s, episode steps:  26, steps per second: 1800, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 30334/100000: episode: 1021, duration: 0.008s, episode steps:  15, steps per second: 1781, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 30374/100000: episode: 1022, duration: 0.019s, episode steps:  40, steps per second: 2121, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30407/100000: episode: 1023, duration: 0.016s, episode steps:  33, steps per second: 2112, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 30441/100000: episode: 1024, duration: 0.017s, episode steps:  34, steps per second: 2038, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 30479/100000: episode: 1025, duration: 0.019s, episode steps:  38, steps per second: 1998, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 30505/100000: episode: 1026, duration: 0.013s, episode steps:  26, steps per second: 2043, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 30516/100000: episode: 1027, duration: 0.006s, episode steps:  11, steps per second: 1769, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  mean_best_reward: --\n",
      " 30537/100000: episode: 1028, duration: 0.011s, episode steps:  21, steps per second: 1959, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 30605/100000: episode: 1029, duration: 0.032s, episode steps:  68, steps per second: 2148, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 30639/100000: episode: 1030, duration: 0.016s, episode steps:  34, steps per second: 2087, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 30672/100000: episode: 1031, duration: 0.019s, episode steps:  33, steps per second: 1695, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  mean_best_reward: --\n",
      " 30778/100000: episode: 1032, duration: 0.059s, episode steps: 106, steps per second: 1803, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 30807/100000: episode: 1033, duration: 0.019s, episode steps:  29, steps per second: 1567, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 30821/100000: episode: 1034, duration: 0.007s, episode steps:  14, steps per second: 1940, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 30860/100000: episode: 1035, duration: 0.018s, episode steps:  39, steps per second: 2116, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 30887/100000: episode: 1036, duration: 0.014s, episode steps:  27, steps per second: 1893, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 30908/100000: episode: 1037, duration: 0.011s, episode steps:  21, steps per second: 1970, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 30931/100000: episode: 1038, duration: 0.012s, episode steps:  23, steps per second: 1952, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 30964/100000: episode: 1039, duration: 0.016s, episode steps:  33, steps per second: 2078, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 31018/100000: episode: 1040, duration: 0.025s, episode steps:  54, steps per second: 2141, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31036/100000: episode: 1041, duration: 0.011s, episode steps:  18, steps per second: 1608, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 31094/100000: episode: 1042, duration: 0.029s, episode steps:  58, steps per second: 1993, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 31126/100000: episode: 1043, duration: 0.015s, episode steps:  32, steps per second: 2080, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31144/100000: episode: 1044, duration: 0.009s, episode steps:  18, steps per second: 1929, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 31155/100000: episode: 1045, duration: 0.006s, episode steps:  11, steps per second: 1890, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 31177/100000: episode: 1046, duration: 0.011s, episode steps:  22, steps per second: 1937, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  mean_best_reward: --\n",
      " 31224/100000: episode: 1047, duration: 0.023s, episode steps:  47, steps per second: 2086, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 31242/100000: episode: 1048, duration: 0.009s, episode steps:  18, steps per second: 2003, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      " 31263/100000: episode: 1049, duration: 0.010s, episode steps:  21, steps per second: 2003, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 31290/100000: episode: 1050, duration: 0.014s, episode steps:  27, steps per second: 1965, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 31306/100000: episode: 1051, duration: 0.009s, episode steps:  16, steps per second: 1847, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 62.000000\n",
      " 31346/100000: episode: 1052, duration: 0.020s, episode steps:  40, steps per second: 1985, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31387/100000: episode: 1053, duration: 0.021s, episode steps:  41, steps per second: 1972, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 31427/100000: episode: 1054, duration: 0.020s, episode steps:  40, steps per second: 1985, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 31445/100000: episode: 1055, duration: 0.009s, episode steps:  18, steps per second: 2039, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 31476/100000: episode: 1056, duration: 0.017s, episode steps:  31, steps per second: 1868, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31507/100000: episode: 1057, duration: 0.018s, episode steps:  31, steps per second: 1754, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 31556/100000: episode: 1058, duration: 0.023s, episode steps:  49, steps per second: 2088, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 31606/100000: episode: 1059, duration: 0.024s, episode steps:  50, steps per second: 2123, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 31634/100000: episode: 1060, duration: 0.014s, episode steps:  28, steps per second: 1985, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 31655/100000: episode: 1061, duration: 0.011s, episode steps:  21, steps per second: 1980, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 31672/100000: episode: 1062, duration: 0.009s, episode steps:  17, steps per second: 1983, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 31721/100000: episode: 1063, duration: 0.023s, episode steps:  49, steps per second: 2108, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 31783/100000: episode: 1064, duration: 0.030s, episode steps:  62, steps per second: 2092, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31811/100000: episode: 1065, duration: 0.014s, episode steps:  28, steps per second: 2035, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 31849/100000: episode: 1066, duration: 0.018s, episode steps:  38, steps per second: 2100, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31896/100000: episode: 1067, duration: 0.022s, episode steps:  47, steps per second: 2115, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 31930/100000: episode: 1068, duration: 0.017s, episode steps:  34, steps per second: 2013, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 31951/100000: episode: 1069, duration: 0.012s, episode steps:  21, steps per second: 1751, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 31975/100000: episode: 1070, duration: 0.011s, episode steps:  24, steps per second: 2091, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 32027/100000: episode: 1071, duration: 0.025s, episode steps:  52, steps per second: 2060, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32090/100000: episode: 1072, duration: 0.030s, episode steps:  63, steps per second: 2109, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 32110/100000: episode: 1073, duration: 0.010s, episode steps:  20, steps per second: 2048, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32147/100000: episode: 1074, duration: 0.017s, episode steps:  37, steps per second: 2120, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 32170/100000: episode: 1075, duration: 0.012s, episode steps:  23, steps per second: 1937, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 32230/100000: episode: 1076, duration: 0.028s, episode steps:  60, steps per second: 2115, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 32264/100000: episode: 1077, duration: 0.017s, episode steps:  34, steps per second: 2034, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32318/100000: episode: 1078, duration: 0.026s, episode steps:  54, steps per second: 2064, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 32344/100000: episode: 1079, duration: 0.014s, episode steps:  26, steps per second: 1900, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 32394/100000: episode: 1080, duration: 0.025s, episode steps:  50, steps per second: 1972, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32425/100000: episode: 1081, duration: 0.015s, episode steps:  31, steps per second: 2066, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 32460/100000: episode: 1082, duration: 0.017s, episode steps:  35, steps per second: 2047, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 32523/100000: episode: 1083, duration: 0.031s, episode steps:  63, steps per second: 2017, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 32560/100000: episode: 1084, duration: 0.019s, episode steps:  37, steps per second: 1980, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 32589/100000: episode: 1085, duration: 0.014s, episode steps:  29, steps per second: 2147, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 32633/100000: episode: 1086, duration: 0.021s, episode steps:  44, steps per second: 2099, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 32679/100000: episode: 1087, duration: 0.022s, episode steps:  46, steps per second: 2111, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32698/100000: episode: 1088, duration: 0.010s, episode steps:  19, steps per second: 1894, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 32744/100000: episode: 1089, duration: 0.022s, episode steps:  46, steps per second: 2085, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 32761/100000: episode: 1090, duration: 0.010s, episode steps:  17, steps per second: 1664, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 32836/100000: episode: 1091, duration: 0.035s, episode steps:  75, steps per second: 2133, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 32867/100000: episode: 1092, duration: 0.016s, episode steps:  31, steps per second: 1968, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 32886/100000: episode: 1093, duration: 0.010s, episode steps:  19, steps per second: 1939, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 32924/100000: episode: 1094, duration: 0.018s, episode steps:  38, steps per second: 2058, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 32948/100000: episode: 1095, duration: 0.012s, episode steps:  24, steps per second: 2036, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 32983/100000: episode: 1096, duration: 0.017s, episode steps:  35, steps per second: 2047, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 33008/100000: episode: 1097, duration: 0.012s, episode steps:  25, steps per second: 2072, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 33049/100000: episode: 1098, duration: 0.020s, episode steps:  41, steps per second: 2027, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 33086/100000: episode: 1099, duration: 0.018s, episode steps:  37, steps per second: 2095, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 33126/100000: episode: 1100, duration: 0.019s, episode steps:  40, steps per second: 2081, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 33144/100000: episode: 1101, duration: 0.009s, episode steps:  18, steps per second: 1957, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: 60.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33172/100000: episode: 1102, duration: 0.015s, episode steps:  28, steps per second: 1848, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33193/100000: episode: 1103, duration: 0.012s, episode steps:  21, steps per second: 1807, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 33218/100000: episode: 1104, duration: 0.013s, episode steps:  25, steps per second: 1964, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 33233/100000: episode: 1105, duration: 0.008s, episode steps:  15, steps per second: 1986, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 33265/100000: episode: 1106, duration: 0.015s, episode steps:  32, steps per second: 2080, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33280/100000: episode: 1107, duration: 0.008s, episode steps:  15, steps per second: 1816, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 33330/100000: episode: 1108, duration: 0.024s, episode steps:  50, steps per second: 2113, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 33344/100000: episode: 1109, duration: 0.008s, episode steps:  14, steps per second: 1823, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 33358/100000: episode: 1110, duration: 0.007s, episode steps:  14, steps per second: 1967, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 33371/100000: episode: 1111, duration: 0.007s, episode steps:  13, steps per second: 1872, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 33391/100000: episode: 1112, duration: 0.011s, episode steps:  20, steps per second: 1851, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 33410/100000: episode: 1113, duration: 0.010s, episode steps:  19, steps per second: 1987, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 33455/100000: episode: 1114, duration: 0.022s, episode steps:  45, steps per second: 2087, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 33486/100000: episode: 1115, duration: 0.015s, episode steps:  31, steps per second: 2023, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 33504/100000: episode: 1116, duration: 0.010s, episode steps:  18, steps per second: 1865, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      " 33545/100000: episode: 1117, duration: 0.019s, episode steps:  41, steps per second: 2130, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 33564/100000: episode: 1118, duration: 0.010s, episode steps:  19, steps per second: 1842, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 33589/100000: episode: 1119, duration: 0.014s, episode steps:  25, steps per second: 1827, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 33603/100000: episode: 1120, duration: 0.008s, episode steps:  14, steps per second: 1789, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 33616/100000: episode: 1121, duration: 0.009s, episode steps:  13, steps per second: 1401, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 33647/100000: episode: 1122, duration: 0.016s, episode steps:  31, steps per second: 1946, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 33662/100000: episode: 1123, duration: 0.011s, episode steps:  15, steps per second: 1410, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 33715/100000: episode: 1124, duration: 0.025s, episode steps:  53, steps per second: 2108, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 33729/100000: episode: 1125, duration: 0.007s, episode steps:  14, steps per second: 1976, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 33749/100000: episode: 1126, duration: 0.011s, episode steps:  20, steps per second: 1882, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 33772/100000: episode: 1127, duration: 0.011s, episode steps:  23, steps per second: 2110, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 33825/100000: episode: 1128, duration: 0.025s, episode steps:  53, steps per second: 2103, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 33916/100000: episode: 1129, duration: 0.042s, episode steps:  91, steps per second: 2192, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 33930/100000: episode: 1130, duration: 0.008s, episode steps:  14, steps per second: 1817, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 33946/100000: episode: 1131, duration: 0.008s, episode steps:  16, steps per second: 1999, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      " 33978/100000: episode: 1132, duration: 0.017s, episode steps:  32, steps per second: 1847, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34053/100000: episode: 1133, duration: 0.035s, episode steps:  75, steps per second: 2146, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 34101/100000: episode: 1134, duration: 0.023s, episode steps:  48, steps per second: 2123, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34143/100000: episode: 1135, duration: 0.019s, episode steps:  42, steps per second: 2154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34195/100000: episode: 1136, duration: 0.025s, episode steps:  52, steps per second: 2119, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 34243/100000: episode: 1137, duration: 0.022s, episode steps:  48, steps per second: 2144, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 34309/100000: episode: 1138, duration: 0.031s, episode steps:  66, steps per second: 2150, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  mean_best_reward: --\n",
      " 34328/100000: episode: 1139, duration: 0.009s, episode steps:  19, steps per second: 2075, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 34380/100000: episode: 1140, duration: 0.024s, episode steps:  52, steps per second: 2128, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34403/100000: episode: 1141, duration: 0.012s, episode steps:  23, steps per second: 1973, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  mean_best_reward: --\n",
      " 34419/100000: episode: 1142, duration: 0.010s, episode steps:  16, steps per second: 1663, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 34443/100000: episode: 1143, duration: 0.012s, episode steps:  24, steps per second: 1939, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34487/100000: episode: 1144, duration: 0.021s, episode steps:  44, steps per second: 2119, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34518/100000: episode: 1145, duration: 0.015s, episode steps:  31, steps per second: 2067, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 34533/100000: episode: 1146, duration: 0.008s, episode steps:  15, steps per second: 1804, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 34557/100000: episode: 1147, duration: 0.011s, episode steps:  24, steps per second: 2097, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34575/100000: episode: 1148, duration: 0.009s, episode steps:  18, steps per second: 2025, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 34708/100000: episode: 1149, duration: 0.062s, episode steps: 133, steps per second: 2151, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  mean_best_reward: --\n",
      " 34746/100000: episode: 1150, duration: 0.019s, episode steps:  38, steps per second: 2001, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 34769/100000: episode: 1151, duration: 0.012s, episode steps:  23, steps per second: 1881, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: 100.500000\n",
      " 34848/100000: episode: 1152, duration: 0.039s, episode steps:  79, steps per second: 2002, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 34888/100000: episode: 1153, duration: 0.019s, episode steps:  40, steps per second: 2080, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 34905/100000: episode: 1154, duration: 0.010s, episode steps:  17, steps per second: 1708, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 34960/100000: episode: 1155, duration: 0.025s, episode steps:  55, steps per second: 2189, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 35008/100000: episode: 1156, duration: 0.023s, episode steps:  48, steps per second: 2102, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35036/100000: episode: 1157, duration: 0.013s, episode steps:  28, steps per second: 2140, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35067/100000: episode: 1158, duration: 0.016s, episode steps:  31, steps per second: 1914, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 35133/100000: episode: 1159, duration: 0.031s, episode steps:  66, steps per second: 2152, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  mean_best_reward: --\n",
      " 35170/100000: episode: 1160, duration: 0.018s, episode steps:  37, steps per second: 2098, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 35294/100000: episode: 1161, duration: 0.058s, episode steps: 124, steps per second: 2127, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35323/100000: episode: 1162, duration: 0.014s, episode steps:  29, steps per second: 2079, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 35394/100000: episode: 1163, duration: 0.033s, episode steps:  71, steps per second: 2141, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 35446/100000: episode: 1164, duration: 0.025s, episode steps:  52, steps per second: 2096, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35477/100000: episode: 1165, duration: 0.015s, episode steps:  31, steps per second: 2048, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 35512/100000: episode: 1166, duration: 0.017s, episode steps:  35, steps per second: 2115, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 35555/100000: episode: 1167, duration: 0.020s, episode steps:  43, steps per second: 2144, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 35600/100000: episode: 1168, duration: 0.022s, episode steps:  45, steps per second: 2058, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 35625/100000: episode: 1169, duration: 0.012s, episode steps:  25, steps per second: 2022, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 35670/100000: episode: 1170, duration: 0.022s, episode steps:  45, steps per second: 2092, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 35698/100000: episode: 1171, duration: 0.014s, episode steps:  28, steps per second: 2061, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 35740/100000: episode: 1172, duration: 0.022s, episode steps:  42, steps per second: 1932, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35769/100000: episode: 1173, duration: 0.014s, episode steps:  29, steps per second: 2080, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 35821/100000: episode: 1174, duration: 0.025s, episode steps:  52, steps per second: 2097, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 35856/100000: episode: 1175, duration: 0.018s, episode steps:  35, steps per second: 1977, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 35891/100000: episode: 1176, duration: 0.017s, episode steps:  35, steps per second: 2093, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 35919/100000: episode: 1177, duration: 0.015s, episode steps:  28, steps per second: 1891, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 35955/100000: episode: 1178, duration: 0.018s, episode steps:  36, steps per second: 1952, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 35977/100000: episode: 1179, duration: 0.011s, episode steps:  22, steps per second: 1953, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36029/100000: episode: 1180, duration: 0.025s, episode steps:  52, steps per second: 2052, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 36057/100000: episode: 1181, duration: 0.013s, episode steps:  28, steps per second: 2087, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 36109/100000: episode: 1182, duration: 0.024s, episode steps:  52, steps per second: 2147, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36147/100000: episode: 1183, duration: 0.019s, episode steps:  38, steps per second: 2034, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36173/100000: episode: 1184, duration: 0.014s, episode steps:  26, steps per second: 1852, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 36216/100000: episode: 1185, duration: 0.020s, episode steps:  43, steps per second: 2138, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 36267/100000: episode: 1186, duration: 0.025s, episode steps:  51, steps per second: 2072, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 36300/100000: episode: 1187, duration: 0.016s, episode steps:  33, steps per second: 2098, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 36353/100000: episode: 1188, duration: 0.025s, episode steps:  53, steps per second: 2154, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 36414/100000: episode: 1189, duration: 0.029s, episode steps:  61, steps per second: 2125, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 36459/100000: episode: 1190, duration: 0.021s, episode steps:  45, steps per second: 2098, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 36495/100000: episode: 1191, duration: 0.017s, episode steps:  36, steps per second: 2119, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 36531/100000: episode: 1192, duration: 0.017s, episode steps:  36, steps per second: 2144, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 36549/100000: episode: 1193, duration: 0.009s, episode steps:  18, steps per second: 1897, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 36650/100000: episode: 1194, duration: 0.049s, episode steps: 101, steps per second: 2065, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 36687/100000: episode: 1195, duration: 0.017s, episode steps:  37, steps per second: 2123, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 36714/100000: episode: 1196, duration: 0.014s, episode steps:  27, steps per second: 1973, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 36791/100000: episode: 1197, duration: 0.035s, episode steps:  77, steps per second: 2193, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 36832/100000: episode: 1198, duration: 0.020s, episode steps:  41, steps per second: 2086, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 36863/100000: episode: 1199, duration: 0.015s, episode steps:  31, steps per second: 2116, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 36883/100000: episode: 1200, duration: 0.010s, episode steps:  20, steps per second: 2029, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 36924/100000: episode: 1201, duration: 0.020s, episode steps:  41, steps per second: 2089, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: 76.000000\n",
      " 36941/100000: episode: 1202, duration: 0.008s, episode steps:  17, steps per second: 2020, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 36956/100000: episode: 1203, duration: 0.008s, episode steps:  15, steps per second: 1779, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 36985/100000: episode: 1204, duration: 0.014s, episode steps:  29, steps per second: 2069, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 37005/100000: episode: 1205, duration: 0.010s, episode steps:  20, steps per second: 2057, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 37041/100000: episode: 1206, duration: 0.018s, episode steps:  36, steps per second: 2051, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 37057/100000: episode: 1207, duration: 0.008s, episode steps:  16, steps per second: 1898, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 37068/100000: episode: 1208, duration: 0.007s, episode steps:  11, steps per second: 1505, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 37098/100000: episode: 1209, duration: 0.017s, episode steps:  30, steps per second: 1730, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37114/100000: episode: 1210, duration: 0.009s, episode steps:  16, steps per second: 1772, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37140/100000: episode: 1211, duration: 0.014s, episode steps:  26, steps per second: 1889, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37156/100000: episode: 1212, duration: 0.008s, episode steps:  16, steps per second: 2028, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 37185/100000: episode: 1213, duration: 0.014s, episode steps:  29, steps per second: 2057, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 37240/100000: episode: 1214, duration: 0.026s, episode steps:  55, steps per second: 2137, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 37282/100000: episode: 1215, duration: 0.019s, episode steps:  42, steps per second: 2174, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 37316/100000: episode: 1216, duration: 0.016s, episode steps:  34, steps per second: 2077, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 37350/100000: episode: 1217, duration: 0.016s, episode steps:  34, steps per second: 2122, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 37408/100000: episode: 1218, duration: 0.027s, episode steps:  58, steps per second: 2181, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 37425/100000: episode: 1219, duration: 0.008s, episode steps:  17, steps per second: 2001, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 37445/100000: episode: 1220, duration: 0.010s, episode steps:  20, steps per second: 1975, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37464/100000: episode: 1221, duration: 0.010s, episode steps:  19, steps per second: 1890, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      " 37480/100000: episode: 1222, duration: 0.009s, episode steps:  16, steps per second: 1734, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 37493/100000: episode: 1223, duration: 0.007s, episode steps:  13, steps per second: 1869, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 37517/100000: episode: 1224, duration: 0.012s, episode steps:  24, steps per second: 2046, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 37546/100000: episode: 1225, duration: 0.014s, episode steps:  29, steps per second: 2098, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  mean_best_reward: --\n",
      " 37569/100000: episode: 1226, duration: 0.011s, episode steps:  23, steps per second: 2000, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 37596/100000: episode: 1227, duration: 0.013s, episode steps:  27, steps per second: 2099, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 37618/100000: episode: 1228, duration: 0.011s, episode steps:  22, steps per second: 1917, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 37638/100000: episode: 1229, duration: 0.010s, episode steps:  20, steps per second: 1998, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 37680/100000: episode: 1230, duration: 0.021s, episode steps:  42, steps per second: 2026, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 37723/100000: episode: 1231, duration: 0.020s, episode steps:  43, steps per second: 2129, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  mean_best_reward: --\n",
      " 37793/100000: episode: 1232, duration: 0.032s, episode steps:  70, steps per second: 2156, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 37825/100000: episode: 1233, duration: 0.015s, episode steps:  32, steps per second: 2086, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 37851/100000: episode: 1234, duration: 0.013s, episode steps:  26, steps per second: 1955, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37870/100000: episode: 1235, duration: 0.010s, episode steps:  19, steps per second: 1827, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 37883/100000: episode: 1236, duration: 0.007s, episode steps:  13, steps per second: 1755, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 37903/100000: episode: 1237, duration: 0.011s, episode steps:  20, steps per second: 1826, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 37946/100000: episode: 1238, duration: 0.020s, episode steps:  43, steps per second: 2111, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 37999/100000: episode: 1239, duration: 0.025s, episode steps:  53, steps per second: 2118, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 38014/100000: episode: 1240, duration: 0.008s, episode steps:  15, steps per second: 1938, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 38064/100000: episode: 1241, duration: 0.023s, episode steps:  50, steps per second: 2141, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 38078/100000: episode: 1242, duration: 0.007s, episode steps:  14, steps per second: 1898, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 38124/100000: episode: 1243, duration: 0.022s, episode steps:  46, steps per second: 2129, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 38152/100000: episode: 1244, duration: 0.014s, episode steps:  28, steps per second: 2048, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 38186/100000: episode: 1245, duration: 0.016s, episode steps:  34, steps per second: 2117, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 38240/100000: episode: 1246, duration: 0.027s, episode steps:  54, steps per second: 1992, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 38263/100000: episode: 1247, duration: 0.012s, episode steps:  23, steps per second: 1989, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 38285/100000: episode: 1248, duration: 0.013s, episode steps:  22, steps per second: 1659, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 38300/100000: episode: 1249, duration: 0.009s, episode steps:  15, steps per second: 1708, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 38325/100000: episode: 1250, duration: 0.013s, episode steps:  25, steps per second: 1988, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 38355/100000: episode: 1251, duration: 0.014s, episode steps:  30, steps per second: 2131, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 79.500000\n",
      " 38366/100000: episode: 1252, duration: 0.006s, episode steps:  11, steps per second: 1785, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  mean_best_reward: --\n",
      " 38401/100000: episode: 1253, duration: 0.018s, episode steps:  35, steps per second: 1971, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 38418/100000: episode: 1254, duration: 0.009s, episode steps:  17, steps per second: 1946, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 38461/100000: episode: 1255, duration: 0.021s, episode steps:  43, steps per second: 2012, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 38471/100000: episode: 1256, duration: 0.005s, episode steps:  10, steps per second: 1852, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 38487/100000: episode: 1257, duration: 0.008s, episode steps:  16, steps per second: 1943, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 38538/100000: episode: 1258, duration: 0.024s, episode steps:  51, steps per second: 2125, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 38571/100000: episode: 1259, duration: 0.016s, episode steps:  33, steps per second: 2078, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 38600/100000: episode: 1260, duration: 0.014s, episode steps:  29, steps per second: 2071, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 38643/100000: episode: 1261, duration: 0.020s, episode steps:  43, steps per second: 2160, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 38684/100000: episode: 1262, duration: 0.019s, episode steps:  41, steps per second: 2113, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 38711/100000: episode: 1263, duration: 0.016s, episode steps:  27, steps per second: 1732, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  mean_best_reward: --\n",
      " 38742/100000: episode: 1264, duration: 0.015s, episode steps:  31, steps per second: 2093, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 38774/100000: episode: 1265, duration: 0.016s, episode steps:  32, steps per second: 2023, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 38821/100000: episode: 1266, duration: 0.022s, episode steps:  47, steps per second: 2174, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 38868/100000: episode: 1267, duration: 0.022s, episode steps:  47, steps per second: 2093, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 38892/100000: episode: 1268, duration: 0.012s, episode steps:  24, steps per second: 1931, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 38933/100000: episode: 1269, duration: 0.019s, episode steps:  41, steps per second: 2152, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 38954/100000: episode: 1270, duration: 0.011s, episode steps:  21, steps per second: 1984, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 38986/100000: episode: 1271, duration: 0.016s, episode steps:  32, steps per second: 2004, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39003/100000: episode: 1272, duration: 0.009s, episode steps:  17, steps per second: 1830, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 39016/100000: episode: 1273, duration: 0.007s, episode steps:  13, steps per second: 1926, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 39064/100000: episode: 1274, duration: 0.023s, episode steps:  48, steps per second: 2080, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 39089/100000: episode: 1275, duration: 0.012s, episode steps:  25, steps per second: 2159, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 39111/100000: episode: 1276, duration: 0.011s, episode steps:  22, steps per second: 1986, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39146/100000: episode: 1277, duration: 0.018s, episode steps:  35, steps per second: 1906, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 39225/100000: episode: 1278, duration: 0.037s, episode steps:  79, steps per second: 2134, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 39266/100000: episode: 1279, duration: 0.019s, episode steps:  41, steps per second: 2104, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 39283/100000: episode: 1280, duration: 0.008s, episode steps:  17, steps per second: 2020, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 39311/100000: episode: 1281, duration: 0.013s, episode steps:  28, steps per second: 2084, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 39355/100000: episode: 1282, duration: 0.022s, episode steps:  44, steps per second: 2023, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39404/100000: episode: 1283, duration: 0.025s, episode steps:  49, steps per second: 1946, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 39449/100000: episode: 1284, duration: 0.022s, episode steps:  45, steps per second: 2036, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 39462/100000: episode: 1285, duration: 0.007s, episode steps:  13, steps per second: 1869, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 39479/100000: episode: 1286, duration: 0.009s, episode steps:  17, steps per second: 1886, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 39537/100000: episode: 1287, duration: 0.027s, episode steps:  58, steps per second: 2178, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39559/100000: episode: 1288, duration: 0.012s, episode steps:  22, steps per second: 1854, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 39599/100000: episode: 1289, duration: 0.020s, episode steps:  40, steps per second: 1994, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39634/100000: episode: 1290, duration: 0.017s, episode steps:  35, steps per second: 2082, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 39684/100000: episode: 1291, duration: 0.023s, episode steps:  50, steps per second: 2140, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39711/100000: episode: 1292, duration: 0.013s, episode steps:  27, steps per second: 2065, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  mean_best_reward: --\n",
      " 39771/100000: episode: 1293, duration: 0.028s, episode steps:  60, steps per second: 2150, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 39800/100000: episode: 1294, duration: 0.015s, episode steps:  29, steps per second: 1928, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 39854/100000: episode: 1295, duration: 0.025s, episode steps:  54, steps per second: 2189, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 39872/100000: episode: 1296, duration: 0.009s, episode steps:  18, steps per second: 1941, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      " 39901/100000: episode: 1297, duration: 0.014s, episode steps:  29, steps per second: 2098, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 39970/100000: episode: 1298, duration: 0.032s, episode steps:  69, steps per second: 2146, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 40005/100000: episode: 1299, duration: 0.019s, episode steps:  35, steps per second: 1829, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 40035/100000: episode: 1300, duration: 0.015s, episode steps:  30, steps per second: 1964, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  mean_best_reward: --\n",
      " 40059/100000: episode: 1301, duration: 0.012s, episode steps:  24, steps per second: 2012, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: 65.000000\n",
      " 40078/100000: episode: 1302, duration: 0.010s, episode steps:  19, steps per second: 1996, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 40116/100000: episode: 1303, duration: 0.018s, episode steps:  38, steps per second: 2071, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 40129/100000: episode: 1304, duration: 0.007s, episode steps:  13, steps per second: 1904, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 40146/100000: episode: 1305, duration: 0.008s, episode steps:  17, steps per second: 2021, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 40170/100000: episode: 1306, duration: 0.012s, episode steps:  24, steps per second: 1995, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 40226/100000: episode: 1307, duration: 0.026s, episode steps:  56, steps per second: 2160, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 40275/100000: episode: 1308, duration: 0.023s, episode steps:  49, steps per second: 2131, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 40291/100000: episode: 1309, duration: 0.008s, episode steps:  16, steps per second: 1983, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      " 40359/100000: episode: 1310, duration: 0.031s, episode steps:  68, steps per second: 2163, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 40390/100000: episode: 1311, duration: 0.015s, episode steps:  31, steps per second: 2118, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 40416/100000: episode: 1312, duration: 0.014s, episode steps:  26, steps per second: 1915, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 40441/100000: episode: 1313, duration: 0.013s, episode steps:  25, steps per second: 1885, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 40469/100000: episode: 1314, duration: 0.015s, episode steps:  28, steps per second: 1850, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 40500/100000: episode: 1315, duration: 0.016s, episode steps:  31, steps per second: 1898, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 40544/100000: episode: 1316, duration: 0.024s, episode steps:  44, steps per second: 1867, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 40585/100000: episode: 1317, duration: 0.020s, episode steps:  41, steps per second: 2076, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  mean_best_reward: --\n",
      " 40642/100000: episode: 1318, duration: 0.028s, episode steps:  57, steps per second: 2026, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 40674/100000: episode: 1319, duration: 0.015s, episode steps:  32, steps per second: 2126, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 40703/100000: episode: 1320, duration: 0.014s, episode steps:  29, steps per second: 2038, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 40739/100000: episode: 1321, duration: 0.017s, episode steps:  36, steps per second: 2140, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 40766/100000: episode: 1322, duration: 0.013s, episode steps:  27, steps per second: 2023, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40827/100000: episode: 1323, duration: 0.028s, episode steps:  61, steps per second: 2155, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 40854/100000: episode: 1324, duration: 0.017s, episode steps:  27, steps per second: 1603, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  mean_best_reward: --\n",
      " 40890/100000: episode: 1325, duration: 0.018s, episode steps:  36, steps per second: 1950, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 40924/100000: episode: 1326, duration: 0.018s, episode steps:  34, steps per second: 1895, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 40947/100000: episode: 1327, duration: 0.011s, episode steps:  23, steps per second: 2070, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 40961/100000: episode: 1328, duration: 0.008s, episode steps:  14, steps per second: 1833, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      " 40973/100000: episode: 1329, duration: 0.006s, episode steps:  12, steps per second: 1935, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 40992/100000: episode: 1330, duration: 0.010s, episode steps:  19, steps per second: 1906, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 41006/100000: episode: 1331, duration: 0.007s, episode steps:  14, steps per second: 1961, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 41052/100000: episode: 1332, duration: 0.022s, episode steps:  46, steps per second: 2105, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 41097/100000: episode: 1333, duration: 0.021s, episode steps:  45, steps per second: 2162, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 41112/100000: episode: 1334, duration: 0.008s, episode steps:  15, steps per second: 1986, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 41136/100000: episode: 1335, duration: 0.012s, episode steps:  24, steps per second: 2004, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 41182/100000: episode: 1336, duration: 0.022s, episode steps:  46, steps per second: 2101, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 41214/100000: episode: 1337, duration: 0.015s, episode steps:  32, steps per second: 2080, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  mean_best_reward: --\n",
      " 41247/100000: episode: 1338, duration: 0.018s, episode steps:  33, steps per second: 1793, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  mean_best_reward: --\n",
      " 41262/100000: episode: 1339, duration: 0.008s, episode steps:  15, steps per second: 1943, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  mean_best_reward: --\n",
      " 41319/100000: episode: 1340, duration: 0.026s, episode steps:  57, steps per second: 2172, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 41333/100000: episode: 1341, duration: 0.007s, episode steps:  14, steps per second: 1956, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 41387/100000: episode: 1342, duration: 0.026s, episode steps:  54, steps per second: 2046, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 41441/100000: episode: 1343, duration: 0.026s, episode steps:  54, steps per second: 2112, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 41466/100000: episode: 1344, duration: 0.012s, episode steps:  25, steps per second: 2046, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 41487/100000: episode: 1345, duration: 0.012s, episode steps:  21, steps per second: 1780, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 41533/100000: episode: 1346, duration: 0.022s, episode steps:  46, steps per second: 2076, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 41543/100000: episode: 1347, duration: 0.005s, episode steps:  10, steps per second: 1823, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 41562/100000: episode: 1348, duration: 0.010s, episode steps:  19, steps per second: 1962, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 41601/100000: episode: 1349, duration: 0.019s, episode steps:  39, steps per second: 2028, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 41628/100000: episode: 1350, duration: 0.014s, episode steps:  27, steps per second: 1881, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 41656/100000: episode: 1351, duration: 0.016s, episode steps:  28, steps per second: 1701, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 103.500000\n",
      " 41675/100000: episode: 1352, duration: 0.011s, episode steps:  19, steps per second: 1708, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 41701/100000: episode: 1353, duration: 0.013s, episode steps:  26, steps per second: 1951, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 41756/100000: episode: 1354, duration: 0.026s, episode steps:  55, steps per second: 2098, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  mean_best_reward: --\n",
      " 41779/100000: episode: 1355, duration: 0.012s, episode steps:  23, steps per second: 1956, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 41874/100000: episode: 1356, duration: 0.043s, episode steps:  95, steps per second: 2187, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 41925/100000: episode: 1357, duration: 0.024s, episode steps:  51, steps per second: 2131, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 42012/100000: episode: 1358, duration: 0.040s, episode steps:  87, steps per second: 2180, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 42032/100000: episode: 1359, duration: 0.010s, episode steps:  20, steps per second: 2012, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 42053/100000: episode: 1360, duration: 0.011s, episode steps:  21, steps per second: 1976, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 42074/100000: episode: 1361, duration: 0.011s, episode steps:  21, steps per second: 1900, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 42120/100000: episode: 1362, duration: 0.023s, episode steps:  46, steps per second: 1985, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 42141/100000: episode: 1363, duration: 0.011s, episode steps:  21, steps per second: 1980, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 42163/100000: episode: 1364, duration: 0.011s, episode steps:  22, steps per second: 2014, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 42192/100000: episode: 1365, duration: 0.014s, episode steps:  29, steps per second: 2085, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 42233/100000: episode: 1366, duration: 0.019s, episode steps:  41, steps per second: 2108, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 42312/100000: episode: 1367, duration: 0.036s, episode steps:  79, steps per second: 2184, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 42324/100000: episode: 1368, duration: 0.006s, episode steps:  12, steps per second: 1869, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  mean_best_reward: --\n",
      " 42353/100000: episode: 1369, duration: 0.014s, episode steps:  29, steps per second: 2088, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 42405/100000: episode: 1370, duration: 0.025s, episode steps:  52, steps per second: 2103, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 42438/100000: episode: 1371, duration: 0.016s, episode steps:  33, steps per second: 2100, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 42480/100000: episode: 1372, duration: 0.019s, episode steps:  42, steps per second: 2155, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42522/100000: episode: 1373, duration: 0.023s, episode steps:  42, steps per second: 1796, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 42564/100000: episode: 1374, duration: 0.020s, episode steps:  42, steps per second: 2116, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 42597/100000: episode: 1375, duration: 0.016s, episode steps:  33, steps per second: 2051, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 42664/100000: episode: 1376, duration: 0.031s, episode steps:  67, steps per second: 2172, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 42686/100000: episode: 1377, duration: 0.011s, episode steps:  22, steps per second: 2088, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 42709/100000: episode: 1378, duration: 0.012s, episode steps:  23, steps per second: 1966, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 42743/100000: episode: 1379, duration: 0.016s, episode steps:  34, steps per second: 2091, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 42773/100000: episode: 1380, duration: 0.016s, episode steps:  30, steps per second: 1898, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 42821/100000: episode: 1381, duration: 0.024s, episode steps:  48, steps per second: 1972, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 42833/100000: episode: 1382, duration: 0.007s, episode steps:  12, steps per second: 1744, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 42934/100000: episode: 1383, duration: 0.046s, episode steps: 101, steps per second: 2208, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 42999/100000: episode: 1384, duration: 0.031s, episode steps:  65, steps per second: 2069, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 43016/100000: episode: 1385, duration: 0.009s, episode steps:  17, steps per second: 1894, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 43063/100000: episode: 1386, duration: 0.022s, episode steps:  47, steps per second: 2134, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 43079/100000: episode: 1387, duration: 0.008s, episode steps:  16, steps per second: 1987, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43135/100000: episode: 1388, duration: 0.026s, episode steps:  56, steps per second: 2113, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43182/100000: episode: 1389, duration: 0.022s, episode steps:  47, steps per second: 2160, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 43225/100000: episode: 1390, duration: 0.020s, episode steps:  43, steps per second: 2129, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 43263/100000: episode: 1391, duration: 0.018s, episode steps:  38, steps per second: 2102, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 43287/100000: episode: 1392, duration: 0.012s, episode steps:  24, steps per second: 2041, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 43368/100000: episode: 1393, duration: 0.037s, episode steps:  81, steps per second: 2203, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 43389/100000: episode: 1394, duration: 0.011s, episode steps:  21, steps per second: 1877, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 43462/100000: episode: 1395, duration: 0.036s, episode steps:  73, steps per second: 2048, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 43486/100000: episode: 1396, duration: 0.012s, episode steps:  24, steps per second: 1990, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 43514/100000: episode: 1397, duration: 0.014s, episode steps:  28, steps per second: 2020, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43535/100000: episode: 1398, duration: 0.010s, episode steps:  21, steps per second: 2134, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 43598/100000: episode: 1399, duration: 0.030s, episode steps:  63, steps per second: 2112, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 43626/100000: episode: 1400, duration: 0.013s, episode steps:  28, steps per second: 2100, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43668/100000: episode: 1401, duration: 0.021s, episode steps:  42, steps per second: 2045, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 105.000000\n",
      " 43694/100000: episode: 1402, duration: 0.014s, episode steps:  26, steps per second: 1896, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      " 43708/100000: episode: 1403, duration: 0.008s, episode steps:  14, steps per second: 1839, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 43825/100000: episode: 1404, duration: 0.056s, episode steps: 117, steps per second: 2103, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 43842/100000: episode: 1405, duration: 0.009s, episode steps:  17, steps per second: 1929, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      " 43865/100000: episode: 1406, duration: 0.012s, episode steps:  23, steps per second: 1944, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 43892/100000: episode: 1407, duration: 0.015s, episode steps:  27, steps per second: 1805, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 43941/100000: episode: 1408, duration: 0.024s, episode steps:  49, steps per second: 2076, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 43987/100000: episode: 1409, duration: 0.023s, episode steps:  46, steps per second: 1978, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44026/100000: episode: 1410, duration: 0.019s, episode steps:  39, steps per second: 2023, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 44068/100000: episode: 1411, duration: 0.020s, episode steps:  42, steps per second: 2130, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44092/100000: episode: 1412, duration: 0.013s, episode steps:  24, steps per second: 1889, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44133/100000: episode: 1413, duration: 0.020s, episode steps:  41, steps per second: 2086, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 44178/100000: episode: 1414, duration: 0.021s, episode steps:  45, steps per second: 2102, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 44231/100000: episode: 1415, duration: 0.024s, episode steps:  53, steps per second: 2192, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 44292/100000: episode: 1416, duration: 0.029s, episode steps:  61, steps per second: 2076, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44347/100000: episode: 1417, duration: 0.027s, episode steps:  55, steps per second: 2014, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 44381/100000: episode: 1418, duration: 0.017s, episode steps:  34, steps per second: 2009, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 44416/100000: episode: 1419, duration: 0.017s, episode steps:  35, steps per second: 2104, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 44451/100000: episode: 1420, duration: 0.017s, episode steps:  35, steps per second: 2058, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 44481/100000: episode: 1421, duration: 0.016s, episode steps:  30, steps per second: 1904, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44506/100000: episode: 1422, duration: 0.012s, episode steps:  25, steps per second: 2094, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 44530/100000: episode: 1423, duration: 0.012s, episode steps:  24, steps per second: 2020, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44550/100000: episode: 1424, duration: 0.011s, episode steps:  20, steps per second: 1807, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 44566/100000: episode: 1425, duration: 0.008s, episode steps:  16, steps per second: 1970, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 44616/100000: episode: 1426, duration: 0.023s, episode steps:  50, steps per second: 2130, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 44644/100000: episode: 1427, duration: 0.014s, episode steps:  28, steps per second: 1994, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 44663/100000: episode: 1428, duration: 0.010s, episode steps:  19, steps per second: 1863, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 44699/100000: episode: 1429, duration: 0.017s, episode steps:  36, steps per second: 2110, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 44719/100000: episode: 1430, duration: 0.010s, episode steps:  20, steps per second: 1919, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 44758/100000: episode: 1431, duration: 0.019s, episode steps:  39, steps per second: 2016, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 44877/100000: episode: 1432, duration: 0.056s, episode steps: 119, steps per second: 2126, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  mean_best_reward: --\n",
      " 44906/100000: episode: 1433, duration: 0.014s, episode steps:  29, steps per second: 2044, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 44937/100000: episode: 1434, duration: 0.015s, episode steps:  31, steps per second: 2070, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 44978/100000: episode: 1435, duration: 0.019s, episode steps:  41, steps per second: 2126, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 45017/100000: episode: 1436, duration: 0.019s, episode steps:  39, steps per second: 2069, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 45060/100000: episode: 1437, duration: 0.020s, episode steps:  43, steps per second: 2098, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 45100/100000: episode: 1438, duration: 0.021s, episode steps:  40, steps per second: 1937, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45168/100000: episode: 1439, duration: 0.033s, episode steps:  68, steps per second: 2079, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 45216/100000: episode: 1440, duration: 0.024s, episode steps:  48, steps per second: 1994, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 45290/100000: episode: 1441, duration: 0.035s, episode steps:  74, steps per second: 2127, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 45307/100000: episode: 1442, duration: 0.008s, episode steps:  17, steps per second: 2010, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 45339/100000: episode: 1443, duration: 0.015s, episode steps:  32, steps per second: 2126, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 45392/100000: episode: 1444, duration: 0.025s, episode steps:  53, steps per second: 2146, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 45442/100000: episode: 1445, duration: 0.023s, episode steps:  50, steps per second: 2132, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45453/100000: episode: 1446, duration: 0.006s, episode steps:  11, steps per second: 1907, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 45497/100000: episode: 1447, duration: 0.021s, episode steps:  44, steps per second: 2103, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45529/100000: episode: 1448, duration: 0.015s, episode steps:  32, steps per second: 2110, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 45576/100000: episode: 1449, duration: 0.022s, episode steps:  47, steps per second: 2173, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 45631/100000: episode: 1450, duration: 0.026s, episode steps:  55, steps per second: 2142, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 45645/100000: episode: 1451, duration: 0.008s, episode steps:  14, steps per second: 1684, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  mean_best_reward: 73.500000\n",
      " 45680/100000: episode: 1452, duration: 0.018s, episode steps:  35, steps per second: 1988, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 45724/100000: episode: 1453, duration: 0.021s, episode steps:  44, steps per second: 2103, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 45764/100000: episode: 1454, duration: 0.019s, episode steps:  40, steps per second: 2134, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45820/100000: episode: 1455, duration: 0.027s, episode steps:  56, steps per second: 2102, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 45869/100000: episode: 1456, duration: 0.024s, episode steps:  49, steps per second: 2074, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 45924/100000: episode: 1457, duration: 0.025s, episode steps:  55, steps per second: 2159, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 45947/100000: episode: 1458, duration: 0.011s, episode steps:  23, steps per second: 2083, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 45984/100000: episode: 1459, duration: 0.018s, episode steps:  37, steps per second: 2087, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 46030/100000: episode: 1460, duration: 0.022s, episode steps:  46, steps per second: 2103, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46068/100000: episode: 1461, duration: 0.019s, episode steps:  38, steps per second: 2042, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 46104/100000: episode: 1462, duration: 0.018s, episode steps:  36, steps per second: 1980, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 46140/100000: episode: 1463, duration: 0.017s, episode steps:  36, steps per second: 2155, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46165/100000: episode: 1464, duration: 0.013s, episode steps:  25, steps per second: 1989, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 46198/100000: episode: 1465, duration: 0.017s, episode steps:  33, steps per second: 1994, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 46240/100000: episode: 1466, duration: 0.021s, episode steps:  42, steps per second: 1981, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46261/100000: episode: 1467, duration: 0.011s, episode steps:  21, steps per second: 1913, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 46293/100000: episode: 1468, duration: 0.017s, episode steps:  32, steps per second: 1877, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46310/100000: episode: 1469, duration: 0.009s, episode steps:  17, steps per second: 1840, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 46344/100000: episode: 1470, duration: 0.017s, episode steps:  34, steps per second: 2041, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46357/100000: episode: 1471, duration: 0.007s, episode steps:  13, steps per second: 1903, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 46392/100000: episode: 1472, duration: 0.017s, episode steps:  35, steps per second: 2002, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 46407/100000: episode: 1473, duration: 0.008s, episode steps:  15, steps per second: 1944, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 46455/100000: episode: 1474, duration: 0.022s, episode steps:  48, steps per second: 2137, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 46480/100000: episode: 1475, duration: 0.014s, episode steps:  25, steps per second: 1851, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 46512/100000: episode: 1476, duration: 0.017s, episode steps:  32, steps per second: 1929, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 46534/100000: episode: 1477, duration: 0.011s, episode steps:  22, steps per second: 2003, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  mean_best_reward: --\n",
      " 46561/100000: episode: 1478, duration: 0.014s, episode steps:  27, steps per second: 1892, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 46606/100000: episode: 1479, duration: 0.021s, episode steps:  45, steps per second: 2138, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 46665/100000: episode: 1480, duration: 0.027s, episode steps:  59, steps per second: 2170, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 46719/100000: episode: 1481, duration: 0.026s, episode steps:  54, steps per second: 2040, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46742/100000: episode: 1482, duration: 0.011s, episode steps:  23, steps per second: 2063, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 46756/100000: episode: 1483, duration: 0.008s, episode steps:  14, steps per second: 1830, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 46791/100000: episode: 1484, duration: 0.017s, episode steps:  35, steps per second: 2103, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 46817/100000: episode: 1485, duration: 0.013s, episode steps:  26, steps per second: 2014, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46837/100000: episode: 1486, duration: 0.010s, episode steps:  20, steps per second: 2009, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46869/100000: episode: 1487, duration: 0.015s, episode steps:  32, steps per second: 2068, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 46932/100000: episode: 1488, duration: 0.030s, episode steps:  63, steps per second: 2069, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 46955/100000: episode: 1489, duration: 0.011s, episode steps:  23, steps per second: 2052, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 47000/100000: episode: 1490, duration: 0.021s, episode steps:  45, steps per second: 2093, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 47067/100000: episode: 1491, duration: 0.031s, episode steps:  67, steps per second: 2178, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 47117/100000: episode: 1492, duration: 0.025s, episode steps:  50, steps per second: 1968, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 47140/100000: episode: 1493, duration: 0.011s, episode steps:  23, steps per second: 2015, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 47163/100000: episode: 1494, duration: 0.011s, episode steps:  23, steps per second: 2039, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 47207/100000: episode: 1495, duration: 0.021s, episode steps:  44, steps per second: 2080, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 47223/100000: episode: 1496, duration: 0.008s, episode steps:  16, steps per second: 1991, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 47250/100000: episode: 1497, duration: 0.014s, episode steps:  27, steps per second: 1976, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 47320/100000: episode: 1498, duration: 0.032s, episode steps:  70, steps per second: 2207, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47346/100000: episode: 1499, duration: 0.013s, episode steps:  26, steps per second: 1988, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 47372/100000: episode: 1500, duration: 0.016s, episode steps:  26, steps per second: 1651, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      " 47433/100000: episode: 1501, duration: 0.030s, episode steps:  61, steps per second: 2026, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: 84.500000\n",
      " 47462/100000: episode: 1502, duration: 0.015s, episode steps:  29, steps per second: 1933, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 47500/100000: episode: 1503, duration: 0.019s, episode steps:  38, steps per second: 2036, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 47531/100000: episode: 1504, duration: 0.014s, episode steps:  31, steps per second: 2165, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 47569/100000: episode: 1505, duration: 0.018s, episode steps:  38, steps per second: 2139, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 47591/100000: episode: 1506, duration: 0.011s, episode steps:  22, steps per second: 1998, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 47636/100000: episode: 1507, duration: 0.022s, episode steps:  45, steps per second: 2090, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 47664/100000: episode: 1508, duration: 0.013s, episode steps:  28, steps per second: 2093, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 47730/100000: episode: 1509, duration: 0.031s, episode steps:  66, steps per second: 2144, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  mean_best_reward: --\n",
      " 47744/100000: episode: 1510, duration: 0.008s, episode steps:  14, steps per second: 1807, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47768/100000: episode: 1511, duration: 0.012s, episode steps:  24, steps per second: 1976, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47830/100000: episode: 1512, duration: 0.030s, episode steps:  62, steps per second: 2046, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47862/100000: episode: 1513, duration: 0.016s, episode steps:  32, steps per second: 1981, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 47874/100000: episode: 1514, duration: 0.006s, episode steps:  12, steps per second: 1875, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 47914/100000: episode: 1515, duration: 0.019s, episode steps:  40, steps per second: 2143, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 47950/100000: episode: 1516, duration: 0.017s, episode steps:  36, steps per second: 2083, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 47963/100000: episode: 1517, duration: 0.007s, episode steps:  13, steps per second: 1966, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 48056/100000: episode: 1518, duration: 0.044s, episode steps:  93, steps per second: 2123, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 48069/100000: episode: 1519, duration: 0.007s, episode steps:  13, steps per second: 1959, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 48172/100000: episode: 1520, duration: 0.047s, episode steps: 103, steps per second: 2196, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 48198/100000: episode: 1521, duration: 0.013s, episode steps:  26, steps per second: 1941, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 48224/100000: episode: 1522, duration: 0.013s, episode steps:  26, steps per second: 1959, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48246/100000: episode: 1523, duration: 0.011s, episode steps:  22, steps per second: 1938, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 48266/100000: episode: 1524, duration: 0.010s, episode steps:  20, steps per second: 2052, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48286/100000: episode: 1525, duration: 0.011s, episode steps:  20, steps per second: 1748, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48348/100000: episode: 1526, duration: 0.029s, episode steps:  62, steps per second: 2160, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 48378/100000: episode: 1527, duration: 0.014s, episode steps:  30, steps per second: 2103, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 48401/100000: episode: 1528, duration: 0.011s, episode steps:  23, steps per second: 2056, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  mean_best_reward: --\n",
      " 48422/100000: episode: 1529, duration: 0.011s, episode steps:  21, steps per second: 1844, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 48468/100000: episode: 1530, duration: 0.024s, episode steps:  46, steps per second: 1920, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 48496/100000: episode: 1531, duration: 0.015s, episode steps:  28, steps per second: 1816, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 48531/100000: episode: 1532, duration: 0.017s, episode steps:  35, steps per second: 2002, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 48543/100000: episode: 1533, duration: 0.007s, episode steps:  12, steps per second: 1668, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 48564/100000: episode: 1534, duration: 0.011s, episode steps:  21, steps per second: 1949, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 48604/100000: episode: 1535, duration: 0.021s, episode steps:  40, steps per second: 1892, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 48642/100000: episode: 1536, duration: 0.018s, episode steps:  38, steps per second: 2079, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 48661/100000: episode: 1537, duration: 0.009s, episode steps:  19, steps per second: 2024, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 48710/100000: episode: 1538, duration: 0.024s, episode steps:  49, steps per second: 2041, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 48735/100000: episode: 1539, duration: 0.012s, episode steps:  25, steps per second: 2064, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 48807/100000: episode: 1540, duration: 0.033s, episode steps:  72, steps per second: 2160, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 48866/100000: episode: 1541, duration: 0.027s, episode steps:  59, steps per second: 2154, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 48881/100000: episode: 1542, duration: 0.008s, episode steps:  15, steps per second: 1922, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 48928/100000: episode: 1543, duration: 0.022s, episode steps:  47, steps per second: 2159, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 48946/100000: episode: 1544, duration: 0.009s, episode steps:  18, steps per second: 2003, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 48978/100000: episode: 1545, duration: 0.015s, episode steps:  32, steps per second: 2085, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49066/100000: episode: 1546, duration: 0.042s, episode steps:  88, steps per second: 2109, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49104/100000: episode: 1547, duration: 0.018s, episode steps:  38, steps per second: 2064, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 49127/100000: episode: 1548, duration: 0.011s, episode steps:  23, steps per second: 2028, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 49214/100000: episode: 1549, duration: 0.042s, episode steps:  87, steps per second: 2095, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 49257/100000: episode: 1550, duration: 0.020s, episode steps:  43, steps per second: 2100, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 49272/100000: episode: 1551, duration: 0.008s, episode steps:  15, steps per second: 1871, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: 117.500000\n",
      " 49293/100000: episode: 1552, duration: 0.010s, episode steps:  21, steps per second: 2029, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 49317/100000: episode: 1553, duration: 0.013s, episode steps:  24, steps per second: 1911, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 49376/100000: episode: 1554, duration: 0.027s, episode steps:  59, steps per second: 2213, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 49397/100000: episode: 1555, duration: 0.012s, episode steps:  21, steps per second: 1822, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 49418/100000: episode: 1556, duration: 0.011s, episode steps:  21, steps per second: 1863, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 49461/100000: episode: 1557, duration: 0.022s, episode steps:  43, steps per second: 1977, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49503/100000: episode: 1558, duration: 0.021s, episode steps:  42, steps per second: 1976, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 49532/100000: episode: 1559, duration: 0.014s, episode steps:  29, steps per second: 2007, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 49568/100000: episode: 1560, duration: 0.018s, episode steps:  36, steps per second: 2017, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49582/100000: episode: 1561, duration: 0.007s, episode steps:  14, steps per second: 1894, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 49626/100000: episode: 1562, duration: 0.021s, episode steps:  44, steps per second: 2093, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 49752/100000: episode: 1563, duration: 0.061s, episode steps: 126, steps per second: 2066, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 49796/100000: episode: 1564, duration: 0.020s, episode steps:  44, steps per second: 2152, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49823/100000: episode: 1565, duration: 0.014s, episode steps:  27, steps per second: 1999, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 49840/100000: episode: 1566, duration: 0.008s, episode steps:  17, steps per second: 2118, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 49859/100000: episode: 1567, duration: 0.011s, episode steps:  19, steps per second: 1704, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 49879/100000: episode: 1568, duration: 0.010s, episode steps:  20, steps per second: 1906, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 49948/100000: episode: 1569, duration: 0.035s, episode steps:  69, steps per second: 1993, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 49983/100000: episode: 1570, duration: 0.017s, episode steps:  35, steps per second: 2015, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 50020/100000: episode: 1571, duration: 0.018s, episode steps:  37, steps per second: 2037, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 50034/100000: episode: 1572, duration: 0.007s, episode steps:  14, steps per second: 1949, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 50073/100000: episode: 1573, duration: 0.018s, episode steps:  39, steps per second: 2111, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 50100/100000: episode: 1574, duration: 0.013s, episode steps:  27, steps per second: 2033, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 50127/100000: episode: 1575, duration: 0.013s, episode steps:  27, steps per second: 2079, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 50162/100000: episode: 1576, duration: 0.017s, episode steps:  35, steps per second: 2087, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 50181/100000: episode: 1577, duration: 0.010s, episode steps:  19, steps per second: 1998, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 50212/100000: episode: 1578, duration: 0.015s, episode steps:  31, steps per second: 2021, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 50229/100000: episode: 1579, duration: 0.009s, episode steps:  17, steps per second: 1929, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 50263/100000: episode: 1580, duration: 0.016s, episode steps:  34, steps per second: 2131, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 50278/100000: episode: 1581, duration: 0.008s, episode steps:  15, steps per second: 1909, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 50293/100000: episode: 1582, duration: 0.008s, episode steps:  15, steps per second: 1927, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 50373/100000: episode: 1583, duration: 0.037s, episode steps:  80, steps per second: 2136, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 50395/100000: episode: 1584, duration: 0.012s, episode steps:  22, steps per second: 1893, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 50414/100000: episode: 1585, duration: 0.010s, episode steps:  19, steps per second: 1842, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 50443/100000: episode: 1586, duration: 0.014s, episode steps:  29, steps per second: 2097, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 50483/100000: episode: 1587, duration: 0.019s, episode steps:  40, steps per second: 2123, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 50504/100000: episode: 1588, duration: 0.010s, episode steps:  21, steps per second: 2011, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 50521/100000: episode: 1589, duration: 0.009s, episode steps:  17, steps per second: 1886, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 50569/100000: episode: 1590, duration: 0.022s, episode steps:  48, steps per second: 2185, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 50668/100000: episode: 1591, duration: 0.045s, episode steps:  99, steps per second: 2187, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 50700/100000: episode: 1592, duration: 0.015s, episode steps:  32, steps per second: 2101, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 50711/100000: episode: 1593, duration: 0.006s, episode steps:  11, steps per second: 1768, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 50745/100000: episode: 1594, duration: 0.016s, episode steps:  34, steps per second: 2140, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 50776/100000: episode: 1595, duration: 0.015s, episode steps:  31, steps per second: 2047, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 50814/100000: episode: 1596, duration: 0.021s, episode steps:  38, steps per second: 1809, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 50849/100000: episode: 1597, duration: 0.019s, episode steps:  35, steps per second: 1836, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 50864/100000: episode: 1598, duration: 0.008s, episode steps:  15, steps per second: 1899, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 50894/100000: episode: 1599, duration: 0.014s, episode steps:  30, steps per second: 2148, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 50907/100000: episode: 1600, duration: 0.007s, episode steps:  13, steps per second: 1766, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 50944/100000: episode: 1601, duration: 0.018s, episode steps:  37, steps per second: 2080, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: 92.000000\n",
      " 50962/100000: episode: 1602, duration: 0.009s, episode steps:  18, steps per second: 2011, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 50992/100000: episode: 1603, duration: 0.014s, episode steps:  30, steps per second: 2123, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 51016/100000: episode: 1604, duration: 0.012s, episode steps:  24, steps per second: 1966, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 51039/100000: episode: 1605, duration: 0.011s, episode steps:  23, steps per second: 2081, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 51063/100000: episode: 1606, duration: 0.012s, episode steps:  24, steps per second: 2070, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 51094/100000: episode: 1607, duration: 0.015s, episode steps:  31, steps per second: 2001, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 51151/100000: episode: 1608, duration: 0.027s, episode steps:  57, steps per second: 2079, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 51196/100000: episode: 1609, duration: 0.021s, episode steps:  45, steps per second: 2108, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51216/100000: episode: 1610, duration: 0.011s, episode steps:  20, steps per second: 1903, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51241/100000: episode: 1611, duration: 0.013s, episode steps:  25, steps per second: 1919, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 51262/100000: episode: 1612, duration: 0.010s, episode steps:  21, steps per second: 2068, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 51302/100000: episode: 1613, duration: 0.019s, episode steps:  40, steps per second: 2121, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51329/100000: episode: 1614, duration: 0.014s, episode steps:  27, steps per second: 1926, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 51372/100000: episode: 1615, duration: 0.020s, episode steps:  43, steps per second: 2101, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 51408/100000: episode: 1616, duration: 0.017s, episode steps:  36, steps per second: 2155, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51437/100000: episode: 1617, duration: 0.014s, episode steps:  29, steps per second: 2102, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 51465/100000: episode: 1618, duration: 0.014s, episode steps:  28, steps per second: 2004, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 51489/100000: episode: 1619, duration: 0.012s, episode steps:  24, steps per second: 2071, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51500/100000: episode: 1620, duration: 0.006s, episode steps:  11, steps per second: 1713, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 51520/100000: episode: 1621, duration: 0.010s, episode steps:  20, steps per second: 2010, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 51555/100000: episode: 1622, duration: 0.016s, episode steps:  35, steps per second: 2123, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 51586/100000: episode: 1623, duration: 0.015s, episode steps:  31, steps per second: 2055, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 51607/100000: episode: 1624, duration: 0.010s, episode steps:  21, steps per second: 2052, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 51649/100000: episode: 1625, duration: 0.022s, episode steps:  42, steps per second: 1883, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 51693/100000: episode: 1626, duration: 0.021s, episode steps:  44, steps per second: 2101, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51711/100000: episode: 1627, duration: 0.009s, episode steps:  18, steps per second: 1996, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51730/100000: episode: 1628, duration: 0.009s, episode steps:  19, steps per second: 2003, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  mean_best_reward: --\n",
      " 51752/100000: episode: 1629, duration: 0.011s, episode steps:  22, steps per second: 1985, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 51798/100000: episode: 1630, duration: 0.021s, episode steps:  46, steps per second: 2152, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 51829/100000: episode: 1631, duration: 0.015s, episode steps:  31, steps per second: 2075, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 51908/100000: episode: 1632, duration: 0.037s, episode steps:  79, steps per second: 2129, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 51956/100000: episode: 1633, duration: 0.024s, episode steps:  48, steps per second: 2018, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 51998/100000: episode: 1634, duration: 0.021s, episode steps:  42, steps per second: 2000, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 52017/100000: episode: 1635, duration: 0.010s, episode steps:  19, steps per second: 1865, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 52088/100000: episode: 1636, duration: 0.035s, episode steps:  71, steps per second: 2047, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 52117/100000: episode: 1637, duration: 0.014s, episode steps:  29, steps per second: 2009, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 52149/100000: episode: 1638, duration: 0.015s, episode steps:  32, steps per second: 2095, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 52216/100000: episode: 1639, duration: 0.031s, episode steps:  67, steps per second: 2162, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 52233/100000: episode: 1640, duration: 0.009s, episode steps:  17, steps per second: 1907, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      " 52256/100000: episode: 1641, duration: 0.011s, episode steps:  23, steps per second: 2029, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 52314/100000: episode: 1642, duration: 0.028s, episode steps:  58, steps per second: 2106, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 52353/100000: episode: 1643, duration: 0.019s, episode steps:  39, steps per second: 2042, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 52372/100000: episode: 1644, duration: 0.009s, episode steps:  19, steps per second: 2061, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 52396/100000: episode: 1645, duration: 0.012s, episode steps:  24, steps per second: 2050, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52431/100000: episode: 1646, duration: 0.017s, episode steps:  35, steps per second: 2066, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 52453/100000: episode: 1647, duration: 0.011s, episode steps:  22, steps per second: 2068, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 52488/100000: episode: 1648, duration: 0.017s, episode steps:  35, steps per second: 2068, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52500/100000: episode: 1649, duration: 0.007s, episode steps:  12, steps per second: 1730, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 52519/100000: episode: 1650, duration: 0.011s, episode steps:  19, steps per second: 1809, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 52548/100000: episode: 1651, duration: 0.015s, episode steps:  29, steps per second: 1929, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: 63.000000\n",
      " 52568/100000: episode: 1652, duration: 0.010s, episode steps:  20, steps per second: 2007, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52631/100000: episode: 1653, duration: 0.029s, episode steps:  63, steps per second: 2164, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 52656/100000: episode: 1654, duration: 0.012s, episode steps:  25, steps per second: 2016, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 52668/100000: episode: 1655, duration: 0.006s, episode steps:  12, steps per second: 1941, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 52682/100000: episode: 1656, duration: 0.007s, episode steps:  14, steps per second: 1901, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52711/100000: episode: 1657, duration: 0.014s, episode steps:  29, steps per second: 2028, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  mean_best_reward: --\n",
      " 52757/100000: episode: 1658, duration: 0.021s, episode steps:  46, steps per second: 2158, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52776/100000: episode: 1659, duration: 0.011s, episode steps:  19, steps per second: 1769, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 52827/100000: episode: 1660, duration: 0.024s, episode steps:  51, steps per second: 2129, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 52870/100000: episode: 1661, duration: 0.020s, episode steps:  43, steps per second: 2146, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 52892/100000: episode: 1662, duration: 0.011s, episode steps:  22, steps per second: 2043, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 52913/100000: episode: 1663, duration: 0.012s, episode steps:  21, steps per second: 1772, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 52927/100000: episode: 1664, duration: 0.008s, episode steps:  14, steps per second: 1747, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  mean_best_reward: --\n",
      " 52943/100000: episode: 1665, duration: 0.009s, episode steps:  16, steps per second: 1852, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 52980/100000: episode: 1666, duration: 0.018s, episode steps:  37, steps per second: 2089, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 53024/100000: episode: 1667, duration: 0.021s, episode steps:  44, steps per second: 2066, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53048/100000: episode: 1668, duration: 0.012s, episode steps:  24, steps per second: 2045, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 53066/100000: episode: 1669, duration: 0.010s, episode steps:  18, steps per second: 1783, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 53077/100000: episode: 1670, duration: 0.006s, episode steps:  11, steps per second: 1711, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      " 53103/100000: episode: 1671, duration: 0.013s, episode steps:  26, steps per second: 1994, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53118/100000: episode: 1672, duration: 0.008s, episode steps:  15, steps per second: 1780, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 53132/100000: episode: 1673, duration: 0.008s, episode steps:  14, steps per second: 1828, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 53200/100000: episode: 1674, duration: 0.031s, episode steps:  68, steps per second: 2168, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 53231/100000: episode: 1675, duration: 0.015s, episode steps:  31, steps per second: 2098, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 53253/100000: episode: 1676, duration: 0.011s, episode steps:  22, steps per second: 1989, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 53288/100000: episode: 1677, duration: 0.016s, episode steps:  35, steps per second: 2129, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 53316/100000: episode: 1678, duration: 0.014s, episode steps:  28, steps per second: 1934, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 53368/100000: episode: 1679, duration: 0.026s, episode steps:  52, steps per second: 2016, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 53399/100000: episode: 1680, duration: 0.015s, episode steps:  31, steps per second: 2062, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 53434/100000: episode: 1681, duration: 0.017s, episode steps:  35, steps per second: 2031, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 53468/100000: episode: 1682, duration: 0.016s, episode steps:  34, steps per second: 2141, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 53493/100000: episode: 1683, duration: 0.013s, episode steps:  25, steps per second: 1914, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 53511/100000: episode: 1684, duration: 0.009s, episode steps:  18, steps per second: 1913, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      " 53539/100000: episode: 1685, duration: 0.013s, episode steps:  28, steps per second: 2093, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 53594/100000: episode: 1686, duration: 0.026s, episode steps:  55, steps per second: 2150, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 53611/100000: episode: 1687, duration: 0.009s, episode steps:  17, steps per second: 1881, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 53639/100000: episode: 1688, duration: 0.013s, episode steps:  28, steps per second: 2088, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53660/100000: episode: 1689, duration: 0.010s, episode steps:  21, steps per second: 2080, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 53698/100000: episode: 1690, duration: 0.019s, episode steps:  38, steps per second: 2025, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 53712/100000: episode: 1691, duration: 0.007s, episode steps:  14, steps per second: 1929, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53732/100000: episode: 1692, duration: 0.013s, episode steps:  20, steps per second: 1541, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53752/100000: episode: 1693, duration: 0.011s, episode steps:  20, steps per second: 1847, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 53773/100000: episode: 1694, duration: 0.011s, episode steps:  21, steps per second: 1983, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 53795/100000: episode: 1695, duration: 0.012s, episode steps:  22, steps per second: 1907, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 53814/100000: episode: 1696, duration: 0.010s, episode steps:  19, steps per second: 1999, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 53842/100000: episode: 1697, duration: 0.014s, episode steps:  28, steps per second: 2040, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53883/100000: episode: 1698, duration: 0.019s, episode steps:  41, steps per second: 2141, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 53904/100000: episode: 1699, duration: 0.010s, episode steps:  21, steps per second: 2011, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 53946/100000: episode: 1700, duration: 0.020s, episode steps:  42, steps per second: 2105, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 53962/100000: episode: 1701, duration: 0.009s, episode steps:  16, steps per second: 1811, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: 83.500000\n",
      " 54006/100000: episode: 1702, duration: 0.020s, episode steps:  44, steps per second: 2170, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 54061/100000: episode: 1703, duration: 0.026s, episode steps:  55, steps per second: 2094, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 54088/100000: episode: 1704, duration: 0.013s, episode steps:  27, steps per second: 2037, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 54123/100000: episode: 1705, duration: 0.017s, episode steps:  35, steps per second: 2099, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 54146/100000: episode: 1706, duration: 0.012s, episode steps:  23, steps per second: 1906, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 54175/100000: episode: 1707, duration: 0.015s, episode steps:  29, steps per second: 1912, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 54206/100000: episode: 1708, duration: 0.016s, episode steps:  31, steps per second: 1902, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 54259/100000: episode: 1709, duration: 0.027s, episode steps:  53, steps per second: 1957, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 54283/100000: episode: 1710, duration: 0.012s, episode steps:  24, steps per second: 1988, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54340/100000: episode: 1711, duration: 0.027s, episode steps:  57, steps per second: 2136, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 54378/100000: episode: 1712, duration: 0.018s, episode steps:  38, steps per second: 2142, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54393/100000: episode: 1713, duration: 0.009s, episode steps:  15, steps per second: 1745, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 54473/100000: episode: 1714, duration: 0.037s, episode steps:  80, steps per second: 2191, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 54511/100000: episode: 1715, duration: 0.018s, episode steps:  38, steps per second: 2141, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 54555/100000: episode: 1716, duration: 0.021s, episode steps:  44, steps per second: 2124, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 54624/100000: episode: 1717, duration: 0.035s, episode steps:  69, steps per second: 1955, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 54648/100000: episode: 1718, duration: 0.012s, episode steps:  24, steps per second: 1984, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 54683/100000: episode: 1719, duration: 0.017s, episode steps:  35, steps per second: 2026, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 54711/100000: episode: 1720, duration: 0.014s, episode steps:  28, steps per second: 1988, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54758/100000: episode: 1721, duration: 0.022s, episode steps:  47, steps per second: 2096, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 54797/100000: episode: 1722, duration: 0.019s, episode steps:  39, steps per second: 2091, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 54828/100000: episode: 1723, duration: 0.015s, episode steps:  31, steps per second: 2030, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 54872/100000: episode: 1724, duration: 0.021s, episode steps:  44, steps per second: 2139, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 54912/100000: episode: 1725, duration: 0.019s, episode steps:  40, steps per second: 2109, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 54928/100000: episode: 1726, duration: 0.008s, episode steps:  16, steps per second: 1965, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 54976/100000: episode: 1727, duration: 0.023s, episode steps:  48, steps per second: 2107, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55016/100000: episode: 1728, duration: 0.019s, episode steps:  40, steps per second: 2094, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 55035/100000: episode: 1729, duration: 0.010s, episode steps:  19, steps per second: 1885, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 55066/100000: episode: 1730, duration: 0.016s, episode steps:  31, steps per second: 1885, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 55080/100000: episode: 1731, duration: 0.007s, episode steps:  14, steps per second: 1954, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 55171/100000: episode: 1732, duration: 0.043s, episode steps:  91, steps per second: 2121, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 55204/100000: episode: 1733, duration: 0.016s, episode steps:  33, steps per second: 2019, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 55231/100000: episode: 1734, duration: 0.013s, episode steps:  27, steps per second: 2022, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 55252/100000: episode: 1735, duration: 0.011s, episode steps:  21, steps per second: 1920, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 55274/100000: episode: 1736, duration: 0.011s, episode steps:  22, steps per second: 1962, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55287/100000: episode: 1737, duration: 0.007s, episode steps:  13, steps per second: 1929, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 55311/100000: episode: 1738, duration: 0.012s, episode steps:  24, steps per second: 1970, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 55335/100000: episode: 1739, duration: 0.012s, episode steps:  24, steps per second: 2061, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55388/100000: episode: 1740, duration: 0.025s, episode steps:  53, steps per second: 2135, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 55421/100000: episode: 1741, duration: 0.016s, episode steps:  33, steps per second: 2122, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55451/100000: episode: 1742, duration: 0.015s, episode steps:  30, steps per second: 1966, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 55496/100000: episode: 1743, duration: 0.023s, episode steps:  45, steps per second: 1973, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 55516/100000: episode: 1744, duration: 0.010s, episode steps:  20, steps per second: 2076, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 55557/100000: episode: 1745, duration: 0.019s, episode steps:  41, steps per second: 2135, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 55596/100000: episode: 1746, duration: 0.019s, episode steps:  39, steps per second: 2097, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 55618/100000: episode: 1747, duration: 0.011s, episode steps:  22, steps per second: 2006, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  mean_best_reward: --\n",
      " 55646/100000: episode: 1748, duration: 0.013s, episode steps:  28, steps per second: 2084, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 55714/100000: episode: 1749, duration: 0.031s, episode steps:  68, steps per second: 2202, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 55848/100000: episode: 1750, duration: 0.061s, episode steps: 134, steps per second: 2187, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55870/100000: episode: 1751, duration: 0.011s, episode steps:  22, steps per second: 1931, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: 70.000000\n",
      " 55941/100000: episode: 1752, duration: 0.036s, episode steps:  71, steps per second: 1973, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 55973/100000: episode: 1753, duration: 0.015s, episode steps:  32, steps per second: 2072, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 55993/100000: episode: 1754, duration: 0.010s, episode steps:  20, steps per second: 1982, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56018/100000: episode: 1755, duration: 0.013s, episode steps:  25, steps per second: 1968, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 56041/100000: episode: 1756, duration: 0.011s, episode steps:  23, steps per second: 2022, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 56082/100000: episode: 1757, duration: 0.020s, episode steps:  41, steps per second: 2102, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 56159/100000: episode: 1758, duration: 0.036s, episode steps:  77, steps per second: 2151, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 56205/100000: episode: 1759, duration: 0.022s, episode steps:  46, steps per second: 2106, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 56236/100000: episode: 1760, duration: 0.016s, episode steps:  31, steps per second: 1956, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  mean_best_reward: --\n",
      " 56290/100000: episode: 1761, duration: 0.026s, episode steps:  54, steps per second: 2087, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 56312/100000: episode: 1762, duration: 0.012s, episode steps:  22, steps per second: 1804, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 56327/100000: episode: 1763, duration: 0.008s, episode steps:  15, steps per second: 1956, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 56368/100000: episode: 1764, duration: 0.021s, episode steps:  41, steps per second: 1938, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 56383/100000: episode: 1765, duration: 0.008s, episode steps:  15, steps per second: 1848, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 56410/100000: episode: 1766, duration: 0.014s, episode steps:  27, steps per second: 1894, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 56450/100000: episode: 1767, duration: 0.019s, episode steps:  40, steps per second: 2148, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 56463/100000: episode: 1768, duration: 0.007s, episode steps:  13, steps per second: 1753, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 56487/100000: episode: 1769, duration: 0.012s, episode steps:  24, steps per second: 1970, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 56498/100000: episode: 1770, duration: 0.006s, episode steps:  11, steps per second: 1880, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 56536/100000: episode: 1771, duration: 0.019s, episode steps:  38, steps per second: 1998, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56556/100000: episode: 1772, duration: 0.010s, episode steps:  20, steps per second: 2042, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56580/100000: episode: 1773, duration: 0.012s, episode steps:  24, steps per second: 1996, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 56605/100000: episode: 1774, duration: 0.012s, episode steps:  25, steps per second: 2044, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 56641/100000: episode: 1775, duration: 0.017s, episode steps:  36, steps per second: 2109, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56697/100000: episode: 1776, duration: 0.027s, episode steps:  56, steps per second: 2082, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 56726/100000: episode: 1777, duration: 0.016s, episode steps:  29, steps per second: 1809, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 56755/100000: episode: 1778, duration: 0.014s, episode steps:  29, steps per second: 2097, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 56780/100000: episode: 1779, duration: 0.013s, episode steps:  25, steps per second: 1966, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 56798/100000: episode: 1780, duration: 0.009s, episode steps:  18, steps per second: 1974, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 56817/100000: episode: 1781, duration: 0.009s, episode steps:  19, steps per second: 2050, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 56863/100000: episode: 1782, duration: 0.022s, episode steps:  46, steps per second: 2137, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 56880/100000: episode: 1783, duration: 0.009s, episode steps:  17, steps per second: 1941, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 56929/100000: episode: 1784, duration: 0.023s, episode steps:  49, steps per second: 2136, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 56974/100000: episode: 1785, duration: 0.021s, episode steps:  45, steps per second: 2142, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 57007/100000: episode: 1786, duration: 0.015s, episode steps:  33, steps per second: 2147, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 57028/100000: episode: 1787, duration: 0.011s, episode steps:  21, steps per second: 1933, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 57070/100000: episode: 1788, duration: 0.019s, episode steps:  42, steps per second: 2166, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 57102/100000: episode: 1789, duration: 0.015s, episode steps:  32, steps per second: 2070, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57122/100000: episode: 1790, duration: 0.011s, episode steps:  20, steps per second: 1842, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57153/100000: episode: 1791, duration: 0.016s, episode steps:  31, steps per second: 1948, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 57194/100000: episode: 1792, duration: 0.019s, episode steps:  41, steps per second: 2112, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 57210/100000: episode: 1793, duration: 0.008s, episode steps:  16, steps per second: 1888, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57266/100000: episode: 1794, duration: 0.026s, episode steps:  56, steps per second: 2174, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 57303/100000: episode: 1795, duration: 0.017s, episode steps:  37, steps per second: 2137, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 57387/100000: episode: 1796, duration: 0.039s, episode steps:  84, steps per second: 2141, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57434/100000: episode: 1797, duration: 0.022s, episode steps:  47, steps per second: 2157, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 57477/100000: episode: 1798, duration: 0.021s, episode steps:  43, steps per second: 2084, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 57511/100000: episode: 1799, duration: 0.017s, episode steps:  34, steps per second: 1974, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 57537/100000: episode: 1800, duration: 0.013s, episode steps:  26, steps per second: 1940, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 57569/100000: episode: 1801, duration: 0.018s, episode steps:  32, steps per second: 1733, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: 103.500000\n",
      " 57584/100000: episode: 1802, duration: 0.009s, episode steps:  15, steps per second: 1733, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 57600/100000: episode: 1803, duration: 0.008s, episode steps:  16, steps per second: 2008, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 57646/100000: episode: 1804, duration: 0.022s, episode steps:  46, steps per second: 2080, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 57673/100000: episode: 1805, duration: 0.013s, episode steps:  27, steps per second: 2027, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 57728/100000: episode: 1806, duration: 0.026s, episode steps:  55, steps per second: 2124, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  mean_best_reward: --\n",
      " 57771/100000: episode: 1807, duration: 0.020s, episode steps:  43, steps per second: 2107, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 57811/100000: episode: 1808, duration: 0.020s, episode steps:  40, steps per second: 2035, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 57838/100000: episode: 1809, duration: 0.013s, episode steps:  27, steps per second: 2088, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 57849/100000: episode: 1810, duration: 0.006s, episode steps:  11, steps per second: 1776, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 57927/100000: episode: 1811, duration: 0.036s, episode steps:  78, steps per second: 2164, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 57980/100000: episode: 1812, duration: 0.026s, episode steps:  53, steps per second: 2012, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 58059/100000: episode: 1813, duration: 0.037s, episode steps:  79, steps per second: 2155, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 58082/100000: episode: 1814, duration: 0.011s, episode steps:  23, steps per second: 2065, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 58161/100000: episode: 1815, duration: 0.037s, episode steps:  79, steps per second: 2151, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 58209/100000: episode: 1816, duration: 0.023s, episode steps:  48, steps per second: 2123, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 58231/100000: episode: 1817, duration: 0.011s, episode steps:  22, steps per second: 1963, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 58279/100000: episode: 1818, duration: 0.022s, episode steps:  48, steps per second: 2176, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 58305/100000: episode: 1819, duration: 0.014s, episode steps:  26, steps per second: 1918, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 58383/100000: episode: 1820, duration: 0.037s, episode steps:  78, steps per second: 2102, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  mean_best_reward: --\n",
      " 58457/100000: episode: 1821, duration: 0.036s, episode steps:  74, steps per second: 2070, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 58489/100000: episode: 1822, duration: 0.016s, episode steps:  32, steps per second: 2009, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 58525/100000: episode: 1823, duration: 0.017s, episode steps:  36, steps per second: 2090, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 58544/100000: episode: 1824, duration: 0.009s, episode steps:  19, steps per second: 2013, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 58564/100000: episode: 1825, duration: 0.010s, episode steps:  20, steps per second: 1939, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 58643/100000: episode: 1826, duration: 0.036s, episode steps:  79, steps per second: 2193, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 58693/100000: episode: 1827, duration: 0.025s, episode steps:  50, steps per second: 2016, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 58723/100000: episode: 1828, duration: 0.015s, episode steps:  30, steps per second: 1984, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 58749/100000: episode: 1829, duration: 0.013s, episode steps:  26, steps per second: 2001, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 58794/100000: episode: 1830, duration: 0.020s, episode steps:  45, steps per second: 2200, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 58853/100000: episode: 1831, duration: 0.028s, episode steps:  59, steps per second: 2133, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58959/100000: episode: 1832, duration: 0.052s, episode steps: 106, steps per second: 2043, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 59002/100000: episode: 1833, duration: 0.020s, episode steps:  43, steps per second: 2157, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 59045/100000: episode: 1834, duration: 0.021s, episode steps:  43, steps per second: 2005, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 59060/100000: episode: 1835, duration: 0.008s, episode steps:  15, steps per second: 1849, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 59117/100000: episode: 1836, duration: 0.027s, episode steps:  57, steps per second: 2075, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 59213/100000: episode: 1837, duration: 0.043s, episode steps:  96, steps per second: 2232, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59239/100000: episode: 1838, duration: 0.013s, episode steps:  26, steps per second: 2021, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 59282/100000: episode: 1839, duration: 0.020s, episode steps:  43, steps per second: 2136, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 59324/100000: episode: 1840, duration: 0.020s, episode steps:  42, steps per second: 2119, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 59338/100000: episode: 1841, duration: 0.007s, episode steps:  14, steps per second: 1985, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 59384/100000: episode: 1842, duration: 0.024s, episode steps:  46, steps per second: 1934, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 59417/100000: episode: 1843, duration: 0.016s, episode steps:  33, steps per second: 2012, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 59436/100000: episode: 1844, duration: 0.010s, episode steps:  19, steps per second: 1914, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 59561/100000: episode: 1845, duration: 0.058s, episode steps: 125, steps per second: 2163, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 59675/100000: episode: 1846, duration: 0.052s, episode steps: 114, steps per second: 2181, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 59696/100000: episode: 1847, duration: 0.011s, episode steps:  21, steps per second: 1910, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 59713/100000: episode: 1848, duration: 0.009s, episode steps:  17, steps per second: 1971, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 59737/100000: episode: 1849, duration: 0.012s, episode steps:  24, steps per second: 2064, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59765/100000: episode: 1850, duration: 0.014s, episode steps:  28, steps per second: 2042, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 59804/100000: episode: 1851, duration: 0.020s, episode steps:  39, steps per second: 1937, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: 98.000000\n",
      " 59822/100000: episode: 1852, duration: 0.011s, episode steps:  18, steps per second: 1662, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 59847/100000: episode: 1853, duration: 0.013s, episode steps:  25, steps per second: 1963, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 59859/100000: episode: 1854, duration: 0.008s, episode steps:  12, steps per second: 1490, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 59925/100000: episode: 1855, duration: 0.032s, episode steps:  66, steps per second: 2093, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 60004/100000: episode: 1856, duration: 0.037s, episode steps:  79, steps per second: 2161, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 60059/100000: episode: 1857, duration: 0.026s, episode steps:  55, steps per second: 2101, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 60099/100000: episode: 1858, duration: 0.019s, episode steps:  40, steps per second: 2127, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 60139/100000: episode: 1859, duration: 0.019s, episode steps:  40, steps per second: 2071, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 60188/100000: episode: 1860, duration: 0.023s, episode steps:  49, steps per second: 2139, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 60306/100000: episode: 1861, duration: 0.056s, episode steps: 118, steps per second: 2102, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 60340/100000: episode: 1862, duration: 0.017s, episode steps:  34, steps per second: 2025, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 60368/100000: episode: 1863, duration: 0.014s, episode steps:  28, steps per second: 2022, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 60450/100000: episode: 1864, duration: 0.037s, episode steps:  82, steps per second: 2200, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 60485/100000: episode: 1865, duration: 0.017s, episode steps:  35, steps per second: 2092, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 60536/100000: episode: 1866, duration: 0.023s, episode steps:  51, steps per second: 2173, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 60594/100000: episode: 1867, duration: 0.027s, episode steps:  58, steps per second: 2138, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 60627/100000: episode: 1868, duration: 0.017s, episode steps:  33, steps per second: 1953, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 60676/100000: episode: 1869, duration: 0.023s, episode steps:  49, steps per second: 2127, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 60748/100000: episode: 1870, duration: 0.035s, episode steps:  72, steps per second: 2068, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 60761/100000: episode: 1871, duration: 0.008s, episode steps:  13, steps per second: 1679, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 60796/100000: episode: 1872, duration: 0.016s, episode steps:  35, steps per second: 2127, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 60845/100000: episode: 1873, duration: 0.023s, episode steps:  49, steps per second: 2109, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  mean_best_reward: --\n",
      " 60882/100000: episode: 1874, duration: 0.018s, episode steps:  37, steps per second: 2016, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 60898/100000: episode: 1875, duration: 0.008s, episode steps:  16, steps per second: 1936, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 60938/100000: episode: 1876, duration: 0.019s, episode steps:  40, steps per second: 2091, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 60987/100000: episode: 1877, duration: 0.023s, episode steps:  49, steps per second: 2088, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 61018/100000: episode: 1878, duration: 0.016s, episode steps:  31, steps per second: 1955, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 61075/100000: episode: 1879, duration: 0.028s, episode steps:  57, steps per second: 2023, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 61144/100000: episode: 1880, duration: 0.031s, episode steps:  69, steps per second: 2193, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61174/100000: episode: 1881, duration: 0.016s, episode steps:  30, steps per second: 1918, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 61187/100000: episode: 1882, duration: 0.007s, episode steps:  13, steps per second: 1745, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 61218/100000: episode: 1883, duration: 0.016s, episode steps:  31, steps per second: 1970, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 61232/100000: episode: 1884, duration: 0.008s, episode steps:  14, steps per second: 1840, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  mean_best_reward: --\n",
      " 61288/100000: episode: 1885, duration: 0.028s, episode steps:  56, steps per second: 1994, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 61317/100000: episode: 1886, duration: 0.014s, episode steps:  29, steps per second: 2134, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 61355/100000: episode: 1887, duration: 0.019s, episode steps:  38, steps per second: 2019, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 61391/100000: episode: 1888, duration: 0.017s, episode steps:  36, steps per second: 2133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 61440/100000: episode: 1889, duration: 0.023s, episode steps:  49, steps per second: 2130, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 61451/100000: episode: 1890, duration: 0.006s, episode steps:  11, steps per second: 1812, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 61498/100000: episode: 1891, duration: 0.022s, episode steps:  47, steps per second: 2129, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 61549/100000: episode: 1892, duration: 0.024s, episode steps:  51, steps per second: 2128, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 61576/100000: episode: 1893, duration: 0.013s, episode steps:  27, steps per second: 2064, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 61594/100000: episode: 1894, duration: 0.011s, episode steps:  18, steps per second: 1645, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  mean_best_reward: --\n",
      " 61640/100000: episode: 1895, duration: 0.022s, episode steps:  46, steps per second: 2099, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 61660/100000: episode: 1896, duration: 0.010s, episode steps:  20, steps per second: 1907, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 61717/100000: episode: 1897, duration: 0.026s, episode steps:  57, steps per second: 2186, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 61753/100000: episode: 1898, duration: 0.017s, episode steps:  36, steps per second: 2116, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 61791/100000: episode: 1899, duration: 0.018s, episode steps:  38, steps per second: 2095, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 61810/100000: episode: 1900, duration: 0.010s, episode steps:  19, steps per second: 1977, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 61828/100000: episode: 1901, duration: 0.010s, episode steps:  18, steps per second: 1812, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: 78.000000\n",
      " 61871/100000: episode: 1902, duration: 0.020s, episode steps:  43, steps per second: 2154, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 61920/100000: episode: 1903, duration: 0.024s, episode steps:  49, steps per second: 2081, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 61953/100000: episode: 1904, duration: 0.017s, episode steps:  33, steps per second: 1982, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 62034/100000: episode: 1905, duration: 0.041s, episode steps:  81, steps per second: 1990, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 62075/100000: episode: 1906, duration: 0.020s, episode steps:  41, steps per second: 2093, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 62111/100000: episode: 1907, duration: 0.017s, episode steps:  36, steps per second: 2130, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 62152/100000: episode: 1908, duration: 0.020s, episode steps:  41, steps per second: 2032, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 62233/100000: episode: 1909, duration: 0.039s, episode steps:  81, steps per second: 2081, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 62293/100000: episode: 1910, duration: 0.029s, episode steps:  60, steps per second: 2099, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 62325/100000: episode: 1911, duration: 0.016s, episode steps:  32, steps per second: 2064, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 62364/100000: episode: 1912, duration: 0.018s, episode steps:  39, steps per second: 2123, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 62388/100000: episode: 1913, duration: 0.013s, episode steps:  24, steps per second: 1871, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 62443/100000: episode: 1914, duration: 0.025s, episode steps:  55, steps per second: 2206, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 62477/100000: episode: 1915, duration: 0.018s, episode steps:  34, steps per second: 1855, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 62489/100000: episode: 1916, duration: 0.006s, episode steps:  12, steps per second: 1885, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 62503/100000: episode: 1917, duration: 0.008s, episode steps:  14, steps per second: 1749, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 62540/100000: episode: 1918, duration: 0.018s, episode steps:  37, steps per second: 2048, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 62554/100000: episode: 1919, duration: 0.007s, episode steps:  14, steps per second: 1974, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 62570/100000: episode: 1920, duration: 0.009s, episode steps:  16, steps per second: 1822, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 62611/100000: episode: 1921, duration: 0.019s, episode steps:  41, steps per second: 2142, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 62650/100000: episode: 1922, duration: 0.019s, episode steps:  39, steps per second: 2073, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  mean_best_reward: --\n",
      " 62690/100000: episode: 1923, duration: 0.019s, episode steps:  40, steps per second: 2078, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 62764/100000: episode: 1924, duration: 0.034s, episode steps:  74, steps per second: 2184, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 62803/100000: episode: 1925, duration: 0.019s, episode steps:  39, steps per second: 2086, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 62842/100000: episode: 1926, duration: 0.019s, episode steps:  39, steps per second: 2017, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 62877/100000: episode: 1927, duration: 0.017s, episode steps:  35, steps per second: 2027, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62896/100000: episode: 1928, duration: 0.010s, episode steps:  19, steps per second: 1822, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 62973/100000: episode: 1929, duration: 0.037s, episode steps:  77, steps per second: 2069, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 63005/100000: episode: 1930, duration: 0.015s, episode steps:  32, steps per second: 2066, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63073/100000: episode: 1931, duration: 0.032s, episode steps:  68, steps per second: 2114, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63090/100000: episode: 1932, duration: 0.008s, episode steps:  17, steps per second: 2016, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 63144/100000: episode: 1933, duration: 0.025s, episode steps:  54, steps per second: 2120, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 63160/100000: episode: 1934, duration: 0.008s, episode steps:  16, steps per second: 2017, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 63178/100000: episode: 1935, duration: 0.009s, episode steps:  18, steps per second: 1915, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63240/100000: episode: 1936, duration: 0.029s, episode steps:  62, steps per second: 2167, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63277/100000: episode: 1937, duration: 0.019s, episode steps:  37, steps per second: 1996, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 63323/100000: episode: 1938, duration: 0.025s, episode steps:  46, steps per second: 1875, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63346/100000: episode: 1939, duration: 0.013s, episode steps:  23, steps per second: 1791, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 63365/100000: episode: 1940, duration: 0.011s, episode steps:  19, steps per second: 1769, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 63379/100000: episode: 1941, duration: 0.007s, episode steps:  14, steps per second: 1976, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 63413/100000: episode: 1942, duration: 0.016s, episode steps:  34, steps per second: 2066, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63473/100000: episode: 1943, duration: 0.029s, episode steps:  60, steps per second: 2083, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63545/100000: episode: 1944, duration: 0.033s, episode steps:  72, steps per second: 2164, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 63568/100000: episode: 1945, duration: 0.011s, episode steps:  23, steps per second: 2056, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 63590/100000: episode: 1946, duration: 0.011s, episode steps:  22, steps per second: 2052, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 63638/100000: episode: 1947, duration: 0.023s, episode steps:  48, steps per second: 2119, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63658/100000: episode: 1948, duration: 0.010s, episode steps:  20, steps per second: 2045, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63740/100000: episode: 1949, duration: 0.040s, episode steps:  82, steps per second: 2032, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 63751/100000: episode: 1950, duration: 0.007s, episode steps:  11, steps per second: 1633, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  mean_best_reward: --\n",
      " 63781/100000: episode: 1951, duration: 0.015s, episode steps:  30, steps per second: 1963, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 84.500000\n",
      " 63849/100000: episode: 1952, duration: 0.034s, episode steps:  68, steps per second: 1975, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 63890/100000: episode: 1953, duration: 0.019s, episode steps:  41, steps per second: 2145, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 63945/100000: episode: 1954, duration: 0.026s, episode steps:  55, steps per second: 2090, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 63981/100000: episode: 1955, duration: 0.017s, episode steps:  36, steps per second: 2075, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 64026/100000: episode: 1956, duration: 0.021s, episode steps:  45, steps per second: 2156, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 64039/100000: episode: 1957, duration: 0.007s, episode steps:  13, steps per second: 1752, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  mean_best_reward: --\n",
      " 64083/100000: episode: 1958, duration: 0.020s, episode steps:  44, steps per second: 2162, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 64100/100000: episode: 1959, duration: 0.009s, episode steps:  17, steps per second: 1885, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 64135/100000: episode: 1960, duration: 0.016s, episode steps:  35, steps per second: 2130, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 64175/100000: episode: 1961, duration: 0.020s, episode steps:  40, steps per second: 1954, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64250/100000: episode: 1962, duration: 0.035s, episode steps:  75, steps per second: 2150, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 64274/100000: episode: 1963, duration: 0.011s, episode steps:  24, steps per second: 2094, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 64308/100000: episode: 1964, duration: 0.016s, episode steps:  34, steps per second: 2074, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 64368/100000: episode: 1965, duration: 0.028s, episode steps:  60, steps per second: 2165, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 64401/100000: episode: 1966, duration: 0.016s, episode steps:  33, steps per second: 2095, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  mean_best_reward: --\n",
      " 64452/100000: episode: 1967, duration: 0.025s, episode steps:  51, steps per second: 2003, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 64479/100000: episode: 1968, duration: 0.013s, episode steps:  27, steps per second: 2000, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 64504/100000: episode: 1969, duration: 0.012s, episode steps:  25, steps per second: 2097, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 64532/100000: episode: 1970, duration: 0.014s, episode steps:  28, steps per second: 1934, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 64557/100000: episode: 1971, duration: 0.012s, episode steps:  25, steps per second: 2035, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64607/100000: episode: 1972, duration: 0.024s, episode steps:  50, steps per second: 2092, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64677/100000: episode: 1973, duration: 0.035s, episode steps:  70, steps per second: 2011, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 64727/100000: episode: 1974, duration: 0.024s, episode steps:  50, steps per second: 2121, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 64777/100000: episode: 1975, duration: 0.024s, episode steps:  50, steps per second: 2068, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64803/100000: episode: 1976, duration: 0.013s, episode steps:  26, steps per second: 2056, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 64955/100000: episode: 1977, duration: 0.070s, episode steps: 152, steps per second: 2170, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 65018/100000: episode: 1978, duration: 0.029s, episode steps:  63, steps per second: 2176, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 65041/100000: episode: 1979, duration: 0.014s, episode steps:  23, steps per second: 1652, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 65139/100000: episode: 1980, duration: 0.047s, episode steps:  98, steps per second: 2086, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 65162/100000: episode: 1981, duration: 0.012s, episode steps:  23, steps per second: 1977, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 65251/100000: episode: 1982, duration: 0.040s, episode steps:  89, steps per second: 2216, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 65285/100000: episode: 1983, duration: 0.016s, episode steps:  34, steps per second: 2069, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 65314/100000: episode: 1984, duration: 0.014s, episode steps:  29, steps per second: 2072, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 65351/100000: episode: 1985, duration: 0.018s, episode steps:  37, steps per second: 2109, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 65387/100000: episode: 1986, duration: 0.017s, episode steps:  36, steps per second: 2140, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 65466/100000: episode: 1987, duration: 0.038s, episode steps:  79, steps per second: 2072, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 65515/100000: episode: 1988, duration: 0.024s, episode steps:  49, steps per second: 2014, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 65534/100000: episode: 1989, duration: 0.009s, episode steps:  19, steps per second: 2042, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 65582/100000: episode: 1990, duration: 0.023s, episode steps:  48, steps per second: 2084, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 65650/100000: episode: 1991, duration: 0.032s, episode steps:  68, steps per second: 2133, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 65689/100000: episode: 1992, duration: 0.019s, episode steps:  39, steps per second: 2084, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 65754/100000: episode: 1993, duration: 0.030s, episode steps:  65, steps per second: 2146, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 65809/100000: episode: 1994, duration: 0.026s, episode steps:  55, steps per second: 2156, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 65848/100000: episode: 1995, duration: 0.020s, episode steps:  39, steps per second: 1973, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 65914/100000: episode: 1996, duration: 0.033s, episode steps:  66, steps per second: 2029, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 65954/100000: episode: 1997, duration: 0.019s, episode steps:  40, steps per second: 2119, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 65988/100000: episode: 1998, duration: 0.017s, episode steps:  34, steps per second: 2048, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 65999/100000: episode: 1999, duration: 0.006s, episode steps:  11, steps per second: 1905, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 66012/100000: episode: 2000, duration: 0.007s, episode steps:  13, steps per second: 1888, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 66049/100000: episode: 2001, duration: 0.019s, episode steps:  37, steps per second: 1931, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: 66.000000\n",
      " 66086/100000: episode: 2002, duration: 0.018s, episode steps:  37, steps per second: 2086, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 66113/100000: episode: 2003, duration: 0.013s, episode steps:  27, steps per second: 2077, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 66149/100000: episode: 2004, duration: 0.017s, episode steps:  36, steps per second: 2071, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 66244/100000: episode: 2005, duration: 0.043s, episode steps:  95, steps per second: 2197, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 66263/100000: episode: 2006, duration: 0.009s, episode steps:  19, steps per second: 2010, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 66295/100000: episode: 2007, duration: 0.016s, episode steps:  32, steps per second: 2060, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 66351/100000: episode: 2008, duration: 0.027s, episode steps:  56, steps per second: 2049, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66396/100000: episode: 2009, duration: 0.021s, episode steps:  45, steps per second: 2095, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 66432/100000: episode: 2010, duration: 0.017s, episode steps:  36, steps per second: 2106, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66463/100000: episode: 2011, duration: 0.015s, episode steps:  31, steps per second: 2044, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 66500/100000: episode: 2012, duration: 0.017s, episode steps:  37, steps per second: 2133, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 66519/100000: episode: 2013, duration: 0.009s, episode steps:  19, steps per second: 2062, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 66536/100000: episode: 2014, duration: 0.009s, episode steps:  17, steps per second: 1895, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 66602/100000: episode: 2015, duration: 0.030s, episode steps:  66, steps per second: 2168, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 66627/100000: episode: 2016, duration: 0.012s, episode steps:  25, steps per second: 2159, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 66672/100000: episode: 2017, duration: 0.022s, episode steps:  45, steps per second: 2065, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 66746/100000: episode: 2018, duration: 0.035s, episode steps:  74, steps per second: 2109, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66776/100000: episode: 2019, duration: 0.016s, episode steps:  30, steps per second: 1922, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 66808/100000: episode: 2020, duration: 0.016s, episode steps:  32, steps per second: 2006, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66829/100000: episode: 2021, duration: 0.011s, episode steps:  21, steps per second: 1912, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 66863/100000: episode: 2022, duration: 0.017s, episode steps:  34, steps per second: 1972, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 66930/100000: episode: 2023, duration: 0.032s, episode steps:  67, steps per second: 2121, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 66963/100000: episode: 2024, duration: 0.017s, episode steps:  33, steps per second: 1921, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 66989/100000: episode: 2025, duration: 0.013s, episode steps:  26, steps per second: 1999, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67015/100000: episode: 2026, duration: 0.013s, episode steps:  26, steps per second: 2052, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67028/100000: episode: 2027, duration: 0.007s, episode steps:  13, steps per second: 1914, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 67042/100000: episode: 2028, duration: 0.007s, episode steps:  14, steps per second: 1913, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 67071/100000: episode: 2029, duration: 0.014s, episode steps:  29, steps per second: 2049, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 67106/100000: episode: 2030, duration: 0.017s, episode steps:  35, steps per second: 2120, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 67137/100000: episode: 2031, duration: 0.015s, episode steps:  31, steps per second: 2030, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 67151/100000: episode: 2032, duration: 0.007s, episode steps:  14, steps per second: 1997, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 67180/100000: episode: 2033, duration: 0.015s, episode steps:  29, steps per second: 1899, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 67204/100000: episode: 2034, duration: 0.015s, episode steps:  24, steps per second: 1626, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67223/100000: episode: 2035, duration: 0.010s, episode steps:  19, steps per second: 1926, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 67248/100000: episode: 2036, duration: 0.012s, episode steps:  25, steps per second: 2004, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 67280/100000: episode: 2037, duration: 0.015s, episode steps:  32, steps per second: 2116, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67353/100000: episode: 2038, duration: 0.033s, episode steps:  73, steps per second: 2183, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 67383/100000: episode: 2039, duration: 0.015s, episode steps:  30, steps per second: 2038, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67412/100000: episode: 2040, duration: 0.014s, episode steps:  29, steps per second: 2099, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 67494/100000: episode: 2041, duration: 0.038s, episode steps:  82, steps per second: 2172, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67518/100000: episode: 2042, duration: 0.012s, episode steps:  24, steps per second: 2038, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67645/100000: episode: 2043, duration: 0.059s, episode steps: 127, steps per second: 2147, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 67662/100000: episode: 2044, duration: 0.009s, episode steps:  17, steps per second: 1997, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 67690/100000: episode: 2045, duration: 0.014s, episode steps:  28, steps per second: 2013, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 67729/100000: episode: 2046, duration: 0.018s, episode steps:  39, steps per second: 2123, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 67781/100000: episode: 2047, duration: 0.025s, episode steps:  52, steps per second: 2108, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 67828/100000: episode: 2048, duration: 0.022s, episode steps:  47, steps per second: 2097, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 67865/100000: episode: 2049, duration: 0.018s, episode steps:  37, steps per second: 2089, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 67880/100000: episode: 2050, duration: 0.009s, episode steps:  15, steps per second: 1665, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 67933/100000: episode: 2051, duration: 0.025s, episode steps:  53, steps per second: 2114, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: 70.500000\n",
      " 68006/100000: episode: 2052, duration: 0.035s, episode steps:  73, steps per second: 2101, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 68025/100000: episode: 2053, duration: 0.010s, episode steps:  19, steps per second: 1951, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 68051/100000: episode: 2054, duration: 0.013s, episode steps:  26, steps per second: 2042, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      " 68085/100000: episode: 2055, duration: 0.018s, episode steps:  34, steps per second: 1913, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68133/100000: episode: 2056, duration: 0.023s, episode steps:  48, steps per second: 2108, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68157/100000: episode: 2057, duration: 0.012s, episode steps:  24, steps per second: 1973, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68208/100000: episode: 2058, duration: 0.024s, episode steps:  51, steps per second: 2120, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 68259/100000: episode: 2059, duration: 0.024s, episode steps:  51, steps per second: 2133, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 68329/100000: episode: 2060, duration: 0.032s, episode steps:  70, steps per second: 2172, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68346/100000: episode: 2061, duration: 0.009s, episode steps:  17, steps per second: 1974, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 68369/100000: episode: 2062, duration: 0.012s, episode steps:  23, steps per second: 1915, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 68416/100000: episode: 2063, duration: 0.022s, episode steps:  47, steps per second: 2142, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 68432/100000: episode: 2064, duration: 0.009s, episode steps:  16, steps per second: 1829, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 68470/100000: episode: 2065, duration: 0.018s, episode steps:  38, steps per second: 2148, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68490/100000: episode: 2066, duration: 0.010s, episode steps:  20, steps per second: 1938, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68550/100000: episode: 2067, duration: 0.030s, episode steps:  60, steps per second: 2024, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68591/100000: episode: 2068, duration: 0.020s, episode steps:  41, steps per second: 2059, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 68632/100000: episode: 2069, duration: 0.020s, episode steps:  41, steps per second: 2078, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 68736/100000: episode: 2070, duration: 0.049s, episode steps: 104, steps per second: 2115, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 68788/100000: episode: 2071, duration: 0.026s, episode steps:  52, steps per second: 1998, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 68823/100000: episode: 2072, duration: 0.017s, episode steps:  35, steps per second: 2083, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 68876/100000: episode: 2073, duration: 0.025s, episode steps:  53, steps per second: 2150, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 68922/100000: episode: 2074, duration: 0.023s, episode steps:  46, steps per second: 2031, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 68950/100000: episode: 2075, duration: 0.013s, episode steps:  28, steps per second: 2082, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 68972/100000: episode: 2076, duration: 0.012s, episode steps:  22, steps per second: 1886, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 68994/100000: episode: 2077, duration: 0.012s, episode steps:  22, steps per second: 1800, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69014/100000: episode: 2078, duration: 0.010s, episode steps:  20, steps per second: 2017, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 69108/100000: episode: 2079, duration: 0.044s, episode steps:  94, steps per second: 2153, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 69145/100000: episode: 2080, duration: 0.018s, episode steps:  37, steps per second: 2097, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  mean_best_reward: --\n",
      " 69171/100000: episode: 2081, duration: 0.013s, episode steps:  26, steps per second: 1970, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69210/100000: episode: 2082, duration: 0.018s, episode steps:  39, steps per second: 2134, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 69224/100000: episode: 2083, duration: 0.008s, episode steps:  14, steps per second: 1791, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 69236/100000: episode: 2084, duration: 0.006s, episode steps:  12, steps per second: 1882, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 69281/100000: episode: 2085, duration: 0.021s, episode steps:  45, steps per second: 2183, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 69346/100000: episode: 2086, duration: 0.030s, episode steps:  65, steps per second: 2139, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 69374/100000: episode: 2087, duration: 0.014s, episode steps:  28, steps per second: 2050, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69443/100000: episode: 2088, duration: 0.033s, episode steps:  69, steps per second: 2086, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 69469/100000: episode: 2089, duration: 0.013s, episode steps:  26, steps per second: 2043, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69503/100000: episode: 2090, duration: 0.016s, episode steps:  34, steps per second: 2066, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 69564/100000: episode: 2091, duration: 0.028s, episode steps:  61, steps per second: 2194, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 69618/100000: episode: 2092, duration: 0.026s, episode steps:  54, steps per second: 2070, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69653/100000: episode: 2093, duration: 0.016s, episode steps:  35, steps per second: 2130, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 69677/100000: episode: 2094, duration: 0.012s, episode steps:  24, steps per second: 1985, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 69690/100000: episode: 2095, duration: 0.007s, episode steps:  13, steps per second: 1947, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 69783/100000: episode: 2096, duration: 0.042s, episode steps:  93, steps per second: 2193, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 69842/100000: episode: 2097, duration: 0.027s, episode steps:  59, steps per second: 2161, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 69860/100000: episode: 2098, duration: 0.010s, episode steps:  18, steps per second: 1839, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 69876/100000: episode: 2099, duration: 0.009s, episode steps:  16, steps per second: 1834, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 69898/100000: episode: 2100, duration: 0.012s, episode steps:  22, steps per second: 1878, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 69926/100000: episode: 2101, duration: 0.014s, episode steps:  28, steps per second: 1974, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 79.500000\n",
      " 69973/100000: episode: 2102, duration: 0.022s, episode steps:  47, steps per second: 2143, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 69989/100000: episode: 2103, duration: 0.008s, episode steps:  16, steps per second: 1925, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70028/100000: episode: 2104, duration: 0.018s, episode steps:  39, steps per second: 2109, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 70042/100000: episode: 2105, duration: 0.008s, episode steps:  14, steps per second: 1751, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70069/100000: episode: 2106, duration: 0.013s, episode steps:  27, steps per second: 2044, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  mean_best_reward: --\n",
      " 70127/100000: episode: 2107, duration: 0.027s, episode steps:  58, steps per second: 2143, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  mean_best_reward: --\n",
      " 70186/100000: episode: 2108, duration: 0.028s, episode steps:  59, steps per second: 2145, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  mean_best_reward: --\n",
      " 70220/100000: episode: 2109, duration: 0.017s, episode steps:  34, steps per second: 2000, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 70248/100000: episode: 2110, duration: 0.013s, episode steps:  28, steps per second: 2085, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70287/100000: episode: 2111, duration: 0.021s, episode steps:  39, steps per second: 1885, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 70322/100000: episode: 2112, duration: 0.017s, episode steps:  35, steps per second: 2036, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 70350/100000: episode: 2113, duration: 0.014s, episode steps:  28, steps per second: 1989, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 70417/100000: episode: 2114, duration: 0.031s, episode steps:  67, steps per second: 2187, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 70441/100000: episode: 2115, duration: 0.012s, episode steps:  24, steps per second: 2086, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 70473/100000: episode: 2116, duration: 0.015s, episode steps:  32, steps per second: 2070, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 70505/100000: episode: 2117, duration: 0.015s, episode steps:  32, steps per second: 2161, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 70547/100000: episode: 2118, duration: 0.021s, episode steps:  42, steps per second: 2011, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70573/100000: episode: 2119, duration: 0.013s, episode steps:  26, steps per second: 2033, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70614/100000: episode: 2120, duration: 0.020s, episode steps:  41, steps per second: 2009, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 70670/100000: episode: 2121, duration: 0.026s, episode steps:  56, steps per second: 2147, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 70697/100000: episode: 2122, duration: 0.013s, episode steps:  27, steps per second: 2090, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 70752/100000: episode: 2123, duration: 0.028s, episode steps:  55, steps per second: 1957, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 70805/100000: episode: 2124, duration: 0.026s, episode steps:  53, steps per second: 2074, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 70833/100000: episode: 2125, duration: 0.015s, episode steps:  28, steps per second: 1844, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 70856/100000: episode: 2126, duration: 0.011s, episode steps:  23, steps per second: 2032, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 70899/100000: episode: 2127, duration: 0.021s, episode steps:  43, steps per second: 2073, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 70918/100000: episode: 2128, duration: 0.010s, episode steps:  19, steps per second: 1941, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 70937/100000: episode: 2129, duration: 0.009s, episode steps:  19, steps per second: 2018, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 70991/100000: episode: 2130, duration: 0.025s, episode steps:  54, steps per second: 2152, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 71048/100000: episode: 2131, duration: 0.027s, episode steps:  57, steps per second: 2102, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 71143/100000: episode: 2132, duration: 0.045s, episode steps:  95, steps per second: 2125, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 71214/100000: episode: 2133, duration: 0.035s, episode steps:  71, steps per second: 2046, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 71260/100000: episode: 2134, duration: 0.022s, episode steps:  46, steps per second: 2103, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 71300/100000: episode: 2135, duration: 0.019s, episode steps:  40, steps per second: 2082, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 71331/100000: episode: 2136, duration: 0.015s, episode steps:  31, steps per second: 2019, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  mean_best_reward: --\n",
      " 71350/100000: episode: 2137, duration: 0.009s, episode steps:  19, steps per second: 2042, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 71366/100000: episode: 2138, duration: 0.008s, episode steps:  16, steps per second: 1981, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 71431/100000: episode: 2139, duration: 0.030s, episode steps:  65, steps per second: 2167, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 71459/100000: episode: 2140, duration: 0.014s, episode steps:  28, steps per second: 2042, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 71501/100000: episode: 2141, duration: 0.020s, episode steps:  42, steps per second: 2142, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 71537/100000: episode: 2142, duration: 0.018s, episode steps:  36, steps per second: 2020, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 71569/100000: episode: 2143, duration: 0.015s, episode steps:  32, steps per second: 2126, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 71627/100000: episode: 2144, duration: 0.027s, episode steps:  58, steps per second: 2142, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 71671/100000: episode: 2145, duration: 0.024s, episode steps:  44, steps per second: 1822, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71732/100000: episode: 2146, duration: 0.028s, episode steps:  61, steps per second: 2150, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 71785/100000: episode: 2147, duration: 0.025s, episode steps:  53, steps per second: 2099, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 71797/100000: episode: 2148, duration: 0.006s, episode steps:  12, steps per second: 1961, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71837/100000: episode: 2149, duration: 0.019s, episode steps:  40, steps per second: 2122, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 71872/100000: episode: 2150, duration: 0.017s, episode steps:  35, steps per second: 2095, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 71919/100000: episode: 2151, duration: 0.022s, episode steps:  47, steps per second: 2103, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: 111.500000\n",
      " 71975/100000: episode: 2152, duration: 0.026s, episode steps:  56, steps per second: 2134, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  mean_best_reward: --\n",
      " 72023/100000: episode: 2153, duration: 0.022s, episode steps:  48, steps per second: 2146, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 72050/100000: episode: 2154, duration: 0.013s, episode steps:  27, steps per second: 2086, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 72072/100000: episode: 2155, duration: 0.011s, episode steps:  22, steps per second: 2010, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72118/100000: episode: 2156, duration: 0.023s, episode steps:  46, steps per second: 1975, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 72187/100000: episode: 2157, duration: 0.032s, episode steps:  69, steps per second: 2142, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 72234/100000: episode: 2158, duration: 0.022s, episode steps:  47, steps per second: 2123, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 72269/100000: episode: 2159, duration: 0.017s, episode steps:  35, steps per second: 2099, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 72311/100000: episode: 2160, duration: 0.019s, episode steps:  42, steps per second: 2175, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 72343/100000: episode: 2161, duration: 0.016s, episode steps:  32, steps per second: 2032, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 72395/100000: episode: 2162, duration: 0.026s, episode steps:  52, steps per second: 2008, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 72426/100000: episode: 2163, duration: 0.014s, episode steps:  31, steps per second: 2151, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 72450/100000: episode: 2164, duration: 0.012s, episode steps:  24, steps per second: 2032, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 72513/100000: episode: 2165, duration: 0.029s, episode steps:  63, steps per second: 2173, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 72593/100000: episode: 2166, duration: 0.038s, episode steps:  80, steps per second: 2085, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 72630/100000: episode: 2167, duration: 0.018s, episode steps:  37, steps per second: 2049, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 72680/100000: episode: 2168, duration: 0.025s, episode steps:  50, steps per second: 2037, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 72733/100000: episode: 2169, duration: 0.025s, episode steps:  53, steps per second: 2123, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  mean_best_reward: --\n",
      " 72768/100000: episode: 2170, duration: 0.017s, episode steps:  35, steps per second: 2077, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 72881/100000: episode: 2171, duration: 0.052s, episode steps: 113, steps per second: 2165, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 72900/100000: episode: 2172, duration: 0.009s, episode steps:  19, steps per second: 2091, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  mean_best_reward: --\n",
      " 72915/100000: episode: 2173, duration: 0.008s, episode steps:  15, steps per second: 1768, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 72935/100000: episode: 2174, duration: 0.010s, episode steps:  20, steps per second: 2052, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 73007/100000: episode: 2175, duration: 0.033s, episode steps:  72, steps per second: 2157, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 73053/100000: episode: 2176, duration: 0.023s, episode steps:  46, steps per second: 2001, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 73082/100000: episode: 2177, duration: 0.015s, episode steps:  29, steps per second: 1971, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 73106/100000: episode: 2178, duration: 0.012s, episode steps:  24, steps per second: 2050, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73126/100000: episode: 2179, duration: 0.010s, episode steps:  20, steps per second: 2053, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 73152/100000: episode: 2180, duration: 0.013s, episode steps:  26, steps per second: 2004, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73238/100000: episode: 2181, duration: 0.039s, episode steps:  86, steps per second: 2196, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73300/100000: episode: 2182, duration: 0.029s, episode steps:  62, steps per second: 2169, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 73315/100000: episode: 2183, duration: 0.008s, episode steps:  15, steps per second: 1978, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 73347/100000: episode: 2184, duration: 0.015s, episode steps:  32, steps per second: 2105, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 73366/100000: episode: 2185, duration: 0.010s, episode steps:  19, steps per second: 1999, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  mean_best_reward: --\n",
      " 73391/100000: episode: 2186, duration: 0.014s, episode steps:  25, steps per second: 1820, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 73411/100000: episode: 2187, duration: 0.010s, episode steps:  20, steps per second: 2015, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 73428/100000: episode: 2188, duration: 0.009s, episode steps:  17, steps per second: 1971, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 73464/100000: episode: 2189, duration: 0.017s, episode steps:  36, steps per second: 2115, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 73495/100000: episode: 2190, duration: 0.016s, episode steps:  31, steps per second: 1941, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 73537/100000: episode: 2191, duration: 0.020s, episode steps:  42, steps per second: 2116, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73581/100000: episode: 2192, duration: 0.021s, episode steps:  44, steps per second: 2085, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 73624/100000: episode: 2193, duration: 0.020s, episode steps:  43, steps per second: 2159, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 73679/100000: episode: 2194, duration: 0.026s, episode steps:  55, steps per second: 2152, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      " 73702/100000: episode: 2195, duration: 0.011s, episode steps:  23, steps per second: 2037, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 73746/100000: episode: 2196, duration: 0.021s, episode steps:  44, steps per second: 2138, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 73775/100000: episode: 2197, duration: 0.014s, episode steps:  29, steps per second: 2033, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 73841/100000: episode: 2198, duration: 0.031s, episode steps:  66, steps per second: 2163, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73887/100000: episode: 2199, duration: 0.022s, episode steps:  46, steps per second: 2113, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 73947/100000: episode: 2200, duration: 0.030s, episode steps:  60, steps per second: 2010, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 74002/100000: episode: 2201, duration: 0.026s, episode steps:  55, steps per second: 2078, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: 93.000000\n",
      " 74018/100000: episode: 2202, duration: 0.008s, episode steps:  16, steps per second: 1970, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      " 74034/100000: episode: 2203, duration: 0.010s, episode steps:  16, steps per second: 1604, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  mean_best_reward: --\n",
      " 74069/100000: episode: 2204, duration: 0.017s, episode steps:  35, steps per second: 2023, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  mean_best_reward: --\n",
      " 74087/100000: episode: 2205, duration: 0.010s, episode steps:  18, steps per second: 1830, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74114/100000: episode: 2206, duration: 0.014s, episode steps:  27, steps per second: 1900, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 74176/100000: episode: 2207, duration: 0.029s, episode steps:  62, steps per second: 2131, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 74242/100000: episode: 2208, duration: 0.032s, episode steps:  66, steps per second: 2069, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 74273/100000: episode: 2209, duration: 0.015s, episode steps:  31, steps per second: 2038, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 74309/100000: episode: 2210, duration: 0.019s, episode steps:  36, steps per second: 1942, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 74347/100000: episode: 2211, duration: 0.018s, episode steps:  38, steps per second: 2080, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 74360/100000: episode: 2212, duration: 0.007s, episode steps:  13, steps per second: 1771, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      " 74379/100000: episode: 2213, duration: 0.010s, episode steps:  19, steps per second: 1999, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  mean_best_reward: --\n",
      " 74427/100000: episode: 2214, duration: 0.024s, episode steps:  48, steps per second: 1967, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74510/100000: episode: 2215, duration: 0.040s, episode steps:  83, steps per second: 2071, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 74525/100000: episode: 2216, duration: 0.008s, episode steps:  15, steps per second: 1922, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 74594/100000: episode: 2217, duration: 0.032s, episode steps:  69, steps per second: 2160, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 74608/100000: episode: 2218, duration: 0.008s, episode steps:  14, steps per second: 1856, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 74624/100000: episode: 2219, duration: 0.008s, episode steps:  16, steps per second: 1909, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 74645/100000: episode: 2220, duration: 0.010s, episode steps:  21, steps per second: 2049, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 74670/100000: episode: 2221, duration: 0.012s, episode steps:  25, steps per second: 2023, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 74707/100000: episode: 2222, duration: 0.017s, episode steps:  37, steps per second: 2131, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  mean_best_reward: --\n",
      " 74732/100000: episode: 2223, duration: 0.014s, episode steps:  25, steps per second: 1728, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 74762/100000: episode: 2224, duration: 0.015s, episode steps:  30, steps per second: 2050, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 74790/100000: episode: 2225, duration: 0.015s, episode steps:  28, steps per second: 1838, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 74820/100000: episode: 2226, duration: 0.014s, episode steps:  30, steps per second: 2102, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 74871/100000: episode: 2227, duration: 0.024s, episode steps:  51, steps per second: 2114, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 74887/100000: episode: 2228, duration: 0.008s, episode steps:  16, steps per second: 1886, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  mean_best_reward: --\n",
      " 74937/100000: episode: 2229, duration: 0.023s, episode steps:  50, steps per second: 2134, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75007/100000: episode: 2230, duration: 0.032s, episode steps:  70, steps per second: 2175, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75028/100000: episode: 2231, duration: 0.011s, episode steps:  21, steps per second: 1899, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 75044/100000: episode: 2232, duration: 0.008s, episode steps:  16, steps per second: 1970, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 75132/100000: episode: 2233, duration: 0.041s, episode steps:  88, steps per second: 2141, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 75168/100000: episode: 2234, duration: 0.019s, episode steps:  36, steps per second: 1938, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75185/100000: episode: 2235, duration: 0.009s, episode steps:  17, steps per second: 1888, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 75223/100000: episode: 2236, duration: 0.018s, episode steps:  38, steps per second: 2095, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 75266/100000: episode: 2237, duration: 0.020s, episode steps:  43, steps per second: 2140, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 75335/100000: episode: 2238, duration: 0.032s, episode steps:  69, steps per second: 2146, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 75378/100000: episode: 2239, duration: 0.020s, episode steps:  43, steps per second: 2109, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 75411/100000: episode: 2240, duration: 0.016s, episode steps:  33, steps per second: 2035, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 75432/100000: episode: 2241, duration: 0.011s, episode steps:  21, steps per second: 1996, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 75452/100000: episode: 2242, duration: 0.010s, episode steps:  20, steps per second: 2014, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 75508/100000: episode: 2243, duration: 0.027s, episode steps:  56, steps per second: 2083, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75524/100000: episode: 2244, duration: 0.008s, episode steps:  16, steps per second: 1991, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 75542/100000: episode: 2245, duration: 0.010s, episode steps:  18, steps per second: 1889, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  mean_best_reward: --\n",
      " 75567/100000: episode: 2246, duration: 0.013s, episode steps:  25, steps per second: 1926, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75589/100000: episode: 2247, duration: 0.012s, episode steps:  22, steps per second: 1842, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 75637/100000: episode: 2248, duration: 0.023s, episode steps:  48, steps per second: 2103, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 75658/100000: episode: 2249, duration: 0.011s, episode steps:  21, steps per second: 1892, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 75675/100000: episode: 2250, duration: 0.009s, episode steps:  17, steps per second: 1999, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 75690/100000: episode: 2251, duration: 0.009s, episode steps:  15, steps per second: 1690, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: 83.500000\n",
      " 75742/100000: episode: 2252, duration: 0.024s, episode steps:  52, steps per second: 2184, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 75870/100000: episode: 2253, duration: 0.058s, episode steps: 128, steps per second: 2196, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 75891/100000: episode: 2254, duration: 0.011s, episode steps:  21, steps per second: 1989, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 75907/100000: episode: 2255, duration: 0.008s, episode steps:  16, steps per second: 1926, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 76049/100000: episode: 2256, duration: 0.066s, episode steps: 142, steps per second: 2161, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76080/100000: episode: 2257, duration: 0.015s, episode steps:  31, steps per second: 2045, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 76109/100000: episode: 2258, duration: 0.014s, episode steps:  29, steps per second: 2024, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 76163/100000: episode: 2259, duration: 0.026s, episode steps:  54, steps per second: 2104, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 76201/100000: episode: 2260, duration: 0.018s, episode steps:  38, steps per second: 2098, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  mean_best_reward: --\n",
      " 76235/100000: episode: 2261, duration: 0.016s, episode steps:  34, steps per second: 2137, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 76289/100000: episode: 2262, duration: 0.025s, episode steps:  54, steps per second: 2119, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76342/100000: episode: 2263, duration: 0.024s, episode steps:  53, steps per second: 2188, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 76360/100000: episode: 2264, duration: 0.009s, episode steps:  18, steps per second: 1951, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 76400/100000: episode: 2265, duration: 0.019s, episode steps:  40, steps per second: 2118, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 76428/100000: episode: 2266, duration: 0.014s, episode steps:  28, steps per second: 2074, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 76453/100000: episode: 2267, duration: 0.012s, episode steps:  25, steps per second: 2001, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 76510/100000: episode: 2268, duration: 0.028s, episode steps:  57, steps per second: 2007, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 76564/100000: episode: 2269, duration: 0.025s, episode steps:  54, steps per second: 2128, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 76581/100000: episode: 2270, duration: 0.009s, episode steps:  17, steps per second: 1915, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 76605/100000: episode: 2271, duration: 0.012s, episode steps:  24, steps per second: 2040, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76642/100000: episode: 2272, duration: 0.019s, episode steps:  37, steps per second: 1992, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 76664/100000: episode: 2273, duration: 0.011s, episode steps:  22, steps per second: 2005, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76725/100000: episode: 2274, duration: 0.028s, episode steps:  61, steps per second: 2160, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 76748/100000: episode: 2275, duration: 0.012s, episode steps:  23, steps per second: 1998, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 76795/100000: episode: 2276, duration: 0.023s, episode steps:  47, steps per second: 2053, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 76826/100000: episode: 2277, duration: 0.015s, episode steps:  31, steps per second: 2027, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 76850/100000: episode: 2278, duration: 0.011s, episode steps:  24, steps per second: 2102, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 76887/100000: episode: 2279, duration: 0.018s, episode steps:  37, steps per second: 2067, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 76920/100000: episode: 2280, duration: 0.016s, episode steps:  33, steps per second: 2027, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 76944/100000: episode: 2281, duration: 0.013s, episode steps:  24, steps per second: 1824, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 76999/100000: episode: 2282, duration: 0.027s, episode steps:  55, steps per second: 2053, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 77053/100000: episode: 2283, duration: 0.025s, episode steps:  54, steps per second: 2157, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77105/100000: episode: 2284, duration: 0.024s, episode steps:  52, steps per second: 2186, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77140/100000: episode: 2285, duration: 0.018s, episode steps:  35, steps per second: 1951, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 77166/100000: episode: 2286, duration: 0.013s, episode steps:  26, steps per second: 2054, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77202/100000: episode: 2287, duration: 0.017s, episode steps:  36, steps per second: 2061, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77218/100000: episode: 2288, duration: 0.008s, episode steps:  16, steps per second: 1992, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 77236/100000: episode: 2289, duration: 0.010s, episode steps:  18, steps per second: 1845, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 77274/100000: episode: 2290, duration: 0.018s, episode steps:  38, steps per second: 2099, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  mean_best_reward: --\n",
      " 77308/100000: episode: 2291, duration: 0.016s, episode steps:  34, steps per second: 2069, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77358/100000: episode: 2292, duration: 0.025s, episode steps:  50, steps per second: 2007, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 77369/100000: episode: 2293, duration: 0.006s, episode steps:  11, steps per second: 1699, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 77425/100000: episode: 2294, duration: 0.027s, episode steps:  56, steps per second: 2106, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  mean_best_reward: --\n",
      " 77467/100000: episode: 2295, duration: 0.020s, episode steps:  42, steps per second: 2104, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 77503/100000: episode: 2296, duration: 0.017s, episode steps:  36, steps per second: 2064, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77544/100000: episode: 2297, duration: 0.019s, episode steps:  41, steps per second: 2152, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 77560/100000: episode: 2298, duration: 0.009s, episode steps:  16, steps per second: 1869, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77595/100000: episode: 2299, duration: 0.016s, episode steps:  35, steps per second: 2145, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 77623/100000: episode: 2300, duration: 0.014s, episode steps:  28, steps per second: 2043, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 77666/100000: episode: 2301, duration: 0.021s, episode steps:  43, steps per second: 2091, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: 117.500000\n",
      " 77732/100000: episode: 2302, duration: 0.031s, episode steps:  66, steps per second: 2151, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77780/100000: episode: 2303, duration: 0.023s, episode steps:  48, steps per second: 2077, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77810/100000: episode: 2304, duration: 0.015s, episode steps:  30, steps per second: 1944, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 77830/100000: episode: 2305, duration: 0.010s, episode steps:  20, steps per second: 1920, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 77855/100000: episode: 2306, duration: 0.012s, episode steps:  25, steps per second: 2090, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 77890/100000: episode: 2307, duration: 0.017s, episode steps:  35, steps per second: 2085, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 77968/100000: episode: 2308, duration: 0.036s, episode steps:  78, steps per second: 2168, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 77985/100000: episode: 2309, duration: 0.008s, episode steps:  17, steps per second: 2011, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 77998/100000: episode: 2310, duration: 0.007s, episode steps:  13, steps per second: 1932, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 78095/100000: episode: 2311, duration: 0.045s, episode steps:  97, steps per second: 2179, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 78145/100000: episode: 2312, duration: 0.024s, episode steps:  50, steps per second: 2117, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  mean_best_reward: --\n",
      " 78183/100000: episode: 2313, duration: 0.019s, episode steps:  38, steps per second: 2040, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 78225/100000: episode: 2314, duration: 0.021s, episode steps:  42, steps per second: 2022, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 78235/100000: episode: 2315, duration: 0.006s, episode steps:  10, steps per second: 1667, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 78324/100000: episode: 2316, duration: 0.041s, episode steps:  89, steps per second: 2154, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 78366/100000: episode: 2317, duration: 0.020s, episode steps:  42, steps per second: 2119, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 78391/100000: episode: 2318, duration: 0.012s, episode steps:  25, steps per second: 2070, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 78417/100000: episode: 2319, duration: 0.012s, episode steps:  26, steps per second: 2085, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 78471/100000: episode: 2320, duration: 0.026s, episode steps:  54, steps per second: 2070, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 78525/100000: episode: 2321, duration: 0.025s, episode steps:  54, steps per second: 2146, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 78569/100000: episode: 2322, duration: 0.021s, episode steps:  44, steps per second: 2071, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 78618/100000: episode: 2323, duration: 0.023s, episode steps:  49, steps per second: 2129, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 78630/100000: episode: 2324, duration: 0.007s, episode steps:  12, steps per second: 1830, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      " 78653/100000: episode: 2325, duration: 0.013s, episode steps:  23, steps per second: 1786, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 78679/100000: episode: 2326, duration: 0.014s, episode steps:  26, steps per second: 1875, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 78707/100000: episode: 2327, duration: 0.014s, episode steps:  28, steps per second: 2032, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 78749/100000: episode: 2328, duration: 0.021s, episode steps:  42, steps per second: 2031, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 78772/100000: episode: 2329, duration: 0.010s, episode steps:  23, steps per second: 2207, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 78875/100000: episode: 2330, duration: 0.048s, episode steps: 103, steps per second: 2150, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 78913/100000: episode: 2331, duration: 0.018s, episode steps:  38, steps per second: 2141, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 78941/100000: episode: 2332, duration: 0.014s, episode steps:  28, steps per second: 1949, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 78976/100000: episode: 2333, duration: 0.016s, episode steps:  35, steps per second: 2210, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 79006/100000: episode: 2334, duration: 0.015s, episode steps:  30, steps per second: 1959, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79035/100000: episode: 2335, duration: 0.015s, episode steps:  29, steps per second: 1993, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79075/100000: episode: 2336, duration: 0.020s, episode steps:  40, steps per second: 2013, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 79143/100000: episode: 2337, duration: 0.033s, episode steps:  68, steps per second: 2091, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 79177/100000: episode: 2338, duration: 0.018s, episode steps:  34, steps per second: 1916, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 79230/100000: episode: 2339, duration: 0.025s, episode steps:  53, steps per second: 2121, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 79268/100000: episode: 2340, duration: 0.018s, episode steps:  38, steps per second: 2140, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 79325/100000: episode: 2341, duration: 0.026s, episode steps:  57, steps per second: 2186, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 79349/100000: episode: 2342, duration: 0.013s, episode steps:  24, steps per second: 1883, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79396/100000: episode: 2343, duration: 0.022s, episode steps:  47, steps per second: 2147, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  mean_best_reward: --\n",
      " 79446/100000: episode: 2344, duration: 0.023s, episode steps:  50, steps per second: 2147, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 79476/100000: episode: 2345, duration: 0.015s, episode steps:  30, steps per second: 2052, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 79510/100000: episode: 2346, duration: 0.018s, episode steps:  34, steps per second: 1912, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79536/100000: episode: 2347, duration: 0.013s, episode steps:  26, steps per second: 1995, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 79599/100000: episode: 2348, duration: 0.030s, episode steps:  63, steps per second: 2107, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 79623/100000: episode: 2349, duration: 0.012s, episode steps:  24, steps per second: 2065, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79655/100000: episode: 2350, duration: 0.015s, episode steps:  32, steps per second: 2104, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 79687/100000: episode: 2351, duration: 0.016s, episode steps:  32, steps per second: 2030, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: 88.000000\n",
      " 79727/100000: episode: 2352, duration: 0.019s, episode steps:  40, steps per second: 2091, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79795/100000: episode: 2353, duration: 0.031s, episode steps:  68, steps per second: 2193, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 79813/100000: episode: 2354, duration: 0.009s, episode steps:  18, steps per second: 1914, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 79832/100000: episode: 2355, duration: 0.009s, episode steps:  19, steps per second: 2050, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 79845/100000: episode: 2356, duration: 0.007s, episode steps:  13, steps per second: 1913, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 79875/100000: episode: 2357, duration: 0.014s, episode steps:  30, steps per second: 2078, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 79950/100000: episode: 2358, duration: 0.036s, episode steps:  75, steps per second: 2089, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 79982/100000: episode: 2359, duration: 0.016s, episode steps:  32, steps per second: 2046, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 80007/100000: episode: 2360, duration: 0.012s, episode steps:  25, steps per second: 2066, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 80044/100000: episode: 2361, duration: 0.018s, episode steps:  37, steps per second: 2086, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 80091/100000: episode: 2362, duration: 0.022s, episode steps:  47, steps per second: 2174, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  mean_best_reward: --\n",
      " 80160/100000: episode: 2363, duration: 0.032s, episode steps:  69, steps per second: 2139, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 80181/100000: episode: 2364, duration: 0.011s, episode steps:  21, steps per second: 1964, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 80210/100000: episode: 2365, duration: 0.014s, episode steps:  29, steps per second: 2089, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 80257/100000: episode: 2366, duration: 0.023s, episode steps:  47, steps per second: 2051, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 80287/100000: episode: 2367, duration: 0.015s, episode steps:  30, steps per second: 2063, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  mean_best_reward: --\n",
      " 80330/100000: episode: 2368, duration: 0.020s, episode steps:  43, steps per second: 2157, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 80470/100000: episode: 2369, duration: 0.067s, episode steps: 140, steps per second: 2100, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 80523/100000: episode: 2370, duration: 0.025s, episode steps:  53, steps per second: 2098, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 80553/100000: episode: 2371, duration: 0.017s, episode steps:  30, steps per second: 1790, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 80565/100000: episode: 2372, duration: 0.007s, episode steps:  12, steps per second: 1821, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 80613/100000: episode: 2373, duration: 0.022s, episode steps:  48, steps per second: 2150, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 80630/100000: episode: 2374, duration: 0.009s, episode steps:  17, steps per second: 1893, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 80661/100000: episode: 2375, duration: 0.016s, episode steps:  31, steps per second: 1931, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 80690/100000: episode: 2376, duration: 0.016s, episode steps:  29, steps per second: 1806, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 80712/100000: episode: 2377, duration: 0.011s, episode steps:  22, steps per second: 1933, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 80739/100000: episode: 2378, duration: 0.013s, episode steps:  27, steps per second: 2050, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 80755/100000: episode: 2379, duration: 0.008s, episode steps:  16, steps per second: 1927, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 80774/100000: episode: 2380, duration: 0.011s, episode steps:  19, steps per second: 1786, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  mean_best_reward: --\n",
      " 80810/100000: episode: 2381, duration: 0.017s, episode steps:  36, steps per second: 2133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 80858/100000: episode: 2382, duration: 0.022s, episode steps:  48, steps per second: 2159, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80905/100000: episode: 2383, duration: 0.023s, episode steps:  47, steps per second: 2001, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 80931/100000: episode: 2384, duration: 0.014s, episode steps:  26, steps per second: 1844, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 80964/100000: episode: 2385, duration: 0.017s, episode steps:  33, steps per second: 1974, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 81004/100000: episode: 2386, duration: 0.019s, episode steps:  40, steps per second: 2090, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 81024/100000: episode: 2387, duration: 0.011s, episode steps:  20, steps per second: 1900, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 81092/100000: episode: 2388, duration: 0.031s, episode steps:  68, steps per second: 2163, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 81124/100000: episode: 2389, duration: 0.015s, episode steps:  32, steps per second: 2114, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 81182/100000: episode: 2390, duration: 0.027s, episode steps:  58, steps per second: 2169, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 81209/100000: episode: 2391, duration: 0.013s, episode steps:  27, steps per second: 2041, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 81227/100000: episode: 2392, duration: 0.009s, episode steps:  18, steps per second: 1945, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 81287/100000: episode: 2393, duration: 0.028s, episode steps:  60, steps per second: 2148, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 81314/100000: episode: 2394, duration: 0.013s, episode steps:  27, steps per second: 2106, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 81382/100000: episode: 2395, duration: 0.034s, episode steps:  68, steps per second: 2025, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 81451/100000: episode: 2396, duration: 0.032s, episode steps:  69, steps per second: 2138, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 81471/100000: episode: 2397, duration: 0.010s, episode steps:  20, steps per second: 1978, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 81499/100000: episode: 2398, duration: 0.013s, episode steps:  28, steps per second: 2116, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 81553/100000: episode: 2399, duration: 0.025s, episode steps:  54, steps per second: 2155, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 81606/100000: episode: 2400, duration: 0.025s, episode steps:  53, steps per second: 2163, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 81686/100000: episode: 2401, duration: 0.037s, episode steps:  80, steps per second: 2171, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: 89.500000\n",
      " 81761/100000: episode: 2402, duration: 0.034s, episode steps:  75, steps per second: 2183, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 81805/100000: episode: 2403, duration: 0.021s, episode steps:  44, steps per second: 2130, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 81824/100000: episode: 2404, duration: 0.010s, episode steps:  19, steps per second: 1810, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 81845/100000: episode: 2405, duration: 0.012s, episode steps:  21, steps per second: 1803, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 81879/100000: episode: 2406, duration: 0.016s, episode steps:  34, steps per second: 2123, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  mean_best_reward: --\n",
      " 81905/100000: episode: 2407, duration: 0.013s, episode steps:  26, steps per second: 2016, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  mean_best_reward: --\n",
      " 81946/100000: episode: 2408, duration: 0.019s, episode steps:  41, steps per second: 2177, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 81966/100000: episode: 2409, duration: 0.010s, episode steps:  20, steps per second: 1941, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 81989/100000: episode: 2410, duration: 0.011s, episode steps:  23, steps per second: 2026, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 82045/100000: episode: 2411, duration: 0.027s, episode steps:  56, steps per second: 2046, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 82059/100000: episode: 2412, duration: 0.007s, episode steps:  14, steps per second: 1971, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 82123/100000: episode: 2413, duration: 0.030s, episode steps:  64, steps per second: 2138, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 82133/100000: episode: 2414, duration: 0.005s, episode steps:  10, steps per second: 1856, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      " 82156/100000: episode: 2415, duration: 0.011s, episode steps:  23, steps per second: 2001, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 82174/100000: episode: 2416, duration: 0.010s, episode steps:  18, steps per second: 1889, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      " 82184/100000: episode: 2417, duration: 0.006s, episode steps:  10, steps per second: 1804, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  mean_best_reward: --\n",
      " 82271/100000: episode: 2418, duration: 0.042s, episode steps:  87, steps per second: 2062, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  mean_best_reward: --\n",
      " 82293/100000: episode: 2419, duration: 0.011s, episode steps:  22, steps per second: 2036, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 82341/100000: episode: 2420, duration: 0.023s, episode steps:  48, steps per second: 2109, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82361/100000: episode: 2421, duration: 0.010s, episode steps:  20, steps per second: 1998, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 82388/100000: episode: 2422, duration: 0.013s, episode steps:  27, steps per second: 2015, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 82453/100000: episode: 2423, duration: 0.031s, episode steps:  65, steps per second: 2115, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 82462/100000: episode: 2424, duration: 0.005s, episode steps:   9, steps per second: 1804, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 82477/100000: episode: 2425, duration: 0.008s, episode steps:  15, steps per second: 1992, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 82503/100000: episode: 2426, duration: 0.013s, episode steps:  26, steps per second: 2035, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82538/100000: episode: 2427, duration: 0.016s, episode steps:  35, steps per second: 2139, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 82552/100000: episode: 2428, duration: 0.007s, episode steps:  14, steps per second: 1906, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n",
      " 82592/100000: episode: 2429, duration: 0.019s, episode steps:  40, steps per second: 2076, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82612/100000: episode: 2430, duration: 0.010s, episode steps:  20, steps per second: 2026, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82644/100000: episode: 2431, duration: 0.016s, episode steps:  32, steps per second: 2058, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 82669/100000: episode: 2432, duration: 0.012s, episode steps:  25, steps per second: 2119, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82694/100000: episode: 2433, duration: 0.013s, episode steps:  25, steps per second: 1856, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 82709/100000: episode: 2434, duration: 0.009s, episode steps:  15, steps per second: 1647, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 82735/100000: episode: 2435, duration: 0.013s, episode steps:  26, steps per second: 1974, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 82758/100000: episode: 2436, duration: 0.011s, episode steps:  23, steps per second: 2085, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 82808/100000: episode: 2437, duration: 0.024s, episode steps:  50, steps per second: 2106, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 82837/100000: episode: 2438, duration: 0.014s, episode steps:  29, steps per second: 2118, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 82872/100000: episode: 2439, duration: 0.017s, episode steps:  35, steps per second: 2070, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 82898/100000: episode: 2440, duration: 0.012s, episode steps:  26, steps per second: 2087, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  mean_best_reward: --\n",
      " 82919/100000: episode: 2441, duration: 0.011s, episode steps:  21, steps per second: 1946, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 82942/100000: episode: 2442, duration: 0.012s, episode steps:  23, steps per second: 1929, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 82987/100000: episode: 2443, duration: 0.021s, episode steps:  45, steps per second: 2163, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 83007/100000: episode: 2444, duration: 0.010s, episode steps:  20, steps per second: 1914, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 83024/100000: episode: 2445, duration: 0.008s, episode steps:  17, steps per second: 2015, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 83040/100000: episode: 2446, duration: 0.008s, episode steps:  16, steps per second: 1896, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 83069/100000: episode: 2447, duration: 0.014s, episode steps:  29, steps per second: 2025, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 83086/100000: episode: 2448, duration: 0.008s, episode steps:  17, steps per second: 2020, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 83102/100000: episode: 2449, duration: 0.010s, episode steps:  16, steps per second: 1651, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83138/100000: episode: 2450, duration: 0.018s, episode steps:  36, steps per second: 1959, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83149/100000: episode: 2451, duration: 0.006s, episode steps:  11, steps per second: 1801, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  mean_best_reward: 86.000000\n",
      " 83191/100000: episode: 2452, duration: 0.020s, episode steps:  42, steps per second: 2088, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 83219/100000: episode: 2453, duration: 0.014s, episode steps:  28, steps per second: 2032, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 83238/100000: episode: 2454, duration: 0.010s, episode steps:  19, steps per second: 1883, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 83260/100000: episode: 2455, duration: 0.011s, episode steps:  22, steps per second: 2051, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83323/100000: episode: 2456, duration: 0.029s, episode steps:  63, steps per second: 2165, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 83350/100000: episode: 2457, duration: 0.014s, episode steps:  27, steps per second: 1974, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 83377/100000: episode: 2458, duration: 0.013s, episode steps:  27, steps per second: 2072, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 83417/100000: episode: 2459, duration: 0.020s, episode steps:  40, steps per second: 1961, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 83447/100000: episode: 2460, duration: 0.014s, episode steps:  30, steps per second: 2115, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 83477/100000: episode: 2461, duration: 0.015s, episode steps:  30, steps per second: 2026, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 83604/100000: episode: 2462, duration: 0.061s, episode steps: 127, steps per second: 2097, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n",
      " 83669/100000: episode: 2463, duration: 0.030s, episode steps:  65, steps per second: 2184, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 83694/100000: episode: 2464, duration: 0.012s, episode steps:  25, steps per second: 2011, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 83704/100000: episode: 2465, duration: 0.006s, episode steps:  10, steps per second: 1577, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  mean_best_reward: --\n",
      " 83716/100000: episode: 2466, duration: 0.007s, episode steps:  12, steps per second: 1751, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  mean_best_reward: --\n",
      " 83777/100000: episode: 2467, duration: 0.029s, episode steps:  61, steps per second: 2123, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 83833/100000: episode: 2468, duration: 0.026s, episode steps:  56, steps per second: 2166, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 83861/100000: episode: 2469, duration: 0.014s, episode steps:  28, steps per second: 2021, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 83902/100000: episode: 2470, duration: 0.020s, episode steps:  41, steps per second: 2094, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 83918/100000: episode: 2471, duration: 0.008s, episode steps:  16, steps per second: 1948, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  mean_best_reward: --\n",
      " 83942/100000: episode: 2472, duration: 0.013s, episode steps:  24, steps per second: 1916, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 84002/100000: episode: 2473, duration: 0.028s, episode steps:  60, steps per second: 2125, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84086/100000: episode: 2474, duration: 0.042s, episode steps:  84, steps per second: 1992, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84111/100000: episode: 2475, duration: 0.013s, episode steps:  25, steps per second: 1915, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 84127/100000: episode: 2476, duration: 0.008s, episode steps:  16, steps per second: 1961, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84158/100000: episode: 2477, duration: 0.017s, episode steps:  31, steps per second: 1837, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 84194/100000: episode: 2478, duration: 0.017s, episode steps:  36, steps per second: 2154, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84276/100000: episode: 2479, duration: 0.038s, episode steps:  82, steps per second: 2142, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 84285/100000: episode: 2480, duration: 0.005s, episode steps:   9, steps per second: 1735, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  mean_best_reward: --\n",
      " 84297/100000: episode: 2481, duration: 0.007s, episode steps:  12, steps per second: 1775, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 84363/100000: episode: 2482, duration: 0.030s, episode steps:  66, steps per second: 2171, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84387/100000: episode: 2483, duration: 0.012s, episode steps:  24, steps per second: 1992, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84416/100000: episode: 2484, duration: 0.014s, episode steps:  29, steps per second: 2010, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 84453/100000: episode: 2485, duration: 0.018s, episode steps:  37, steps per second: 2076, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 84488/100000: episode: 2486, duration: 0.016s, episode steps:  35, steps per second: 2140, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 84553/100000: episode: 2487, duration: 0.032s, episode steps:  65, steps per second: 2062, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 84601/100000: episode: 2488, duration: 0.023s, episode steps:  48, steps per second: 2105, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 84635/100000: episode: 2489, duration: 0.016s, episode steps:  34, steps per second: 2095, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 84656/100000: episode: 2490, duration: 0.011s, episode steps:  21, steps per second: 1930, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  mean_best_reward: --\n",
      " 84687/100000: episode: 2491, duration: 0.015s, episode steps:  31, steps per second: 2040, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 84726/100000: episode: 2492, duration: 0.019s, episode steps:  39, steps per second: 2067, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 84754/100000: episode: 2493, duration: 0.014s, episode steps:  28, steps per second: 2049, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 84793/100000: episode: 2494, duration: 0.018s, episode steps:  39, steps per second: 2119, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 84846/100000: episode: 2495, duration: 0.025s, episode steps:  53, steps per second: 2113, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 84860/100000: episode: 2496, duration: 0.007s, episode steps:  14, steps per second: 1953, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 84889/100000: episode: 2497, duration: 0.015s, episode steps:  29, steps per second: 1981, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  mean_best_reward: --\n",
      " 84925/100000: episode: 2498, duration: 0.018s, episode steps:  36, steps per second: 2005, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 84942/100000: episode: 2499, duration: 0.009s, episode steps:  17, steps per second: 1809, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 85006/100000: episode: 2500, duration: 0.031s, episode steps:  64, steps per second: 2056, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 85063/100000: episode: 2501, duration: 0.028s, episode steps:  57, steps per second: 2038, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: 87.000000\n",
      " 85087/100000: episode: 2502, duration: 0.012s, episode steps:  24, steps per second: 2009, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 85141/100000: episode: 2503, duration: 0.025s, episode steps:  54, steps per second: 2129, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 85161/100000: episode: 2504, duration: 0.010s, episode steps:  20, steps per second: 2038, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 85193/100000: episode: 2505, duration: 0.016s, episode steps:  32, steps per second: 2051, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 85235/100000: episode: 2506, duration: 0.020s, episode steps:  42, steps per second: 2122, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 85278/100000: episode: 2507, duration: 0.021s, episode steps:  43, steps per second: 2087, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 85336/100000: episode: 2508, duration: 0.027s, episode steps:  58, steps per second: 2178, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 85388/100000: episode: 2509, duration: 0.025s, episode steps:  52, steps per second: 2050, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 85422/100000: episode: 2510, duration: 0.017s, episode steps:  34, steps per second: 2011, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 85460/100000: episode: 2511, duration: 0.020s, episode steps:  38, steps per second: 1931, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 85509/100000: episode: 2512, duration: 0.024s, episode steps:  49, steps per second: 2078, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 85528/100000: episode: 2513, duration: 0.010s, episode steps:  19, steps per second: 1959, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 85601/100000: episode: 2514, duration: 0.034s, episode steps:  73, steps per second: 2177, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 85619/100000: episode: 2515, duration: 0.009s, episode steps:  18, steps per second: 1907, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 85666/100000: episode: 2516, duration: 0.022s, episode steps:  47, steps per second: 2157, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 85694/100000: episode: 2517, duration: 0.014s, episode steps:  28, steps per second: 2058, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  mean_best_reward: --\n",
      " 85712/100000: episode: 2518, duration: 0.009s, episode steps:  18, steps per second: 2018, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 85740/100000: episode: 2519, duration: 0.014s, episode steps:  28, steps per second: 2073, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 85795/100000: episode: 2520, duration: 0.026s, episode steps:  55, steps per second: 2148, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 85811/100000: episode: 2521, duration: 0.008s, episode steps:  16, steps per second: 1979, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85836/100000: episode: 2522, duration: 0.012s, episode steps:  25, steps per second: 2010, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 85854/100000: episode: 2523, duration: 0.010s, episode steps:  18, steps per second: 1784, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 85894/100000: episode: 2524, duration: 0.020s, episode steps:  40, steps per second: 2031, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 85914/100000: episode: 2525, duration: 0.010s, episode steps:  20, steps per second: 1953, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 85967/100000: episode: 2526, duration: 0.025s, episode steps:  53, steps per second: 2152, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 85999/100000: episode: 2527, duration: 0.015s, episode steps:  32, steps per second: 2074, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 86050/100000: episode: 2528, duration: 0.025s, episode steps:  51, steps per second: 2057, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 86080/100000: episode: 2529, duration: 0.014s, episode steps:  30, steps per second: 2091, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86107/100000: episode: 2530, duration: 0.014s, episode steps:  27, steps per second: 1888, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 86127/100000: episode: 2531, duration: 0.010s, episode steps:  20, steps per second: 1978, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 86174/100000: episode: 2532, duration: 0.022s, episode steps:  47, steps per second: 2148, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 86223/100000: episode: 2533, duration: 0.023s, episode steps:  49, steps per second: 2090, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 86236/100000: episode: 2534, duration: 0.007s, episode steps:  13, steps per second: 1902, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  mean_best_reward: --\n",
      " 86269/100000: episode: 2535, duration: 0.018s, episode steps:  33, steps per second: 1851, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 86284/100000: episode: 2536, duration: 0.008s, episode steps:  15, steps per second: 1919, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 86339/100000: episode: 2537, duration: 0.027s, episode steps:  55, steps per second: 2012, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 86408/100000: episode: 2538, duration: 0.033s, episode steps:  69, steps per second: 2106, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 86423/100000: episode: 2539, duration: 0.008s, episode steps:  15, steps per second: 1920, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  mean_best_reward: --\n",
      " 86479/100000: episode: 2540, duration: 0.026s, episode steps:  56, steps per second: 2145, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: --\n",
      " 86520/100000: episode: 2541, duration: 0.019s, episode steps:  41, steps per second: 2122, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  mean_best_reward: --\n",
      " 86542/100000: episode: 2542, duration: 0.011s, episode steps:  22, steps per second: 2006, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86572/100000: episode: 2543, duration: 0.014s, episode steps:  30, steps per second: 2112, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 86599/100000: episode: 2544, duration: 0.013s, episode steps:  27, steps per second: 2031, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 86635/100000: episode: 2545, duration: 0.017s, episode steps:  36, steps per second: 2071, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 86678/100000: episode: 2546, duration: 0.020s, episode steps:  43, steps per second: 2112, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 86745/100000: episode: 2547, duration: 0.033s, episode steps:  67, steps per second: 2002, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 86763/100000: episode: 2548, duration: 0.010s, episode steps:  18, steps per second: 1872, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 86798/100000: episode: 2549, duration: 0.017s, episode steps:  35, steps per second: 2118, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 86840/100000: episode: 2550, duration: 0.020s, episode steps:  42, steps per second: 2110, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 86861/100000: episode: 2551, duration: 0.011s, episode steps:  21, steps per second: 1965, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  mean_best_reward: 82.000000\n",
      " 86898/100000: episode: 2552, duration: 0.018s, episode steps:  37, steps per second: 2089, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 86927/100000: episode: 2553, duration: 0.014s, episode steps:  29, steps per second: 2048, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 86946/100000: episode: 2554, duration: 0.010s, episode steps:  19, steps per second: 1916, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 86959/100000: episode: 2555, duration: 0.007s, episode steps:  13, steps per second: 1798, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 86977/100000: episode: 2556, duration: 0.008s, episode steps:  18, steps per second: 2169, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  mean_best_reward: --\n",
      " 86987/100000: episode: 2557, duration: 0.006s, episode steps:  10, steps per second: 1666, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  mean_best_reward: --\n",
      " 87016/100000: episode: 2558, duration: 0.014s, episode steps:  29, steps per second: 2113, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 87060/100000: episode: 2559, duration: 0.021s, episode steps:  44, steps per second: 2081, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 87071/100000: episode: 2560, duration: 0.006s, episode steps:  11, steps per second: 1742, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      " 87092/100000: episode: 2561, duration: 0.010s, episode steps:  21, steps per second: 2007, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 87118/100000: episode: 2562, duration: 0.012s, episode steps:  26, steps per second: 2112, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87135/100000: episode: 2563, duration: 0.009s, episode steps:  17, steps per second: 1908, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87179/100000: episode: 2564, duration: 0.022s, episode steps:  44, steps per second: 2017, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 87198/100000: episode: 2565, duration: 0.010s, episode steps:  19, steps per second: 1868, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 87251/100000: episode: 2566, duration: 0.025s, episode steps:  53, steps per second: 2132, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 87277/100000: episode: 2567, duration: 0.013s, episode steps:  26, steps per second: 2072, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87304/100000: episode: 2568, duration: 0.013s, episode steps:  27, steps per second: 2056, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 87330/100000: episode: 2569, duration: 0.013s, episode steps:  26, steps per second: 2001, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87376/100000: episode: 2570, duration: 0.021s, episode steps:  46, steps per second: 2161, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 87402/100000: episode: 2571, duration: 0.013s, episode steps:  26, steps per second: 2049, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 87434/100000: episode: 2572, duration: 0.015s, episode steps:  32, steps per second: 2077, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 87449/100000: episode: 2573, duration: 0.007s, episode steps:  15, steps per second: 2011, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 87466/100000: episode: 2574, duration: 0.009s, episode steps:  17, steps per second: 1970, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  mean_best_reward: --\n",
      " 87510/100000: episode: 2575, duration: 0.021s, episode steps:  44, steps per second: 2144, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 87525/100000: episode: 2576, duration: 0.008s, episode steps:  15, steps per second: 1961, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 87571/100000: episode: 2577, duration: 0.022s, episode steps:  46, steps per second: 2104, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 87591/100000: episode: 2578, duration: 0.010s, episode steps:  20, steps per second: 1938, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 87611/100000: episode: 2579, duration: 0.010s, episode steps:  20, steps per second: 1906, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87627/100000: episode: 2580, duration: 0.008s, episode steps:  16, steps per second: 1904, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  mean_best_reward: --\n",
      " 87687/100000: episode: 2581, duration: 0.028s, episode steps:  60, steps per second: 2129, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87761/100000: episode: 2582, duration: 0.034s, episode steps:  74, steps per second: 2202, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 87780/100000: episode: 2583, duration: 0.010s, episode steps:  19, steps per second: 1920, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  mean_best_reward: --\n",
      " 87795/100000: episode: 2584, duration: 0.009s, episode steps:  15, steps per second: 1673, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 87811/100000: episode: 2585, duration: 0.008s, episode steps:  16, steps per second: 1910, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 87826/100000: episode: 2586, duration: 0.008s, episode steps:  15, steps per second: 1917, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 87840/100000: episode: 2587, duration: 0.007s, episode steps:  14, steps per second: 1988, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  mean_best_reward: --\n",
      " 87892/100000: episode: 2588, duration: 0.024s, episode steps:  52, steps per second: 2146, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 87933/100000: episode: 2589, duration: 0.020s, episode steps:  41, steps per second: 2102, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 87979/100000: episode: 2590, duration: 0.021s, episode steps:  46, steps per second: 2155, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 88003/100000: episode: 2591, duration: 0.013s, episode steps:  24, steps per second: 1876, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  mean_best_reward: --\n",
      " 88047/100000: episode: 2592, duration: 0.022s, episode steps:  44, steps per second: 2021, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 88063/100000: episode: 2593, duration: 0.008s, episode steps:  16, steps per second: 1890, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  mean_best_reward: --\n",
      " 88086/100000: episode: 2594, duration: 0.011s, episode steps:  23, steps per second: 2033, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 88136/100000: episode: 2595, duration: 0.025s, episode steps:  50, steps per second: 2015, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 88161/100000: episode: 2596, duration: 0.013s, episode steps:  25, steps per second: 1961, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 88183/100000: episode: 2597, duration: 0.011s, episode steps:  22, steps per second: 2082, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 88214/100000: episode: 2598, duration: 0.015s, episode steps:  31, steps per second: 2112, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 88287/100000: episode: 2599, duration: 0.034s, episode steps:  73, steps per second: 2132, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 88308/100000: episode: 2600, duration: 0.011s, episode steps:  21, steps per second: 1947, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 88332/100000: episode: 2601, duration: 0.012s, episode steps:  24, steps per second: 1931, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: 69.500000\n",
      " 88359/100000: episode: 2602, duration: 0.014s, episode steps:  27, steps per second: 1979, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 88386/100000: episode: 2603, duration: 0.013s, episode steps:  27, steps per second: 2124, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88419/100000: episode: 2604, duration: 0.017s, episode steps:  33, steps per second: 1969, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  mean_best_reward: --\n",
      " 88472/100000: episode: 2605, duration: 0.026s, episode steps:  53, steps per second: 2047, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 88503/100000: episode: 2606, duration: 0.015s, episode steps:  31, steps per second: 2083, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 88532/100000: episode: 2607, duration: 0.015s, episode steps:  29, steps per second: 1939, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 88605/100000: episode: 2608, duration: 0.035s, episode steps:  73, steps per second: 2088, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  mean_best_reward: --\n",
      " 88614/100000: episode: 2609, duration: 0.005s, episode steps:   9, steps per second: 1820, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  mean_best_reward: --\n",
      " 88645/100000: episode: 2610, duration: 0.016s, episode steps:  31, steps per second: 1959, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 88694/100000: episode: 2611, duration: 0.023s, episode steps:  49, steps per second: 2175, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 88747/100000: episode: 2612, duration: 0.025s, episode steps:  53, steps per second: 2112, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 88765/100000: episode: 2613, duration: 0.010s, episode steps:  18, steps per second: 1793, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  mean_best_reward: --\n",
      " 88793/100000: episode: 2614, duration: 0.014s, episode steps:  28, steps per second: 2073, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 88806/100000: episode: 2615, duration: 0.007s, episode steps:  13, steps per second: 1936, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 88871/100000: episode: 2616, duration: 0.031s, episode steps:  65, steps per second: 2078, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 88890/100000: episode: 2617, duration: 0.010s, episode steps:  19, steps per second: 1881, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 88931/100000: episode: 2618, duration: 0.020s, episode steps:  41, steps per second: 2040, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 88971/100000: episode: 2619, duration: 0.019s, episode steps:  40, steps per second: 2077, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  mean_best_reward: --\n",
      " 89031/100000: episode: 2620, duration: 0.028s, episode steps:  60, steps per second: 2133, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 89056/100000: episode: 2621, duration: 0.012s, episode steps:  25, steps per second: 2107, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 89070/100000: episode: 2622, duration: 0.007s, episode steps:  14, steps per second: 1918, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89103/100000: episode: 2623, duration: 0.016s, episode steps:  33, steps per second: 2039, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 89124/100000: episode: 2624, duration: 0.011s, episode steps:  21, steps per second: 1947, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 89147/100000: episode: 2625, duration: 0.012s, episode steps:  23, steps per second: 1974, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 89178/100000: episode: 2626, duration: 0.015s, episode steps:  31, steps per second: 2093, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 89211/100000: episode: 2627, duration: 0.017s, episode steps:  33, steps per second: 1944, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 89237/100000: episode: 2628, duration: 0.012s, episode steps:  26, steps per second: 2087, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89259/100000: episode: 2629, duration: 0.012s, episode steps:  22, steps per second: 1910, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89294/100000: episode: 2630, duration: 0.018s, episode steps:  35, steps per second: 1893, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 89319/100000: episode: 2631, duration: 0.014s, episode steps:  25, steps per second: 1841, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 89349/100000: episode: 2632, duration: 0.014s, episode steps:  30, steps per second: 2119, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89372/100000: episode: 2633, duration: 0.012s, episode steps:  23, steps per second: 1890, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 89394/100000: episode: 2634, duration: 0.011s, episode steps:  22, steps per second: 2006, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 89423/100000: episode: 2635, duration: 0.014s, episode steps:  29, steps per second: 2093, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 89447/100000: episode: 2636, duration: 0.014s, episode steps:  24, steps per second: 1692, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 89485/100000: episode: 2637, duration: 0.020s, episode steps:  38, steps per second: 1901, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 89548/100000: episode: 2638, duration: 0.038s, episode steps:  63, steps per second: 1644, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 89575/100000: episode: 2639, duration: 0.018s, episode steps:  27, steps per second: 1514, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 89619/100000: episode: 2640, duration: 0.026s, episode steps:  44, steps per second: 1710, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 89645/100000: episode: 2641, duration: 0.014s, episode steps:  26, steps per second: 1868, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 89657/100000: episode: 2642, duration: 0.007s, episode steps:  12, steps per second: 1670, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      " 89698/100000: episode: 2643, duration: 0.021s, episode steps:  41, steps per second: 1979, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 89725/100000: episode: 2644, duration: 0.013s, episode steps:  27, steps per second: 2068, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 89750/100000: episode: 2645, duration: 0.013s, episode steps:  25, steps per second: 1981, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 89775/100000: episode: 2646, duration: 0.012s, episode steps:  25, steps per second: 2060, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 89798/100000: episode: 2647, duration: 0.011s, episode steps:  23, steps per second: 2106, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      " 89813/100000: episode: 2648, duration: 0.008s, episode steps:  15, steps per second: 1822, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  mean_best_reward: --\n",
      " 89854/100000: episode: 2649, duration: 0.019s, episode steps:  41, steps per second: 2170, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 89891/100000: episode: 2650, duration: 0.018s, episode steps:  37, steps per second: 2003, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 89934/100000: episode: 2651, duration: 0.021s, episode steps:  43, steps per second: 2094, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: 91.500000\n",
      " 90061/100000: episode: 2652, duration: 0.058s, episode steps: 127, steps per second: 2191, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90126/100000: episode: 2653, duration: 0.035s, episode steps:  65, steps per second: 1875, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 90143/100000: episode: 2654, duration: 0.009s, episode steps:  17, steps per second: 1872, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 90192/100000: episode: 2655, duration: 0.023s, episode steps:  49, steps per second: 2143, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 90254/100000: episode: 2656, duration: 0.030s, episode steps:  62, steps per second: 2101, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 90285/100000: episode: 2657, duration: 0.015s, episode steps:  31, steps per second: 2104, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  mean_best_reward: --\n",
      " 90340/100000: episode: 2658, duration: 0.026s, episode steps:  55, steps per second: 2133, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 90364/100000: episode: 2659, duration: 0.012s, episode steps:  24, steps per second: 2035, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 90393/100000: episode: 2660, duration: 0.014s, episode steps:  29, steps per second: 2098, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 90432/100000: episode: 2661, duration: 0.018s, episode steps:  39, steps per second: 2129, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 90455/100000: episode: 2662, duration: 0.011s, episode steps:  23, steps per second: 2068, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 90536/100000: episode: 2663, duration: 0.037s, episode steps:  81, steps per second: 2173, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 90554/100000: episode: 2664, duration: 0.010s, episode steps:  18, steps per second: 1715, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 90620/100000: episode: 2665, duration: 0.031s, episode steps:  66, steps per second: 2129, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  mean_best_reward: --\n",
      " 90653/100000: episode: 2666, duration: 0.017s, episode steps:  33, steps per second: 1941, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 90699/100000: episode: 2667, duration: 0.022s, episode steps:  46, steps per second: 2118, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 90811/100000: episode: 2668, duration: 0.051s, episode steps: 112, steps per second: 2194, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 90845/100000: episode: 2669, duration: 0.016s, episode steps:  34, steps per second: 2067, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  mean_best_reward: --\n",
      " 90868/100000: episode: 2670, duration: 0.012s, episode steps:  23, steps per second: 1948, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  mean_best_reward: --\n",
      " 90905/100000: episode: 2671, duration: 0.017s, episode steps:  37, steps per second: 2124, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  mean_best_reward: --\n",
      " 90946/100000: episode: 2672, duration: 0.019s, episode steps:  41, steps per second: 2104, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 91017/100000: episode: 2673, duration: 0.035s, episode steps:  71, steps per second: 2021, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 91067/100000: episode: 2674, duration: 0.024s, episode steps:  50, steps per second: 2122, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 91079/100000: episode: 2675, duration: 0.006s, episode steps:  12, steps per second: 1892, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 91137/100000: episode: 2676, duration: 0.027s, episode steps:  58, steps per second: 2148, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 91157/100000: episode: 2677, duration: 0.010s, episode steps:  20, steps per second: 2098, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 91189/100000: episode: 2678, duration: 0.016s, episode steps:  32, steps per second: 2052, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 91213/100000: episode: 2679, duration: 0.012s, episode steps:  24, steps per second: 2044, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 91259/100000: episode: 2680, duration: 0.021s, episode steps:  46, steps per second: 2143, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 91290/100000: episode: 2681, duration: 0.015s, episode steps:  31, steps per second: 2112, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 91331/100000: episode: 2682, duration: 0.020s, episode steps:  41, steps per second: 2102, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 91393/100000: episode: 2683, duration: 0.029s, episode steps:  62, steps per second: 2170, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 91457/100000: episode: 2684, duration: 0.031s, episode steps:  64, steps per second: 2087, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  mean_best_reward: --\n",
      " 91474/100000: episode: 2685, duration: 0.009s, episode steps:  17, steps per second: 1883, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 91546/100000: episode: 2686, duration: 0.034s, episode steps:  72, steps per second: 2146, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  mean_best_reward: --\n",
      " 91569/100000: episode: 2687, duration: 0.012s, episode steps:  23, steps per second: 1966, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 91603/100000: episode: 2688, duration: 0.016s, episode steps:  34, steps per second: 2159, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 91655/100000: episode: 2689, duration: 0.025s, episode steps:  52, steps per second: 2120, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  mean_best_reward: --\n",
      " 91667/100000: episode: 2690, duration: 0.006s, episode steps:  12, steps per second: 1902, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  mean_best_reward: --\n",
      " 91716/100000: episode: 2691, duration: 0.023s, episode steps:  49, steps per second: 2144, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 91752/100000: episode: 2692, duration: 0.018s, episode steps:  36, steps per second: 2054, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 91779/100000: episode: 2693, duration: 0.013s, episode steps:  27, steps per second: 2076, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 91804/100000: episode: 2694, duration: 0.012s, episode steps:  25, steps per second: 2043, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91872/100000: episode: 2695, duration: 0.031s, episode steps:  68, steps per second: 2164, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 91968/100000: episode: 2696, duration: 0.045s, episode steps:  96, steps per second: 2119, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 92016/100000: episode: 2697, duration: 0.024s, episode steps:  48, steps per second: 2032, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 92039/100000: episode: 2698, duration: 0.012s, episode steps:  23, steps per second: 2000, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  mean_best_reward: --\n",
      " 92134/100000: episode: 2699, duration: 0.043s, episode steps:  95, steps per second: 2192, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 92164/100000: episode: 2700, duration: 0.014s, episode steps:  30, steps per second: 2100, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 92231/100000: episode: 2701, duration: 0.031s, episode steps:  67, steps per second: 2146, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: 115.000000\n",
      " 92278/100000: episode: 2702, duration: 0.023s, episode steps:  47, steps per second: 2074, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  mean_best_reward: --\n",
      " 92362/100000: episode: 2703, duration: 0.040s, episode steps:  84, steps per second: 2118, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 92374/100000: episode: 2704, duration: 0.007s, episode steps:  12, steps per second: 1774, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  mean_best_reward: --\n",
      " 92429/100000: episode: 2705, duration: 0.027s, episode steps:  55, steps per second: 2034, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  mean_best_reward: --\n",
      " 92463/100000: episode: 2706, duration: 0.016s, episode steps:  34, steps per second: 2062, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 92474/100000: episode: 2707, duration: 0.006s, episode steps:  11, steps per second: 1778, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 92494/100000: episode: 2708, duration: 0.011s, episode steps:  20, steps per second: 1864, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92539/100000: episode: 2709, duration: 0.021s, episode steps:  45, steps per second: 2160, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 92564/100000: episode: 2710, duration: 0.013s, episode steps:  25, steps per second: 1992, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 92595/100000: episode: 2711, duration: 0.015s, episode steps:  31, steps per second: 2097, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  mean_best_reward: --\n",
      " 92640/100000: episode: 2712, duration: 0.021s, episode steps:  45, steps per second: 2137, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  mean_best_reward: --\n",
      " 92669/100000: episode: 2713, duration: 0.015s, episode steps:  29, steps per second: 1986, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  mean_best_reward: --\n",
      " 92725/100000: episode: 2714, duration: 0.026s, episode steps:  56, steps per second: 2144, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 92783/100000: episode: 2715, duration: 0.026s, episode steps:  58, steps per second: 2199, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  mean_best_reward: --\n",
      " 92879/100000: episode: 2716, duration: 0.046s, episode steps:  96, steps per second: 2071, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 92888/100000: episode: 2717, duration: 0.005s, episode steps:   9, steps per second: 1733, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  mean_best_reward: --\n",
      " 92907/100000: episode: 2718, duration: 0.009s, episode steps:  19, steps per second: 2025, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 92918/100000: episode: 2719, duration: 0.006s, episode steps:  11, steps per second: 1785, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  mean_best_reward: --\n",
      " 92950/100000: episode: 2720, duration: 0.016s, episode steps:  32, steps per second: 2013, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 92973/100000: episode: 2721, duration: 0.011s, episode steps:  23, steps per second: 2007, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  mean_best_reward: --\n",
      " 93023/100000: episode: 2722, duration: 0.024s, episode steps:  50, steps per second: 2075, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  mean_best_reward: --\n",
      " 93043/100000: episode: 2723, duration: 0.010s, episode steps:  20, steps per second: 1945, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93100/100000: episode: 2724, duration: 0.027s, episode steps:  57, steps per second: 2149, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: --\n",
      " 93121/100000: episode: 2725, duration: 0.011s, episode steps:  21, steps per second: 1946, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 93206/100000: episode: 2726, duration: 0.039s, episode steps:  85, steps per second: 2187, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 93255/100000: episode: 2727, duration: 0.023s, episode steps:  49, steps per second: 2100, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  mean_best_reward: --\n",
      " 93290/100000: episode: 2728, duration: 0.018s, episode steps:  35, steps per second: 1897, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  mean_best_reward: --\n",
      " 93333/100000: episode: 2729, duration: 0.022s, episode steps:  43, steps per second: 1990, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  mean_best_reward: --\n",
      " 93366/100000: episode: 2730, duration: 0.016s, episode steps:  33, steps per second: 2026, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 93379/100000: episode: 2731, duration: 0.007s, episode steps:  13, steps per second: 1959, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 93449/100000: episode: 2732, duration: 0.032s, episode steps:  70, steps per second: 2187, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93493/100000: episode: 2733, duration: 0.021s, episode steps:  44, steps per second: 2071, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  mean_best_reward: --\n",
      " 93510/100000: episode: 2734, duration: 0.009s, episode steps:  17, steps per second: 1914, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  mean_best_reward: --\n",
      " 93531/100000: episode: 2735, duration: 0.011s, episode steps:  21, steps per second: 1952, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 93572/100000: episode: 2736, duration: 0.020s, episode steps:  41, steps per second: 2074, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 93612/100000: episode: 2737, duration: 0.019s, episode steps:  40, steps per second: 2104, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 93626/100000: episode: 2738, duration: 0.007s, episode steps:  14, steps per second: 1940, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 93684/100000: episode: 2739, duration: 0.028s, episode steps:  58, steps per second: 2092, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93750/100000: episode: 2740, duration: 0.033s, episode steps:  66, steps per second: 2006, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93792/100000: episode: 2741, duration: 0.020s, episode steps:  42, steps per second: 2104, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93870/100000: episode: 2742, duration: 0.036s, episode steps:  78, steps per second: 2174, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 93913/100000: episode: 2743, duration: 0.020s, episode steps:  43, steps per second: 2178, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 93981/100000: episode: 2744, duration: 0.032s, episode steps:  68, steps per second: 2134, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 93993/100000: episode: 2745, duration: 0.006s, episode steps:  12, steps per second: 1941, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 94008/100000: episode: 2746, duration: 0.008s, episode steps:  15, steps per second: 1883, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  mean_best_reward: --\n",
      " 94040/100000: episode: 2747, duration: 0.016s, episode steps:  32, steps per second: 2056, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  mean_best_reward: --\n",
      " 94064/100000: episode: 2748, duration: 0.011s, episode steps:  24, steps per second: 2093, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94093/100000: episode: 2749, duration: 0.014s, episode steps:  29, steps per second: 2107, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 94162/100000: episode: 2750, duration: 0.032s, episode steps:  69, steps per second: 2133, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 94195/100000: episode: 2751, duration: 0.018s, episode steps:  33, steps per second: 1848, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: 83.000000\n",
      " 94214/100000: episode: 2752, duration: 0.011s, episode steps:  19, steps per second: 1755, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 94240/100000: episode: 2753, duration: 0.013s, episode steps:  26, steps per second: 1933, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94267/100000: episode: 2754, duration: 0.013s, episode steps:  27, steps per second: 2138, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 94306/100000: episode: 2755, duration: 0.019s, episode steps:  39, steps per second: 2045, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 94369/100000: episode: 2756, duration: 0.029s, episode steps:  63, steps per second: 2137, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 94387/100000: episode: 2757, duration: 0.009s, episode steps:  18, steps per second: 1956, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94459/100000: episode: 2758, duration: 0.034s, episode steps:  72, steps per second: 2147, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 94483/100000: episode: 2759, duration: 0.012s, episode steps:  24, steps per second: 1965, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 94511/100000: episode: 2760, duration: 0.014s, episode steps:  28, steps per second: 2052, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 94538/100000: episode: 2761, duration: 0.014s, episode steps:  27, steps per second: 1909, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 94586/100000: episode: 2762, duration: 0.023s, episode steps:  48, steps per second: 2122, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 94670/100000: episode: 2763, duration: 0.040s, episode steps:  84, steps per second: 2124, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 94709/100000: episode: 2764, duration: 0.019s, episode steps:  39, steps per second: 2062, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 94827/100000: episode: 2765, duration: 0.053s, episode steps: 118, steps per second: 2212, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 94849/100000: episode: 2766, duration: 0.011s, episode steps:  22, steps per second: 1993, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 94893/100000: episode: 2767, duration: 0.020s, episode steps:  44, steps per second: 2158, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 94953/100000: episode: 2768, duration: 0.028s, episode steps:  60, steps per second: 2113, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 94966/100000: episode: 2769, duration: 0.007s, episode steps:  13, steps per second: 1774, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 94994/100000: episode: 2770, duration: 0.014s, episode steps:  28, steps per second: 1955, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 95077/100000: episode: 2771, duration: 0.039s, episode steps:  83, steps per second: 2150, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  mean_best_reward: --\n",
      " 95109/100000: episode: 2772, duration: 0.017s, episode steps:  32, steps per second: 1919, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  mean_best_reward: --\n",
      " 95128/100000: episode: 2773, duration: 0.010s, episode steps:  19, steps per second: 1814, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 95183/100000: episode: 2774, duration: 0.026s, episode steps:  55, steps per second: 2131, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 95197/100000: episode: 2775, duration: 0.007s, episode steps:  14, steps per second: 1950, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 95229/100000: episode: 2776, duration: 0.016s, episode steps:  32, steps per second: 2053, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95243/100000: episode: 2777, duration: 0.007s, episode steps:  14, steps per second: 1944, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95257/100000: episode: 2778, duration: 0.007s, episode steps:  14, steps per second: 1912, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  mean_best_reward: --\n",
      " 95283/100000: episode: 2779, duration: 0.013s, episode steps:  26, steps per second: 2026, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 95323/100000: episode: 2780, duration: 0.018s, episode steps:  40, steps per second: 2168, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95364/100000: episode: 2781, duration: 0.021s, episode steps:  41, steps per second: 1999, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 95387/100000: episode: 2782, duration: 0.011s, episode steps:  23, steps per second: 2098, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 95407/100000: episode: 2783, duration: 0.010s, episode steps:  20, steps per second: 1910, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  mean_best_reward: --\n",
      " 95420/100000: episode: 2784, duration: 0.007s, episode steps:  13, steps per second: 1791, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  mean_best_reward: --\n",
      " 95434/100000: episode: 2785, duration: 0.008s, episode steps:  14, steps per second: 1845, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95570/100000: episode: 2786, duration: 0.063s, episode steps: 136, steps per second: 2152, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 95588/100000: episode: 2787, duration: 0.010s, episode steps:  18, steps per second: 1894, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 95608/100000: episode: 2788, duration: 0.010s, episode steps:  20, steps per second: 1969, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  mean_best_reward: --\n",
      " 95627/100000: episode: 2789, duration: 0.009s, episode steps:  19, steps per second: 2014, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  mean_best_reward: --\n",
      " 95651/100000: episode: 2790, duration: 0.012s, episode steps:  24, steps per second: 1993, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  mean_best_reward: --\n",
      " 95692/100000: episode: 2791, duration: 0.019s, episode steps:  41, steps per second: 2114, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 95721/100000: episode: 2792, duration: 0.014s, episode steps:  29, steps per second: 2041, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 95794/100000: episode: 2793, duration: 0.034s, episode steps:  73, steps per second: 2171, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 95821/100000: episode: 2794, duration: 0.013s, episode steps:  27, steps per second: 2009, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 95851/100000: episode: 2795, duration: 0.014s, episode steps:  30, steps per second: 2090, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 95906/100000: episode: 2796, duration: 0.026s, episode steps:  55, steps per second: 2124, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 95935/100000: episode: 2797, duration: 0.014s, episode steps:  29, steps per second: 2099, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  mean_best_reward: --\n",
      " 95960/100000: episode: 2798, duration: 0.012s, episode steps:  25, steps per second: 2006, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 95984/100000: episode: 2799, duration: 0.012s, episode steps:  24, steps per second: 1997, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96015/100000: episode: 2800, duration: 0.017s, episode steps:  31, steps per second: 1807, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 96072/100000: episode: 2801, duration: 0.027s, episode steps:  57, steps per second: 2113, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  mean_best_reward: 98.500000\n",
      " 96104/100000: episode: 2802, duration: 0.015s, episode steps:  32, steps per second: 2115, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96139/100000: episode: 2803, duration: 0.017s, episode steps:  35, steps per second: 2064, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 96161/100000: episode: 2804, duration: 0.011s, episode steps:  22, steps per second: 2013, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96184/100000: episode: 2805, duration: 0.011s, episode steps:  23, steps per second: 2002, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 96209/100000: episode: 2806, duration: 0.013s, episode steps:  25, steps per second: 1993, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  mean_best_reward: --\n",
      " 96222/100000: episode: 2807, duration: 0.007s, episode steps:  13, steps per second: 1953, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  mean_best_reward: --\n",
      " 96268/100000: episode: 2808, duration: 0.023s, episode steps:  46, steps per second: 2018, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 96316/100000: episode: 2809, duration: 0.023s, episode steps:  48, steps per second: 2127, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 96327/100000: episode: 2810, duration: 0.006s, episode steps:  11, steps per second: 1891, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  mean_best_reward: --\n",
      " 96379/100000: episode: 2811, duration: 0.025s, episode steps:  52, steps per second: 2103, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  mean_best_reward: --\n",
      " 96437/100000: episode: 2812, duration: 0.029s, episode steps:  58, steps per second: 2013, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96465/100000: episode: 2813, duration: 0.013s, episode steps:  28, steps per second: 2092, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  mean_best_reward: --\n",
      " 96487/100000: episode: 2814, duration: 0.011s, episode steps:  22, steps per second: 2023, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  mean_best_reward: --\n",
      " 96542/100000: episode: 2815, duration: 0.026s, episode steps:  55, steps per second: 2091, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  mean_best_reward: --\n",
      " 96555/100000: episode: 2816, duration: 0.008s, episode steps:  13, steps per second: 1645, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  mean_best_reward: --\n",
      " 96591/100000: episode: 2817, duration: 0.017s, episode steps:  36, steps per second: 2133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  mean_best_reward: --\n",
      " 96641/100000: episode: 2818, duration: 0.023s, episode steps:  50, steps per second: 2131, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96699/100000: episode: 2819, duration: 0.027s, episode steps:  58, steps per second: 2115, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 96751/100000: episode: 2820, duration: 0.024s, episode steps:  52, steps per second: 2123, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  mean_best_reward: --\n",
      " 96843/100000: episode: 2821, duration: 0.042s, episode steps:  92, steps per second: 2192, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 96891/100000: episode: 2822, duration: 0.025s, episode steps:  48, steps per second: 1924, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 96907/100000: episode: 2823, duration: 0.008s, episode steps:  16, steps per second: 1978, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 96980/100000: episode: 2824, duration: 0.035s, episode steps:  73, steps per second: 2105, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 97003/100000: episode: 2825, duration: 0.012s, episode steps:  23, steps per second: 1945, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 97044/100000: episode: 2826, duration: 0.019s, episode steps:  41, steps per second: 2176, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 97075/100000: episode: 2827, duration: 0.016s, episode steps:  31, steps per second: 1998, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 97093/100000: episode: 2828, duration: 0.009s, episode steps:  18, steps per second: 1907, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  mean_best_reward: --\n",
      " 97142/100000: episode: 2829, duration: 0.024s, episode steps:  49, steps per second: 2044, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  mean_best_reward: --\n",
      " 97186/100000: episode: 2830, duration: 0.021s, episode steps:  44, steps per second: 2081, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97232/100000: episode: 2831, duration: 0.021s, episode steps:  46, steps per second: 2174, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  mean_best_reward: --\n",
      " 97261/100000: episode: 2832, duration: 0.014s, episode steps:  29, steps per second: 2047, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97339/100000: episode: 2833, duration: 0.037s, episode steps:  78, steps per second: 2092, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97370/100000: episode: 2834, duration: 0.015s, episode steps:  31, steps per second: 2001, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 97411/100000: episode: 2835, duration: 0.019s, episode steps:  41, steps per second: 2118, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 97436/100000: episode: 2836, duration: 0.014s, episode steps:  25, steps per second: 1820, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  mean_best_reward: --\n",
      " 97464/100000: episode: 2837, duration: 0.013s, episode steps:  28, steps per second: 2075, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n",
      " 97551/100000: episode: 2838, duration: 0.040s, episode steps:  87, steps per second: 2165, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  mean_best_reward: --\n",
      " 97615/100000: episode: 2839, duration: 0.029s, episode steps:  64, steps per second: 2182, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97675/100000: episode: 2840, duration: 0.028s, episode steps:  60, steps per second: 2128, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97737/100000: episode: 2841, duration: 0.029s, episode steps:  62, steps per second: 2169, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 97832/100000: episode: 2842, duration: 0.047s, episode steps:  95, steps per second: 2018, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 97853/100000: episode: 2843, duration: 0.011s, episode steps:  21, steps per second: 1975, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  mean_best_reward: --\n",
      " 97896/100000: episode: 2844, duration: 0.022s, episode steps:  43, steps per second: 1960, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  mean_best_reward: --\n",
      " 97944/100000: episode: 2845, duration: 0.024s, episode steps:  48, steps per second: 1987, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  mean_best_reward: --\n",
      " 97975/100000: episode: 2846, duration: 0.015s, episode steps:  31, steps per second: 2107, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 98034/100000: episode: 2847, duration: 0.027s, episode steps:  59, steps per second: 2150, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  mean_best_reward: --\n",
      " 98058/100000: episode: 2848, duration: 0.012s, episode steps:  24, steps per second: 1981, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  mean_best_reward: --\n",
      " 98089/100000: episode: 2849, duration: 0.017s, episode steps:  31, steps per second: 1861, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 98158/100000: episode: 2850, duration: 0.032s, episode steps:  69, steps per second: 2141, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  mean_best_reward: --\n",
      " 98214/100000: episode: 2851, duration: 0.027s, episode steps:  56, steps per second: 2057, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  mean_best_reward: 102.500000\n",
      " 98259/100000: episode: 2852, duration: 0.023s, episode steps:  45, steps per second: 1979, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  mean_best_reward: --\n",
      " 98300/100000: episode: 2853, duration: 0.020s, episode steps:  41, steps per second: 2074, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 98350/100000: episode: 2854, duration: 0.023s, episode steps:  50, steps per second: 2135, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  mean_best_reward: --\n",
      " 98404/100000: episode: 2855, duration: 0.025s, episode steps:  54, steps per second: 2127, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 98441/100000: episode: 2856, duration: 0.018s, episode steps:  37, steps per second: 2053, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  mean_best_reward: --\n",
      " 98480/100000: episode: 2857, duration: 0.018s, episode steps:  39, steps per second: 2118, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  mean_best_reward: --\n",
      " 98522/100000: episode: 2858, duration: 0.020s, episode steps:  42, steps per second: 2106, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  mean_best_reward: --\n",
      " 98551/100000: episode: 2859, duration: 0.014s, episode steps:  29, steps per second: 2039, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  mean_best_reward: --\n",
      " 98603/100000: episode: 2860, duration: 0.024s, episode steps:  52, steps per second: 2162, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  mean_best_reward: --\n",
      " 98644/100000: episode: 2861, duration: 0.019s, episode steps:  41, steps per second: 2124, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  mean_best_reward: --\n",
      " 98677/100000: episode: 2862, duration: 0.017s, episode steps:  33, steps per second: 1996, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  mean_best_reward: --\n",
      " 98717/100000: episode: 2863, duration: 0.020s, episode steps:  40, steps per second: 1984, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  mean_best_reward: --\n",
      " 98734/100000: episode: 2864, duration: 0.009s, episode steps:  17, steps per second: 1845, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 98775/100000: episode: 2865, duration: 0.019s, episode steps:  41, steps per second: 2124, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  mean_best_reward: --\n",
      " 98847/100000: episode: 2866, duration: 0.036s, episode steps:  72, steps per second: 1992, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  mean_best_reward: --\n",
      " 98889/100000: episode: 2867, duration: 0.020s, episode steps:  42, steps per second: 2126, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 98933/100000: episode: 2868, duration: 0.022s, episode steps:  44, steps per second: 2023, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  mean_best_reward: --\n",
      " 99029/100000: episode: 2869, duration: 0.044s, episode steps:  96, steps per second: 2194, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  mean_best_reward: --\n",
      " 99054/100000: episode: 2870, duration: 0.018s, episode steps:  25, steps per second: 1419, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  mean_best_reward: --\n",
      " 99085/100000: episode: 2871, duration: 0.018s, episode steps:  31, steps per second: 1749, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  mean_best_reward: --\n",
      " 99109/100000: episode: 2872, duration: 0.013s, episode steps:  24, steps per second: 1795, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99145/100000: episode: 2873, duration: 0.018s, episode steps:  36, steps per second: 1964, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  mean_best_reward: --\n",
      " 99204/100000: episode: 2874, duration: 0.029s, episode steps:  59, steps per second: 2061, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  mean_best_reward: --\n",
      " 99255/100000: episode: 2875, duration: 0.024s, episode steps:  51, steps per second: 2131, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  mean_best_reward: --\n",
      " 99285/100000: episode: 2876, duration: 0.016s, episode steps:  30, steps per second: 1893, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99328/100000: episode: 2877, duration: 0.020s, episode steps:  43, steps per second: 2111, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  mean_best_reward: --\n",
      " 99382/100000: episode: 2878, duration: 0.025s, episode steps:  54, steps per second: 2165, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  mean_best_reward: --\n",
      " 99428/100000: episode: 2879, duration: 0.022s, episode steps:  46, steps per second: 2087, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99456/100000: episode: 2880, duration: 0.014s, episode steps:  28, steps per second: 1989, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99490/100000: episode: 2881, duration: 0.016s, episode steps:  34, steps per second: 2141, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 99507/100000: episode: 2882, duration: 0.010s, episode steps:  17, steps per second: 1665, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  mean_best_reward: --\n",
      " 99604/100000: episode: 2883, duration: 0.045s, episode steps:  97, steps per second: 2159, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  mean_best_reward: --\n",
      " 99646/100000: episode: 2884, duration: 0.020s, episode steps:  42, steps per second: 2065, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  mean_best_reward: --\n",
      " 99685/100000: episode: 2885, duration: 0.018s, episode steps:  39, steps per second: 2143, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  mean_best_reward: --\n",
      " 99758/100000: episode: 2886, duration: 0.034s, episode steps:  73, steps per second: 2142, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  mean_best_reward: --\n",
      " 99782/100000: episode: 2887, duration: 0.012s, episode steps:  24, steps per second: 1992, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99830/100000: episode: 2888, duration: 0.023s, episode steps:  48, steps per second: 2107, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  mean_best_reward: --\n",
      " 99871/100000: episode: 2889, duration: 0.019s, episode steps:  41, steps per second: 2159, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  mean_best_reward: --\n",
      " 99893/100000: episode: 2890, duration: 0.011s, episode steps:  22, steps per second: 1985, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  mean_best_reward: --\n",
      " 99909/100000: episode: 2891, duration: 0.009s, episode steps:  16, steps per second: 1811, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  mean_best_reward: --\n",
      " 99983/100000: episode: 2892, duration: 0.036s, episode steps:  74, steps per second: 2080, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  mean_best_reward: --\n",
      "done, took 51.051 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 67.000, steps: 67\n",
      "Episode 2: reward: 53.000, steps: 53\n",
      "Episode 3: reward: 44.000, steps: 44\n",
      "Episode 4: reward: 84.000, steps: 84\n",
      "Episode 5: reward: 67.000, steps: 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x631b3f2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in tensorflow.keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "rga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
